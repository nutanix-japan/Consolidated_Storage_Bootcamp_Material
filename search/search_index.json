{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":"<p>Welcome to the Nutanix Unified Storage APJ TTT workshop 2024.</p> <p>This lab introduces Nutanix Files, Data Lens, Objects, Volumes and many common management tasks. Each section is a lesson with hands-on practice. The instructor is onsite to answers any additional questions that you may have.</p> <p>The goal of this workshop is to adequate you with the relevant techniques to conduct similar training to your partners in your own country. </p> <p>Traditionally, SAN, NAS and object storage have created silos within the Datacentre. This introduces un-necessary complexity and the ability to scale and the lack of continuous innovation seen in SAN storage. Nutanix believes in eliminating silos in the Enterprise Cloud by approaching file storage as an app, i.e running as software define storage on top of a proven HCI core, Nutanix Files delivers high performance, scalability, and rapid innovation through One Click management.</p> <p>Please ensure you read the whole lab before starting</p> <p>In this lab you will step through managing SMB shares and NFS exports, scale out the environment, and explore upcoming Files features. The lab will provide key considerations around deployment, configuration, and use cases.</p>"},{"location":"#whats-new","title":"What's New","text":"<ul> <li> <p>Workshop updated for the following software versions:</p> </li> <li> <p>AOS 6.5.2.5</p> </li> <li>Prism Central pc.2022.6.0.3</li> <li>MSP 2.4.2.1</li> <li>Files 4.3</li> <li>Files Manager 4.3</li> <li>Objects Manager 3.6</li> <li>Objects Service 3.6</li> </ul>"},{"location":"#agenda","title":"Agenda","text":"<ul> <li> <p>Initial NUS Deployment</p> <ul> <li>File - Deploy Nutanix Files</li> <li>Object = Deploy Nutanix Objects</li> <li>Files &amp; Objects - RBAC on Files and Objects</li> </ul> </li> <li> <p>Start Consuming Storage Services</p> <ul> <li>Files - Create SMB Share</li> <li>Volumes - Deploy Nutanix Volumes</li> <li>Objects - Buckets &amp; Users Management</li> </ul> </li> <li> <p>Migrating from Existing File Shares</p> <ul> <li>Files - Share Migration</li> </ul> </li> <li> <p>Data Visibility &amp; Intelligence - The lab will use either Data Lens or File Analytics, the instructor will let you know which lab you should do.</p> <ul> <li> <p>Data Visibility &amp; Intelligence (A) - Data Lens </p> <ul> <li>Deploy Data Lens</li> <li>Anomaly Detection</li> <li>Ransomware Protection</li> </ul> </li> <li> <p>Data Visibility &amp; Intelligence (B) - File Analytics</p> <ul> <li>Enable File Analytics</li> <li>Anomaly Detection</li> <li>Ransomware Protection</li> </ul> </li> </ul> </li> <li> <p>Data Protection &amp; Lifecycle Management</p> <ul> <li>Files - Replication</li> <li>Files - Tiering</li> <li>Objects - Replication</li> </ul> </li> <li> <p>Optional Labs</p> <ul> <li>Files - Create NFS Export</li> <li>Files - Expand Files Cluster</li> <li>Files - Enable Files Multi-protocol</li> <li>Objects - Access Objects from CLI and Scripts</li> </ul> </li> </ul> <p>The labs are designed to run on any Nutanix AHV cluster provided the following are present:</p> <ul> <li>AHV IPAM Network</li> <li>DHCP Scope</li> <li>Gateway</li> <li>DNS server (this lab uses a Microsoft DNS configuration)</li> <li>IP address allocations for the OCP nodes and other infrastructure elements</li> <li>Internet connectivity (no dark site setup information is documented yet)</li> <li>A Linux VM (any cloud distribution is ok. CloudInit file is provided in the LinuxToolsVM) section.</li> </ul>"},{"location":"index_dsta/","title":"Getting Started","text":"<p>Welcome to the Nutanix Consolidated Storage Bootcamp for CX offsite 2023.</p> <p>This workbook introduces Nutanix Files, Data Lens, Objects, Volumes and many common management tasks. Each section has a lesson to give you hands-on practice. The instructor is onsite to answers any additional questions that you may have.</p> <p>Traditionally, SAN, NAS and object storage have created silos within IT, introducing un-necessary complexity and suffering from the same issues of scale and lack of continuous innovation seen in SAN storage. Nutanix believes there is no room for silos in the Enterprise Cloud. By approaching file storage as an app, running in software on top of a proven HCI core, Nutanix Files delivers high performance, scalability, and rapid innovation through One Click management.</p> <p>Please ensure you read the whole lab before starting</p> <p>In this lab you will step through managing SMB shares and NFS exports, scale out the environment, and explore upcoming Files features. The lab will provide key considerations around deployment, configuration, and use cases.</p>"},{"location":"index_dsta/#whats-new","title":"What's New","text":"<ul> <li> <p>Workshop updated for the following software versions:</p> </li> <li> <p>AOS 6.5.2.5</p> </li> <li>Prism Central pc.2022.6.0.3</li> <li>MSP 2.4.2.1</li> <li>Files 4.3</li> <li>Files Manager 4.3</li> <li>Objects Manager 3.6</li> <li>Objects Service 3.6</li> </ul>"},{"location":"index_dsta/#agenda","title":"Agenda","text":"<ul> <li> <p>Initial NUS Deployment</p> <ul> <li>File - Deploy Nutanix Files</li> <li>Object = Deploy Nutanix Objects</li> <li>Files &amp; Objects - RBAC on Files and Objects</li> </ul> </li> <li> <p>Start Consuming Storage Services</p> <ul> <li>Files - Create SMB Share</li> <li>Volumes - Deploy Nutanix Volumes</li> <li>Objects - Buckets &amp; Users Management</li> </ul> </li> <li> <p>Migrating from Existing File Shares</p> <ul> <li>Files - Share Migration</li> </ul> </li> <li> <p>Data Visibility &amp; Intelligence (A) - Data Lens </p> <ul> <li>Deploy Data Lens</li> <li>Anomaly Detection</li> <li>Ransomware Protection</li> </ul> </li> <li> <p>Data Visibility &amp; Intelligence (B) - File Analytics</p> <ul> <li>Enable File Analytics</li> <li>Anomaly Detection</li> <li>Ransomware Protection</li> </ul> </li> <li> <p>Data Protection &amp; Lifecycle Management</p> <ul> <li>Files - Replication</li> <li>Files - Tiering</li> </ul> </li> <li> <p>Optional Labs</p> <ul> <li>Files - Create NFS Export</li> <li>Files - Expand Files Cluster</li> <li>Files - Enable Files Multi-protocol</li> <li>Objects - Access Objects from CLI and Scripts</li> </ul> </li> </ul>"},{"location":"index_dsta/#initial-setup","title":"Initial Setup","text":"<ul> <li>Take note of the Passwords being used in this lab<ul> <li>Prism Element, Prism Central, CVMs, PCVM password will be provided by instructor.</li> <li>FSVM, all AD users share the same password of nutanix/4u</li> </ul> </li> <li>Login to Global Protect VPN, select gateway without (ST). This is required to access Data Lens lab</li> <li>Log into your virtual desktops if required (connection info below)</li> </ul>"},{"location":"index_dsta/#cluster-assignment","title":"Cluster Assignment","text":"<p>The instructor will inform the attendees their assigned clusters.</p> <p>Note</p> <pre><code>If these are Single Node Clusters (SNCs) pay close attention on the\nnetworking part. The SNCs are setup and configured differently\nto the 3 or 4 node clusters\n</code></pre>"},{"location":"index_dsta/#environment-details","title":"Environment Details","text":"<p>Nutanix Workshops are intended to be run in the Nutanix Hosted POC environment. Your cluster will be provisioned with all necessary images, networks, and VMs required to complete the exercises.</p>"},{"location":"index_dsta/#networking","title":"Networking","text":"<p>As we are able to provide three/four node clusters and single node clusters in the HPOC environment, we need to describe each sort of cluster separately. The clusters are setup and configured differently.</p>"},{"location":"index_dsta/#threefour-node-hpoc-clusters","title":"Three/Four node HPOC clusters","text":"<p>Three or four node Hosted POC clusters follow a standard naming convention:</p> <ul> <li>Cluster Name - POCXYZ</li> <li>Subnet - 10.42.XYZ.0</li> <li>Cluster IP - 10.42.XYZ.37</li> </ul> <p>For example:</p> <ul> <li>Cluster Name - POC055</li> <li>Subnet - 10.42.55.0</li> <li>Cluster IP - 10.42.55.37 for the VIP of the Cluster</li> </ul> <p>Throughout the Workshop there are multiple instances where you will need to substitute XYZ with the correct octet for your subnet, for example:</p> IP Address Description 10.42.XYZ.37 Nutanix Cluster Virtual IP 10.42.XYZ.39 PC VM IP, Prism Central 10.42.XYZ.41 DC VM IP, NTNXLAB.local Domain Controller <p>Each cluster is configured with 2 VLANs which can be used for VMs:</p> Network Name Address VLAN DHCP Scope Primary 10.42.XYZ.1/25 0 10.42.XYZ.50-10.42.XYZ.124 Secondary 10.42.XYZ.129/25 XYZ1 10.42.XYZ.132-10.42.XYZ.253"},{"location":"index_dsta/#single-node-hpoc-clusters-spoc","title":"Single Node HPOC Clusters (SPOC)","text":"<p>For some workshops we are using Single Node Clusters (SPOC). The reason for this is to allow more people to have a dedicated cluster but still have enough free clusters for the bigger workshops including those for customers.</p> <p>The network in the SPOC config is using a /26 network. This splits the network address into four equal sizes that can be used for workshops. The below table describes the setup of the network in the four partitions. It provides essential information for the workshop with respect to the IP addresses and the services running at that IP address.</p> <p>Naming convention: </p> <ul> <li>Cluster Name - PHX-SPOCXYZ-A</li> <li>XYZ is the SPOC number</li> <li>A is the partition number</li> <li>Subnet - 10.38.XYZ.0</li> <li>Cluster IP - 10.42.XYZ.37</li> </ul> Partition 1 Partition 2 Partition 3 Partition 4 Service Comment 10.38.x.1 10.38.x.65 10.38.x.129 10.38.x.193 Gateway 10.38.x.5 10.38.x.69 10.38.x.133 10.38.x.197 AHV Host 10.38.x.6 10.38.x.70 10.38.x.134 10.38.x.198 CVM IP 10.38.x.7 10.38.x.71 10.38.x.135 10.38.x.199 Cluster IP 10.38.x.8 10.38.x.72 10.38.x.136 10.38.x.200 Data Services IP 10.38.x.9 10.38.x.73 10.38.x.137 10.38.x.201 Prism Central IP 10.38.x.11 10.38.x.75 10.38.x.139 10.38.x.203 AutoDC IP(DC) 10.38.x.32-37 10.38.x.96-101 10.38.x.160-165 10.38.x.224-229 Objects 1 10.38.x.38-58 10.38.x.102-122 10.38.x.166-186 10.38.x.230-250 Primary 6 Free IPs for static assignment 10.38.x.0 10.38.x.64 10.38.x.128 10.38.x.192 Subnet"},{"location":"index_dsta/#credentials","title":"Credentials","text":"<p>Note</p> <pre><code>The *Cluster Password* is unique to each cluster and will be\nprovided by the leader of the Workshop.\n</code></pre> Credential Username Password Prism Element admin Cluster Password Prism Central admin Cluster Password Controller VM nutanix Cluster Password Prism Central VM nutanix Cluster Password <p>Each cluster has a dedicated domain controller VM, DC, responsible for providing AD services for the NTNXLAB.local domain. The domain is populated with the following Users and Groups:</p> Group Username(s) Password Administrators Administrator nutanix/4u SSP Admins adminuser01-adminuser25 nutanix/4u SSP Developers devuser01-devuser25 nutanix/4u SSP Consumers consumer01-consumer25 nutanix/4u SSP Operators operator01-operator25 nutanix/4u SSP Custom custom01-custom25 nutanix/4u Bootcamp Users user01-user25 nutanix/4u"},{"location":"index_dsta/#access-instructions","title":"Access Instructions","text":"<p>The Nutanix Hosted POC environment can be accessed a number of different ways:</p>"},{"location":"index_dsta/#lab-access-user-credentials","title":"Lab Access User Credentials","text":"<p>PHX Based Clusters: </p> <ul> <li>Username: PHX-POCxxx-User01 (up to PHX-POCxxx-User20), </li> <li>Password: Provided by Instructor</li> </ul> <p>RTP Based Clusters: </p> <ul> <li>Username: RTP-POCxxx-User01 (up to RTP-POCxxx-User20), </li> <li>Password: Provided by Instructor</li> </ul>"},{"location":"index_dsta/#frame-vdi","title":"Frame VDI","text":"<p>Login to: https://console.nutanix.com/x/labs</p> <p>Nutanix Employees - Use your NUTANIXDC credentials Non-Employees - Use Lab Access User Credentials</p>"},{"location":"index_dsta/#parallels-vdi","title":"Parallels VDI","text":"<p>PHX Based Clusters Login to: https://xld-uswest1.nutanix.com</p> <p>RTP Based Clusters Login to: https://xld-useast1.nutanix.com</p> <p>Nutanix Employees - Use your NUTANIXDC credentials Non-Employees - Use Lab Access User Credentials</p>"},{"location":"index_dsta/#employee-pulse-secure-vpn","title":"Employee Pulse Secure VPN","text":"<p>Download the client:</p> <p>PHX Based Clusters Login to: https://xld-uswest1.nutanix.com</p> <p>RTP Based Clusters Login to: https://xld-useast1.nutanix.com</p> <p>Nutanix Employees - Use your NUTANIXDC credentials Non-Employees - Use Lab Access User Credentials</p> <p>Install the client.</p> <p>In Pulse Secure Client, Add a connection:</p> <p>For PHX:</p> <ul> <li>Type - Policy Secure (UAC) or Connection Server</li> <li>Name - X-Labs - PHX</li> <li>Server URL - xlv-uswest1.nutanix.com</li> </ul> <p>For RTP:</p> <ul> <li>Type - Policy Secure (UAC) or Connection Server</li> <li>Name - X-Labs - RTP</li> <li>Server URL - xlv-useast1.nutanix.com</li> </ul>"},{"location":"index_dsta/#nutanix-version-info","title":"Nutanix Version Info","text":"<ul> <li>AOS 5.20.3.x &amp; </li> <li>PC pc.2022.4.0.1</li> <li>Calm 3.3.1</li> </ul>"},{"location":"datalens_anomaly/datalens_anomaly/","title":"Data Lens - Anomaly Detection","text":""},{"location":"datalens_anomaly/datalens_anomaly/#overview","title":"Overview","text":"<p>One of the major features that Data Lens have is the ability to identify abnormal behavior. It gives administrators a way to monitor analytics data for potential threats such as mass file deletion or perimission changes. When anomalies are detected based on administrator defined policies, alerts are sent to warn the administrator of the threat.</p> <p>In this exercise you will create anomaly rules and trigger events against the rules.</p>"},{"location":"datalens_anomaly/datalens_anomaly/#login-to-data-lens-if-you-are-not-already-there","title":"Login to Data Lens (if you are not already there)","text":"<ol> <li> <p>Connect to corp VPN, select the gateway without (ST)</p> </li> <li> <p>Go to https://datalens-qa.nutanix.com/ </p> </li> <li> <p>Your instructor will give you a my portal account to login</p> </li> <li> <p>Choose Common Tenant and then Proceed.</p> <p></p> </li> <li> <p>In the Data Lens Global Dashboard, go to File Servers and search the FQDN of your File Server Name(FSxyz-a-prod).</p> <p></p> <p>Note</p> <p>Your File Server is already added and enabled to the Data Lens Dashboard. Contact lab instructor if you cannot find it.</p> </li> <li> <p>Click the File Server Name to enter the Dashboard.</p> </li> </ol>"},{"location":"datalens_anomaly/datalens_anomaly/#define-anomaly-rules","title":"Define Anomaly Rules","text":"<ol> <li> <p>Click on  &gt; Anomalies</p> </li> <li> <p>Click Define Anomaly Rules</p> </li> <li> <p>Add your email address to the Anomaly Email Recipients, then click Save</p> </li> <li> <p>Click + Define Anomaly Rules, then create the first rule with the following settings:</p> <ul> <li>Operations: Delete</li> <li>Minimum Operation %: 1</li> <li>Minimum Operation Count: 1</li> <li>User: All Users</li> <li>Monitors: Hourly</li> <li>Interval: 1</li> </ul> </li> <li> <p>Click  to save the rule</p> </li> <li> <p>Click + Configure new anomaly and create a second rule with the following settings:</p> <ul> <li>Operations: Create</li> <li>Minimum Operation %: 1</li> <li>Minimum Operation Count: 1</li> <li>User: All Users</li> <li>Monitors: Hourly</li> <li>Interval: 1</li> </ul> </li> <li> <p>Click  to save the rule</p> </li> <li> <p>Click Close to go back to the dashboard         </p> </li> <li> <p>From the dashboard you are able to see number of anomaly alerts. top users and top folders that trigger anomaly rules on the past period.</p> </li> </ol>"},{"location":"datalens_anomaly/datalens_anomaly/#trigger-anomaly-rules","title":"Trigger anomaly rules","text":"<ol> <li> <p>While waiting for the Anomaly Alerts to populate,  we'll create a permission denial.</p> </li> <li> <p>Create a new directory called RO in the Marketing share</p> </li> <li> <p>Create TWO text files in the RO directory called myfile01.txt &amp; myfile02.txt with some text.</p> </li> <li> <p>Delete the TWO files. The creation and deletion of files should trigger the anomaly rules.</p> </li> <li> <p>Continue on the next lab until you receive email warning about anomalies detected.          </p> </li> <li> <p>Go back to Data Lens Anomaly Detection dashboard and check the charts.         </p> </li> </ol>"},{"location":"datalens_deploy/datalens_deploy/","title":"Data Lens - Deploy &amp; Basic Configuration","text":""},{"location":"datalens_deploy/datalens_deploy/#overview","title":"Overview","text":"<p>In this exercise you will explore Data Lens, auditing &amp; ransomware protection features and trigger a ransomware event.</p> <p>Note</p> <pre><code>As we are using a Nutanix Dev environment, this lab is only available for **Employees**. Please consult the instructor for assistence if you are a Nutanix customers or partners.\nPlease **ONLY** use your registered server\n</code></pre>"},{"location":"datalens_deploy/datalens_deploy/#login-to-data-lens","title":"Login to Data Lens","text":"<ol> <li> <p>Connect to corp VPN gateway without (ST)</p> </li> <li> <p>Go to https://datalens-qa.nutanix.com/ </p> </li> <li> <p>Please use the Datalens/password credentials provided in the HPOC lookup.</p> </li> <li> <p>Choose Common Tenant and then Proceed.</p> <p></p> </li> <li> <p>In the Data Lens Global Dashboard, go to File Servers and search the FQDN of your File Server Name (FSxyz-a-prod).</p> <p></p> <p>Note</p> <p>Your File Server is already added and enabled to the Data Lens Dashboard. Contact lab instructor if you cannot find it.</p> </li> <li> <p>Click the File Server Name to enter the Dashboard.</p> </li> </ol>"},{"location":"datalens_deploy/datalens_deploy/#basic-data-lens-operations","title":"Basic Data Lens Operations","text":"<ol> <li> <p>Check the Dashboard page in your browser to see the usage for the File Server. </p> </li> <li> <p>Check the Share All on the top left corner, and select any share to see the share level usage.</p> <p></p> </li> <li> <p>Here we will create a permission denial. Create a new directory called RO in your XYZ-GSO share</p> </li> <li> <p>Create a text file in the RO directory called myfile.txt with some text like \"hello world\"</p> </li> <li> <p>Go to the Properties of the RO folder and select the Security tab</p> </li> <li> <p>Select Advanced</p> </li> <li> <p>Choose Disable inheritance and select the Convert... option</p> </li> <li> <p>Then add the Everyone permissions with the following attributes:</p> <ul> <li>Read &amp; Execute</li> <li>List folder contents</li> <li>Read</li> </ul> <p></p> </li> <li> <p>Choose OK, OK and OK again</p> </li> <li> <p>Open a PowerShell window as a specific user</p> <ul> <li>Hold down Shift and right click on the PowerShell     icon on the taskbar</li> <li>Select Run as different user</li> </ul> <p></p> </li> <li> <p>Enter the following</p> <ul> <li>User name: devuser01</li> <li>Password: nutanix/4u</li> </ul> </li> <li> <p>Change Directories to RO under the xyz-GSO</p> <pre><code>cd \\\\FSxyz-a-prod.ntnxlab.local\\xyz-GSO\\RO\n</code></pre> </li> <li> <p>Execute the following commands, the first should succeed, the second should fail:</p> <pre><code>more .\\myfile.txt\nrm .\\myfile.txt\n</code></pre> <p></p> </li> <li> <p>After a minute or so you should see Permission Denials in both the dashboard and the Audit Trails view. (You may need to refresh your browser).</p> <p></p> </li> <li> <p>Refresh the Dashboard page to see the updates on Top 5 active users, Top 5 accessed files and File Operations panels.</p> </li> <li> <p>Click on your user under Top 5 Active Users. This will take you to the audit trail of the user. Alternatively,  click on the hamburger icon then Audit Trails menu and search for either your user or a given file. You can use wildcards for your search, for example *.txt. </p> </li> </ol> <p>You may continue to explore the data lens dashboard.</p>"},{"location":"datalens_permission_monitoring/datalens_permission_monitoring/","title":"Data Lens - Permission Monitoring","text":""},{"location":"datalens_permission_monitoring/datalens_permission_monitoring/#background","title":"Background","text":"<p>Starting with Data Lens Sept 2023 release, Data Lens provides features to monitor:</p> <ul> <li>Permissions for shares and directories </li> <li>Permissions for user and group level to identify sources access control</li> <li>Monitor source of risk using risk score trend analysis to identify data and user vulnerabilities.</li> </ul>"},{"location":"datalens_permission_monitoring/datalens_permission_monitoring/#lab","title":"Lab","text":"<p>In this lab you will be able to see how permission monitoring can be setup and how to find out which folders are in high risks to access control.</p>"},{"location":"datalens_permission_monitoring/datalens_permission_monitoring/#permission-monitoring-from-data-lens","title":"Permission Monitoring from Data Lens","text":"<ol> <li>Connect to corp VPN, select the gateway without (ST)</li> <li>Go to <code>https://datalens-qa.nutanix.com/</code></li> <li>Use the loging information provided in HPOC lookup tool to login</li> <li>Choose Common Tenant and then Proceed.</li> <li>In the Data Lens Global Dashboard, go to File Servers and search the FQDN of your File Server Name POCxyz-#.ntnxlab.local. </li> <li>Click the name to enter the Data Lens of the particular file server.</li> <li>From the pulldown list on the top left corner, click Permission Monitoring. Or click Go to Permission Dashboard from the Permission Monitoring tag in the Dashboard page.</li> </ol>"},{"location":"datalens_permission_monitoring/datalens_permission_monitoring/#create-risk-profile","title":"Create Risk Profile","text":"<ol> <li>In Permission Monitoring, click Settings from the right side, then Configure Risk Profiles </li> <li>Click + Add to add a new profile.</li> <li>Give a Profile Name as UserXX-Profile where XX is your assigned user number</li> <li>In Profile Configuration, there are 5 Risk Factor you can config. you can define your own risk score (High / Medium / Low Risk)</li> <li>Set all Threshold to be 1 and Unit as Count</li> <li>In Assign Profile to Shares, choose usershareXX </li> <li>Click Save </li> </ol>"},{"location":"datalens_permission_monitoring/datalens_permission_monitoring/#define-admin-access-open-access-criteria","title":"Define Admin Access &amp; Open Access Criteria","text":"<ol> <li>Define the default admin group which is used to determine if a folder has the default admin access.</li> <li>In Data Lens, under File Server &gt; Permission Monitoring &gt; Settings, click Define Admin Access Criteria</li> <li> <p>Check to select These Groups have access to it:, and add GroupXX in it where XX is your assigned user number. And then click Save.</p> <p>Warning</p> <pre><code>   Do not remove other groups in the list.\n</code></pre> </li> <li> <p>Close the box by click the cross icon      </p> </li> <li> <p>Now you can define open access by adding open access groups.</p> </li> <li>Click Settings again, then click Define Open Access Criteria</li> <li>Tick to select Specific Group, and then put Domain Users as the group.</li> <li>Close the box by clicking the  icon.      </li> </ol>"},{"location":"datalens_permission_monitoring/datalens_permission_monitoring/#create-folders-permissions","title":"Create Folders &amp; Permissions","text":"<ol> <li> <p>Login to your WinTools via RDP</p> </li> <li> <p>Map the usershareXX drive</p> </li> <li> <p>Create at least 5 folders, you can also try to create some sub-folders to test the permission monitoring</p> <p>Note</p> <p>To test the five risk factors in Data Lens, set the folder permissions that match the following criteria:</p> <ul> <li>Folders with control points - folders with non-inherited permission</li> <li>Folders with full control - folders with full control permissions to some users/groups</li> <li>Folders with modify access - folder with modify permissions to some users/groups</li> <li>Folders with open access - folders accessible by a wide number of users/groups</li> <li>Folders without admin access - folders without access to configure admin groups</li> </ul> <p>Info</p> <p>To change permission of the folders n your WinToolsVM, right click on Properties &gt; Security</p> <p>If existing permission needs to be removed, disable inheritance.</p> <p>Go to Properties &gt; Security &gt; Advanced &gt; Disable inheritance</p> <p>Choose to keep the existing permission or remove all and add again</p> <p></p> </li> </ol>"},{"location":"datalens_permission_monitoring/datalens_permission_monitoring/#verify-the-result-in-data-lens","title":"Verify the result in Data Lens","text":"<ol> <li>Go back to Data Lens &gt; Permission Monitoring</li> <li>It would take a few minutes for Data Lens learn of the permission changes</li> <li> <p>Click Share All, and select your share</p> <p></p> </li> <li> <p>Refresh to see the Share Risk Score, Risk Details and Recommendations.</p> </li> <li>You can change the permission and see how that will impact the score.</li> </ol>"},{"location":"datalens_permission_monitoring/datalens_permission_monitoring/#view-permission-details","title":"View Permission Details","text":"<ol> <li>Go to Data Lens, go to File Server &gt; Permission Monitoring</li> <li>From the top, click Shares &amp; Directories, you will see the list of shares in the file server with the risk score.</li> <li>Click Risk Breakdown, you will see the detail score and how the score is calculated, after viewing, click Close.</li> <li>On the top right search bar, key in one of the folder you created, and select it      </li> <li>From the result table, click Users with Access of the folder path      </li> <li>It will show the permission details, also click View NTFS Permissions of each user / group to see the detail permission rights      </li> <li> <p>Click the  icon on the top right corner to exit this page. Next to the folder, click Permission Changes </p> </li> <li> <p>Find the last latest permission change record and click View Details</p> </li> <li> <p>The changes that was done before will be shown here, verify if this is what you have done</p> <p></p> </li> <li> <p>Go back to Permission Monitoring, from the top, click Users &amp; Groups, you will see the list of groups or users with the permission details</p> </li> <li> <p>Click on Effective Access and All Permissions for group / user to see the permission list of them.</p> </li> </ol>"},{"location":"datalens_permission_monitoring/datalens_permission_monitoring/#takeaways","title":"Takeaways","text":"<p>With Permission monitoring, customers can see the full details of SMB permission per users &amp; groups, and per shares &amp; directories. Data Lens also gather the permission information and analyses the risk exposure of the shares and folders according to the scoring criteria. Customers will be able to see the highest risk shares or directories and mitigate the risk with Data Lens recommendations.</p>"},{"location":"datalens_ransomware/datalens_ransomware/","title":"Data Lens - Ransomware Protection","text":""},{"location":"datalens_ransomware/datalens_ransomware/#overview","title":"Overview","text":"<p>One of the core capability of Data Lens is the enhanced ransomware protection feature. Data Lens maintains a list of blocked signatures of file name or extension patterns of known ransomware variants and automatically applies the list to your file servers. An attack is prevented by locking the files when file extensions matches in the blocked signature list.</p> <p>Furthermore, the activity is bubbled into ransomware vulnerability dashboard reporting the client, user and the files impacted. In case of an attack, Data Lens performs remediation by blocking clients that trigger the attack or set the File Server to read-only mode. It will also track the infected files and recommend the best snapshot to recover the data.</p>"},{"location":"datalens_ransomware/datalens_ransomware/#lab-setup","title":"Lab Setup","text":"<p>In this lab, we will simulate a ransomware attack and verify how remediation works in Data Lens. </p>"},{"location":"datalens_ransomware/datalens_ransomware/#login-to-data-lens-if-you-are-not-already-there","title":"Login to Data Lens (if you are not already there)","text":"<ol> <li> <p>Connect to corp VPN, select the gateway without (ST)</p> </li> <li> <p>Go to https://datalens-qa.nutanix.com/ </p> </li> <li> <p>Your instructor will give you a my portal account to login</p> </li> <li> <p>Choose Common Tenant and then Proceed.</p> <p></p> </li> <li> <p>In the Data Lens Global Dashboard, go to File Servers and search the FQDN of your File Server Name (FSxyz-a-prod).</p> <p></p> <p>Note</p> <p>Your File Server is already added and enabled to the Data Lens Dashboard. Contact lab instructor if you cannot find it.</p> </li> <li> <p>Click the File Server Name to enter the Dashboard.</p> </li> </ol>"},{"location":"datalens_ransomware/datalens_ransomware/#enable-ransomware-protection-in-data-lens","title":"Enable Ransomware Protection in Data Lens","text":"<ol> <li> <p>Click on  &gt; Ransonware Protection</p> </li> <li> <p>If Ransomware Protection is not enabled, click Enable Ransomware Protection</p> </li> </ol> <p></p> <ol> <li> <p>If Ransomware Protection is enabled, go to Settings &gt; Edit Policy Configuration</p> </li> <li> <p>From Detect and Act on Ransomware Threats, select Make File Server Read-Only. Put your email address in Email Recipients, then Enable.</p> <p></p> </li> </ol>"},{"location":"datalens_ransomware/datalens_ransomware/#ransomware-protection-simulation","title":"Ransomware Protection Simulation","text":"<ol> <li> <p>Go to Settings &gt; Update Signature List, search for *.satana,*.AngryDuck in the search box. If it is not in the signature list, click Add to add it, otherwise just click Close.             </p> </li> <li> <p>Login to WinToolsVM using username : administrator@ntnxlab.local</p> </li> <li> <p>Check the following 2 shares in File Explorer:</p> <ul> <li><code>FS*XYZ-A*-prod.ntnxlab.local\\DLtest-prod</code></li> <li><code>FS*XYZ-A*-dr.ntnxlab.local\\DLtest-dr</code></li> </ul> </li> <li> <p>Copy all the files from DLtest-prod to DLtest-dr. We will use these TWO shares to compare the result with and without Data Lens ransomware protection.</p> </li> <li> <p>Create a new file ransomware_test.ps1 in Downloads, open the file and put the following content in the file. Save it.</p> Template commandsSample commands <pre><code>new-item \\\\fsXYZ-a-prod.ntnxlab.local\\DLtest-prod\\IamAngry.AngryDuck -ItemType file\nGet-ChildItem \\\\fsXYZ-a-prod.ntnxlab.local\\DLtest-prod\\*.txt | Rename-Item -NewName { $_.Name -replace '.txt','.satana' }\nnew-item \\\\fsXYZ-a-dr.ntnxlab.local\\DLtest-dr\\IamAngry.AngryDuck -ItemType file\nGet-ChildItem \\\\fsXYZ-a-dr.ntnxlab.local\\DLtest-dr\\*.txt | Rename-Item -NewName { $_.Name -replace '.txt','.satana' }\n</code></pre> <pre><code>new-item \\\\fs002-3-prod.ntnxlab.local\\DLtest-prod\\IamAngry.AngryDuck -ItemType file\nGet-ChildItem \\\\fs002-3-prod.ntnxlab.local\\DLtest-prod\\*.txt | Rename-Item -NewName { $_.Name -replace '.txt','.satana' }\nnew-item \\\\fs002-3-dr.ntnxlab.local\\DLtest-dr\\IamAngry.AngryDuck -ItemType file\nGet-ChildItem \\\\fs002-3-dr.ntnxlab.local\\DLtest-dr\\*.txt | Rename-Item -NewName { $_.Name -replace '.txt','.satana' }\n</code></pre> </li> <li> <p>Right click the  (PowerShell icon) and select Run as Administrator.</p> </li> <li> <p>Run the script, which has some file creation and changing file type to both shares.</p> <pre><code>C:\\Users\\Administrator\\Downloads\\ransomware_test.ps1\n</code></pre> </li> <li> <p>Check the share <code>FSxyz-a-prod.ntnxlab.local\\DLtest-prod,</code> you can see all files are .txt, meaning Data Lens is protecting against the operation of creating new or changing ransomware files according to the signature list. Check <code>FSxyz-a-dr.ntnxlab.local\\DLtest-dr</code>, you will see files were created and modified without Data Lens' protection.</p> </li> <li> <p>Wait until you receive email about ransomware attack alert.             </p> <p>Note</p> <pre><code>It may take up to 15 minutes to receive this email. You can proceed to other labs while you are waiting.\n</code></pre> </li> <li> <p>Go back to Data Lens &gt; FSxyz-a-prod.ntnxlab.local &gt;  &gt; Ransomware Protection, you can see the threats are recorded and the File Server is set to Read-Only mode automatically.</p> <p></p> </li> <li> <p>Login to your WinToolsVM and go to \\\\FSxyz-a-prod.ntnxlab.local\\DLtest-prod\\ from File Explorer; try to create a folder, you will see access denied as Data Lens has set it to read-only.</p> <p></p> <p>Note</p> <p>Data Lens sets the whole File Server to Read-Only mode. You can use another users to test on any shares in the same File Server. It should give you the same result.</p> </li> <li> <p>Go back to Data Lens &gt; FSxyz-a-prod.ntnxlab.local &gt;  &gt; Ransomware Protection. Under Blocked Entities, click Unblock &gt; Confirm to resume to read-write access.</p> <p>Note</p> <pre><code> You can always click the green circle ![](images/greencircle.png) to check the status of the tasks.\n</code></pre> </li> <li> <p>Return to Data Lens sometimes later to check if it is done. Verify the access from your WinToolsVM.</p> </li> </ol>"},{"location":"file_analytics_anomaly/file_analytics_anomaly/","title":"File Analytics - Anomaly Detection","text":""},{"location":"file_analytics_anomaly/file_analytics_anomaly/#overview","title":"Overview","text":"<p>One of the major features that Data Lens have is the ability to identify abnormal behavior. It gives administrators a way to monitor analytics data for potential threats such as mass file deletion or perimission changes. When anomalies are detected based on administrator defined policies, alerts are sent to warn the administrator of the threat.</p> <p>In this exercise you will create anomaly rules and trigger events against the rules.</p>"},{"location":"file_analytics_anomaly/file_analytics_anomaly/#login-to-file-analytics-if-you-are-not-already-there","title":"Login to File Analytics (If you are not already there)","text":"<ol> <li> <p>Login to Prism Element &gt; File Server &gt; click the File Server FSXYZ-#-prod &gt; File Analytics</p> <p></p> </li> </ol>"},{"location":"file_analytics_anomaly/file_analytics_anomaly/#define-anomaly-rules","title":"Define Anomaly Rules","text":"<ol> <li> <p>Create two anomaly rules by going to  &gt; Define Anomaly Rules </p> <p></p> </li> <li> <p>Click Configure SMTP to add recipients, then fill in the following settings:</p> <ul> <li>Hostname Or IP Address: 10.40.64.35</li> <li>Port: 25</li> <li>Security Mode: NONE</li> <li>From Email Address: phx-infra01@xlabs.nutanix.com</li> <li>Recipient Email Address: [your email address]</li> </ul> <p></p> </li> <li> <p>Click Test then Save.</p> </li> <li> <p>Go to  &gt; Define Anomaly Rules again</p> </li> <li> <p>Create the first rule with the following settings:</p> <ul> <li>Events:: Delete</li> <li>Minimum Operation %:: 1</li> <li>Minimum Operation Count:: 1</li> <li>User:: All Users</li> <li>Type:: Hourly</li> <li>Interval:: 1</li> </ul> </li> <li> <p>Choose Save for that anomaly table entry</p> </li> <li> <p>Choose + Configure new anomaly and create a second rule with the     following settings</p> <ul> <li>Events: Create</li> <li>Minimum Operation %: 1</li> <li>Minimum Operation Count: 1</li> <li>User: All Users</li> <li>Type: Hourly</li> <li>Interval: 1</li> </ul> </li> <li> <p>Choose Save for that anomaly table entry</p> <p></p> </li> <li> <p>Select Save to exit the Define Anomaly Rules window</p> </li> </ol>"},{"location":"file_analytics_anomaly/file_analytics_anomaly/#load-sample-data","title":"Load Sample Data","text":"<ol> <li> <p>Go to the folder usershareXX and copy and paste all the files and folders in the same share.</p> <p></p> </li> <li> <p>Now delete the copied files.</p> </li> </ol>"},{"location":"file_analytics_anomaly/file_analytics_anomaly/#cause-error-condition","title":"Cause Error Condition","text":"<ol> <li> <p>While waiting for the Anomaly Alerts to populate we'll create a     permission denial.</p> <p>The Anomaly engine runs every 30 minutes. While this setting is configurable from the File Analytics VM, modifying this variable is outside the scope of this lab. :::</p> </li> <li> <p>Create a new directory called RO in the same share</p> </li> <li> <p>Create a text file in the RO directory with some text like     \"hello world\" called myfile.txt</p> </li> <li> <p>Go to the Properties of the RO folder and select the     Security tab</p> </li> <li> <p>Select Advanced</p> </li> <li> <p>Choose Disable inheritance and select the Convert... option</p> </li> <li> <p>Then add the Everyone permissions with the following:</p> <ul> <li>Read &amp; Execute</li> <li>List folder contents</li> <li>Read</li> </ul> <p></p> </li> <li> <p>Choose OK, OK and OK again</p> </li> <li> <p>Open a PowerShell window as a specific user</p> <ul> <li>Hold down Shift and right click on the PowerShell     icon on the taskbar</li> <li>Select Run as different user</li> </ul> <p></p> </li> <li> <p>Enter the following</p> <ul> <li>User name: devuser01</li> <li>Password: nutanix/4u</li> </ul> </li> <li> <p>Change Directories into the share share and the RO directory</p> <pre><code>    cd \\\\FSXYZ-#-prod.ntnxlab.local\\usershareXX\\RO\n</code></pre> </li> <li> <p>Execute the following commands, the first should succeed, the second     should fail:</p> <pre><code>    more .\\myfile.txt\n    rm .\\myfile.txt\n</code></pre> <p></p> </li> <li> <p>After a minute or so you should see Permission Denials in both     the dashboard and the Audit Trails view. You may need to refresh     your browser.</p> <p></p> </li> <li> <p>You can try different operations in the share and check the audit trail in File Analytics.</p> </li> </ol>"},{"location":"file_analytics_deploy/file_analytics_deploy/","title":"Deploy File Analytics","text":""},{"location":"file_analytics_deploy/file_analytics_deploy/#overview","title":"Overview","text":"<p>In this exercise you will walk through deploying the File Analytics VM and scan the existing shares to build out the dashboard. You will also create anomaly alerts and view the audit details for your file server instance.</p> <p>If File Analytics VM is already deployed, you may skip this lab and go to Anomaly Detection to continue the exercise.</p>"},{"location":"file_analytics_deploy/file_analytics_deploy/#deploy-file-analytics_1","title":"Deploy File Analytics","text":"<ol> <li> <p>Login to Prism Element &gt; File Server &gt; click Deploy File Analytics</p> <p></p> </li> <li> <p>Choose the Files Analytics latest software.</p> <p>For the purpose of saving time, the File Analytics package 3.2.1 or latest version has already been uploaded to your cluster. Files binaries can be downloaded from the Nutanix Portal and uploaded manually.</p> <p></p> </li> <li> <p>Click Next</p> </li> <li> <p>Input the following details:</p> <ul> <li>Name - FA</li> <li>Server Size - Small (can be Small or Large based on the     number of files)</li> <li>Storage Container - Self Service Container</li> <li>Network - Primary-Managed</li> </ul> <p></p> </li> <li> <p>Select Show Advanced Settings</p> </li> <li> <p>Ensure DNS Resolver IP is set to your Active Directory,     ntnxlab.local, domain controller/DNS IP address and ONLY that     address.</p> </li> <li> <p>Choose Deploy</p> </li> <li> <p>You can monitor the deployment from the Tasks page. The     Analytics VM deployment should take \\~5 minutes.</p> </li> <li> <p>Once deployed, In Prism &gt; File Server</p> </li> <li> <p>Select your file server FS-XYZ-Prod for which you would like enable analytics and click File Analytics</p> <p></p> <p>This will open in a new tab.</p> </li> <li> <p>On the Enable File Analytic, choose the audit data retention period as Last 1 month     and the following credentials</p> <ul> <li>Active Directory Realm Name - ntnxlab.local</li> <li>User Name - ntnxlab\\administrator</li> <li>Password - nutanix/4u</li> </ul> </li> <li> <p>Click on Enable</p> <p></p> </li> </ol> <p>You have now successfully deployed Files Analytics VM and setup analytics for your files server FS-XYZ-Prod.</p>"},{"location":"file_analytics_ransomware/file_analytics_ransomware/","title":"File Analytics: Ransomware","text":""},{"location":"file_analytics_ransomware/file_analytics_ransomware/#ransomware-protection","title":"Ransomware Protection","text":"<p>In this exercise you will enable File Analytics ransomware protection and trigger a ransomware event.</p> <ol> <li> <p>In Prism Element, go File Server</p> </li> <li> <p>Select your FSXYZ-#-prod file server and click on File Analytics</p> <p></p> <p>This will open in a new tab</p> </li> <li> <p>Go to Files Analytics &gt; Main Menu Bar &gt; Ransomware</p> <p></p> </li> <li> <p>Select Enable Ransomware Protection, put your email address in the Ramsomware Email Receipts then click Enable</p> <p>Note</p> <p>You should have set SMTP service from the previous lab on Anomaly Detection, if not, you should set SMTP here. If someone put his email address in the same box, add your email address at the end and separate the email address by comma.</p> <p>!!!</p> <p> </p> </li> <li> <p>In Ransomware page, click Settings, search for *.satana,*.AngryDuck in the search box. If it is not in the signature list, click Add to add it, otherwise just close this box.</p> </li> <li> <p>Login to WinToolsVM using username : administrator@ntnxlab.local</p> </li> <li> <p>Check the following share in File Explorer: </p> <ul> <li>FSXYZ-#-prod.ntnxlab.local\\ransomwaretestXX</li> </ul> </li> <li> <p>Create a new file ransomware_test.ps1 in Downloads, open the file and put the following content in the file. Save it.       <pre><code>  new-item \\\\FSXYZ-#-prod.ntnxlab.local\\ransomwaretestXX\\IamAngry.AngryDuck -ItemType file\n  Get-ChildItem \\\\FSXYZ-#-prod.ntnxlab.local\\ransomwaretestXX\\*.txt | Rename-Item -NewName { $_.Name -replace '.txt','.satana' }\n</code></pre></p> </li> <li> <p>Right click the  (PowerShell icon) and select Run as Administrator.</p> </li> <li> <p>Run the script, which has some file creation and changing file type to both shares.</p> <pre><code>C:\\Users\\Administrator\\Downloads\\ransomware_test.ps1\n</code></pre> </li> <li> <p>Check the share FSXYZ-#-prod.ntnxlab.local\\ransomwaretestXX, you can see all files are .txt, meaning File Analytics is protecting against the operation of creating new or changing ransomware files according to the signature list.</p> </li> <li> <p>Wait until you receive email about ransomware attack alert.</p> </li> </ol> <p>Note: You can view the permission denied event in the Audit Trails as well.</p>"},{"location":"file_analytics_scan/file_analytics_scan/","title":"File Analytics: File System Scan {#file_analytics_scan}","text":""},{"location":"file_analytics_scan/file_analytics_scan/#overview","title":"Overview","text":"<p>In this exercise you will enable File Analytics and experience auditing &amp; ransomware protection; and trigger a ransomware event.</p>"},{"location":"file_analytics_scan/file_analytics_scan/#file-analytics-scan","title":"File Analytics Scan","text":"<ol> <li> <p>In Prism &gt; File Server &gt; select your BootcampFS files     server</p> </li> <li> <p>Click File Analytics</p> <p></p> </li> <li> <p>If an option appears to enable Files Analytics appears, enter your     AD details as shown below and click on Enable</p> <ul> <li>Active Directory Realm Name - ntnxlab.local</li> <li>Username - administrator@ntnxlab.local</li> <li>Password - nutanix/4u</li> </ul> <p></p> </li> <li> <p>Analytics will perform an initial scan of the existing shares which     will take just a couple minutes. You can see the scan by going to     the <code>cog</code>{.interpreted-text role=\"fa\"} icon within the Analytics UI     and selecting Scan File System</p> <p></p> </li> <li> <p>Choose Cancel to exit the scan details window</p> </li> <li> <p>After viewing the scan details, refresh your browser. You should see     the Data Age, File Distribution by Size and File     Distribution by Type dashboard panels update.</p> <p></p> </li> <li> <p>Create some audit trail activity by going to the marketing share and     opening one of the word files under Sample Data &gt; Documents</p> <p>::: note ::: title Note :::</p> <p>You may need to complete a short wizard for OpenOffice if using that application to open a file. :::</p> </li> <li> <p>Refresh the Dashboard page in your browser to see the Top 5     active users, Top 5 accessed files and File Operations     panels update</p> <p></p> </li> <li> <p>Click on your user under Top 5 active users. This will take you     to the audit trail of the user.</p> </li> <li> <p>You can also click on the <code>bars</code>{.interpreted-text role=\"fa\"} &gt;     Audit Trails menu and search for either your user or a given     file. You can use wildcards for your search, for example .doc</p> <p></p> </li> </ol>"},{"location":"file_analytics_scan/file_analytics_scan/#takeaways","title":"Takeaways","text":"<p>What are the key things you should know about Nutanix File Analytics?</p> <ul> <li>File Analytics helps you better understand how data is utilized by     your organizations to help you meet your data auditing, data access     minimization and compliance requirements.</li> </ul>"},{"location":"files_deploy/files_deploy/","title":"Files: Deploy","text":""},{"location":"files_deploy/files_deploy/#deploy-files","title":"Deploy Files","text":"<ol> <li> <p>In Prism Central &gt; Files, click + File Server to open the     Create File Server dialogue.</p> <p>Note</p> <p>Before deploying File Server in PC, you need to go to LCM to perform inventory one time to get the latest version of File Server in PC.</p> </li> <li> <p>Select your cluster and 4.4.0.1 as the File Server Version, then click Proceed.       </p> </li> <li> <p>Fill out the following fields:</p> <ul> <li>FSXYZ-A-dr (e.g. FS002-3-dr)</li> <li>Domain - ntnxlab.local</li> <li>File Server Size - 1 TiB</li> </ul> <p></p> </li> <li> <p>Click Next.</p> </li> <li> <p>Select the Primary - Managed VLAN for the Client Network.</p> <p>Each Files VM will consume a single IP on the client network.</p> Check here for ESXi Hypervisor information <p>As this is an AHV managed network, configuration of individual IPs    is not necessary. In an ESXi environment, or using an unmanaged AHV    network, you would specify the network details and available IPs as    shown below.</p> <p></p> </li> <li> <p>Click Next.</p> </li> <li> <p>Select the Primary - Managed VLAN for the Storage Network.</p> <p>Each Files VM will consume a single IP on the storage network, plus 1 additional IP for the cluster.</p> </li> <li> <p>Click Next.</p> </li> <li> <p>Leave the DNS servers and NTP servers as default and click Next.</p> </li> <li> <p>Check the configuration again and click Create to start deploying a File Server.</p> <p>Note</p> <p>PC deployed File Server do not support PD based replication and snapshot, so we do not need to set PD snapshot schedule.</p> </li> <li> <p>After the file server is created, click the name from PC &gt; Files &gt; File Servers to go to the file management console.</p> </li> <li> <p>Go to Configuration &gt; Authenication</p> </li> <li> <p>Click Use SMB Protocol and fill out the following fields:</p> <ul> <li>Select Use SMB Protocol</li> <li>Username - administrator@ntnxlab.local</li> <li>Password - nutanix/4u</li> <li>Select Make this user a File Server admin</li> </ul> </li> <li> <p>Click Use NFS Protocol</p> <ul> <li>User Management and Authentication - Unmanaged</li> <li>Select Show NFS Advanced Options</li> <li>Click Enable NFSv3 by default for all exports</li> </ul> <p></p> <p>Info</p> <pre><code>In un-managed mode, users are only identified by UID/GID.\n</code></pre> </li> <li> <p>Click Update.</p> </li> <li> <p>Go to Data Management &gt; Protection and then Self Service Restore</p> <p></p> </li> <li> <p>Observe the default Self Service Restore schedules, this feature controls the snapshot schedule for Windows\\' Previous Versions functionality. Supporting Previous Versions allows end users to roll back changes to files without engaging storage or backup administrators. Note these local snapshots do not protect the file server cluster from local failures and that replication of the entire file server cluster can be performed to remote Nutanix clusters.</p> </li> </ol>"},{"location":"files_deploy/files_deploy/#takeaways","title":"Takeaways","text":"<p>What are the key things you should know about Nutanix Files?</p> <ul> <li>Files can be rapidly deployed on top of existing Nutanix clusters,     providing SMB and NFS storage for user shares, home directories,     departmental shares, applications, and any other general purpose     file storage needs.</li> <li>Files is not a point solution. VM, File, Block, and Object storage     can all be delivered by the same platform using the same management     tools, reducing complexity and management silos.</li> </ul>"},{"location":"files_deploy/files_deploy_pe/","title":"Files: Deploy","text":""},{"location":"files_deploy/files_deploy_pe/#deploy-files","title":"Deploy Files","text":"<ol> <li> <p>In Prism Element &gt; File Server, click + File Server to open the     Create File Server dialogue.</p> <p>Note</p> <p>As we have deployed a file server in the cluster, it will use the current FS version to deploy the new file server. In a normal deployment if this is the first file server to create, you will need to go through a pre-check and select Files version from a list of available compatible versions. You can also choose to manually upload a File version to start the deployment.</p> </li> <li> <p>Fill out the following fields:</p> <ul> <li>FSXYZ-A-dr (e.g. FS002-3-dr)</li> <li>Domain - ntnxlab.local</li> <li>File Server Size - 1 TiB</li> </ul> <p></p> </li> <li> <p>Click Next.</p> </li> <li> <p>Select the Primary - Managed VLAN for the Client Network.</p> <p>Each Files VM will consume a single IP on the client network.</p> <p>Note</p> <p>It is typically desirable in production environments to deploy Files with dedicated virtual networks for client and storage traffic. When using two networks, Files will, by design, disallow client traffic the storage network, meaning VMs assigned to the primary network will be unable to access shares.</p> Check here for ESXi Hypervisor information <p>As this is an AHV managed network, configuration of individual IPs    is not necessary. In an ESXi environment, or using an unmanaged AHV    network, you would specify the network details and available IPs as    shown below.</p> <p></p> </li> <li> <p>Specify your cluster's Domain Controller VM IP (look for     AutoAD VM IP address in Prism Element) as the DNS Resolver     IP (e.g. 10.XX.YY.41)). Leave the default (cluster) NTP Server.</p> <p>Danger</p> <pre><code> In order for the Files cluster to successfully find and join the NTNXLAB.local domain it is critical that the DNS Resolver IP is set to the Domain Controller VM IP FOR YOUR CLUSTER. By default, this field is set to the primary Name Server IP configured for the Nutanix cluster, this value is incorrect and will not work.\n\n ![](images/7.png)\n</code></pre> </li> <li> <p>Click Next.</p> </li> <li> <p>Select the Primary - Managed VLAN for the Storage Network.</p> <p>Each Files VM will consume a single IP on the storage network, plus 1 additional IP for the cluster.</p> <p></p> </li> <li> <p>Click Next.</p> </li> <li> <p>Fill out the following fields:</p> <ul> <li>Select Use SMB Protocol</li> <li>Username - administrator@ntnxlab.local</li> <li>Password - nutanix/4u</li> <li>Select Make this user a File Server admin</li> <li>Select Use NFS Protocol</li> <li>User Management and Authentication - Unmanaged</li> <li>Select Show NFS Advanced Options</li> <li>For 4-node HPOC only: NFS protocol version - Enable NFSv4 by default for all     exports</li> </ul> <p></p> <p></p> <p>Info</p> <pre><code>In un-managed mode, users are only identified by UID/GID. Starting from Files 3.5, Files supports both NFSv3 and NFSv4.\n</code></pre> </li> <li> <p>Click Next.</p> <p>By default, Files will automatically create a Protection Domain to take daily snapshots of the Files cluster and retain the previous 2 snapshots. After deployment, the snapshot schedule can be modified and remote replication sites can be defined.</p> <p></p> </li> <li> <p>Click Create to begin the Files deployment.</p> </li> <li> <p>Monitor deployment progress in Prism &gt; Tasks.</p> <p>Deployment should take approximately 10 minutes. </p> <p>Note</p> <pre><code>You should start working on the next lab while the File Server is creating.\n</code></pre> <p></p> Having trouble with File Server not joining Active Directory? <p>If you accidentally did not configure Files to use the Active Directory domain controller (AutoAD or customer-provided) as the DNS server, after deploying the File Server you will get the following errors.</p> <ul> <li>DNS 'NS' records not found for domain</li> <li>Failed to lookup IP address of domain. Please verify the       domain name, DNS configuration and network connectivity.</li> </ul> <p>This can easily be corrected after deployment, without having to delete and redeploy the Files Server.</p> <ul> <li>Launch File Console, go to Configuration &gt; Authentication &gt; Directory Services</li> <li>Click Use SMB Protocol, put ntnxlab.local in Active Directory Realm Name</li> <li>Put administrator@ntnxlab.local and nutanix/4u as username and password</li> <li>Select Make this user a File Server admin</li> </ul> </li> <li> <p>Go to Prism &gt; File Server and select the FSXYZ-A-dr     server and click Protect.</p> <p></p> </li> <li> <p>Observe the default Self Service Restore schedules, this feature controls the snapshot schedule for Windows\\' Previous Versions functionality. Supporting Previous Versions allows end users to roll back changes to files without engaging storage or backup administrators. Note these local snapshots do not protect the file server cluster from local failures and that replication of the entire file server cluster can be performed to remote Nutanix clusters. Click Close.</p> <p></p> </li> </ol>"},{"location":"files_deploy/files_deploy_pe/#takeaways","title":"Takeaways","text":"<p>What are the key things you should know about Nutanix Files?</p> <ul> <li>Files can be rapidly deployed on top of existing Nutanix clusters,     providing SMB and NFS storage for user shares, home directories,     departmental shares, applications, and any other general purpose     file storage needs.</li> <li>Files is not a point solution. VM, File, Block, and Object storage     can all be delivered by the same platform using the same management     tools, reducing complexity and management silos.</li> </ul>"},{"location":"files_expand_cluster/files_expand_cluster/","title":"Files: Expand Cluster","text":""},{"location":"files_expand_cluster/files_expand_cluster/#overview","title":"Overview","text":"<p>Warning</p> <p>This lab can only be done on a HPOC with more than 1 node. Single Node    HPOC clusters cannot be used to run this lab.</p> <p>Note that the number of Files Server VMs is limited to number of    physical nodes in a Nutanix cluster. Most HPOCs will have 3 to 4 nodes.</p> <p>At the time of Files Server 4.0.0.2 release, a Files Server with just 1    FSVM cannot be scaled out.</p> <p>Files offers the ability to scale up and scale out a deployment. Scaling up the CPU and memory of Files VMs allows an environment to support higher storage throughput and number of concurrent sessions. Currently, Files VMs can be scaled up to a maximum of 12 vCPU and 96GB of RAM each.</p> <p>The true power of Files scalability is the ability to simply add more Files VMs, scaling out much like the underlying Nutanix distributed storage fabric. An individual Files cluster can scale out up to the number of physical nodes in the Nutanix cluster, ensuring that no more than 1 Files VM runs on a single node during normal operation.</p> <p>In this exercise we will scale out the Intials-Files Files Server (e.g. XYZ-Files) server we deployed in optional lab Deploy Nutanix Files</p>"},{"location":"files_expand_cluster/files_expand_cluster/#expanding-a-files-cluster","title":"Expanding a Files Cluster","text":"<ol> <li> <p>Return to Prism &gt; File Server and select your Intials-Files     Files Server (e.g. xyz-Files).</p> </li> <li> <p>Click Update &gt; Scale in/out FSVMs.</p> <p></p> </li> <li> <p>Increment the number of Files VMs from 3 to 4 and click Next.</p> <p></p> <p>Note that an additional IP will be consumed for both the client and storage networks to support the added Files VM.</p> </li> <li> <p>Click Next &gt; Save.</p> <p>The cluster will now deploy and power on a 4th Files VM. Status can be monitored in Prism &gt; Tasks.</p> <p>Note</p> <p>Files cluster expansion should take approximately 10 minutes to    complete.</p> <p>Following the expansion, verify client connections can now be load    balanced to the new VM.</p> </li> <li> <p>Connect to your Initials-ToolsVM via RDP or console.</p> </li> <li> <p>Open Control Panel &gt; Administrative Tools &gt; DNS.</p> </li> <li> <p>Fill out the following fields and click OK:</p> <ul> <li>Select The following computer</li> <li>Specify dc.ntnxlab.local</li> <li>Select Connect to the specified computer now</li> </ul> <p></p> </li> <li> <p>Open DC.ntnxlab.local &gt; Forward Lookup Zones &gt; ntnxlab.local     and verify there are now four entries for BootcampFS. Files     leverages round robin DNS to load balance connections across Files     VMs.</p> <p></p> <p>Note</p> <p>If only three entries are present, you can automatically update DNS    entries from Prism &gt; File Server by selecting your Files    cluster and clicking DNS.</p> </li> </ol>"},{"location":"files_expand_cluster/files_expand_cluster/#takeaways","title":"Takeaways","text":"<p>What are the key things you should know about Nutanix Files?</p> <p>Files can scale up and scale out with One Click performance optimization.</p>"},{"location":"files_file_blocking/files_file_blocking/","title":"Files: File Blocking {#files_file_blocking}","text":""},{"location":"files_file_blocking/files_file_blocking/#selective-file-blocking","title":"Selective File Blocking","text":"<p>In this exercise you will configure Files to block specific file extensions for the file server and the Marketing share.</p> <ol> <li> <p>In Prism Element &gt; File Server, select your file server and     click Launch Files Console.</p> </li> <li> <p>The Files Console will open in a new browser tab</p> </li> <li> <p>Click on Configuration &gt; Authentication</p> </li> <li> <p>Under Blocked File Types enter a comma separated list of     extensions like .flv,.mov and click Save</p> <p></p> </li> <li> <p>In your Windows Tools VM, open a PowerShell window by clicking on     the PowerShell icon on the taskbar. Enter the following command     where you will see an access denied error message:</p> <pre><code>new-item \\\\BootcampFS.ntnxlab.local\\xyz-marketing\\MyMovie.flv -ItemType file\n</code></pre> <p></p> </li> <li> <p>In Prism Element &gt; File Server, click on your BootcampFS File     Server</p> </li> <li> <p>Click on Launch Files Console</p> </li> <li> <p>Click on three dots on the right hand corner of the     Initials-Marketing share and select Edit</p> <p></p> </li> <li> <p>Select Next to get to the Settings page.</p> </li> <li> <p>Check Blocked File Types and enter .none as a file extension.</p> <p></p> </li> <li> <p>Select Next then Update on the Summary page to complete     the update.</p> </li> <li> <p>Blocked file type settings at the share level override the server     level setting. Using PowerShell issue the same command as the     previous step. The command will now complete successfully.</p> <p></p> </li> </ol>"},{"location":"files_manager/files_manager/","title":"Files: Files Manager","text":"<p>Smart DR feature for Files share replication is activated and maintained in Prism Central using Files Manager. In this section we will configure Smart DR requirements in Prism Central.</p> <p>The Files Manager lets you view and control all of your file servers from a single control plane. Clicking a file server directs you to Nutanix Files in Prism Element (PE) where you can manage the shares, exports, and configurations of the file server. File server alerts for all registered file servers appear in a single pane for consolidated viewing, as do file server events.</p> <p>The Files Manager provides the Smart DR service for Nutanix Files, which lets you protect file servers at the share-level.</p>"},{"location":"files_manager/files_manager/#enabling-the-files-manager","title":"Enabling the Files Manager","text":"<p>Enable the Files Manager service on PC</p> <ol> <li> <p>Log on to Prism Central</p> </li> <li> <p>Click the collapsed menu icon</p> </li> <li> <p>In the Services &gt; Files</p> <p></p> </li> <li> <p>Read the information on enabling the Files Manager and click     Enable Files</p> </li> <li> <p>In the Enable Service dialog box, click Enable (if not already     enabled)</p> </li> </ol> <p>Unable to see any new features (Data Protection)?</p> <p>Upgrading Files Manager in LCM might help. See the section below to upgrade Files Manager in LCM.</p>"},{"location":"files_manager/files_manager/#upgrading-files-manager","title":"Upgrading Files Manager","text":"<p>Files Manger introduces new features and bug fixes with updates. For example, Files Manager version 2.0.0 introduced Smart DR (Disaster Recover) feature.</p> <p>You can perform Files Manager updates using the Life-Cycle Manager (LCM).</p> <p>We will verify the current version of Files Manager in Life Cycle Management.</p> <ol> <li> <p>In Prism Central, use the search window to search for Lcm</p> </li> <li> <p>Select Lcm from the search result</p> </li> <li> <p>In LCM, click on Inventory</p> </li> <li> <p>If Inventory doesn\\'t show any existing software versions, click     on Perform Inventory</p> <p></p> </li> <li> <p>Click on Proceed in the confirmation window</p> <p>Info</p> <pre><code>The Inventory operation may take up to 30 minutes\n</code></pre> </li> <li> <p>Once the Inventory operation is finished, Click on LCM &gt; Updates     &gt; Software to check the available updates for software in your     Nutanix clusters</p> <p></p> </li> <li> <p>Here you can notice that Files Manager version <code>2.0.0</code> is available     and you can also see that the current version <code>1.0.1</code></p> </li> <li> <p>Select the check-box nex to Files Manager and click on     Update</p> <p></p> </li> <li> <p>Life Cycle Manager will now generate update plans and present an     option to apply updates.</p> <p></p> </li> <li> <p>Click on Apply 1 Updates</p> <p>Note</p> <p>The number of updates to apply might vary from your choice of    software that you choose to upgrade. Since Files Manager is the only    software we are upgrading in this labs, so it is showing 1 update.</p> </li> <li> <p>Confirm that the updates are succesful</p> <p></p> </li> <li> <p>After the updates are applied, go back to Life Cycle Manager and     perform and Inventory</p> </li> <li> <p>Verify that Files Manager is of version <code>2.0.0</code> (or whichever is     latest at the time).</p> <p></p> </li> <li> <p>Go back to Prism Central &gt; Services &gt; Files</p> </li> <li> <p>Verify that the Data Protection features are present</p> <p></p> </li> </ol>"},{"location":"files_migrate/files_migrate/","title":"Files: Share Migration","text":""},{"location":"files_migrate/files_migrate/#overview","title":"Overview","text":"<p>Many customers have existing file shares on existing NAS infrastructure that need to be migrated to Nutanix Files. A lot of time they will leverage free migration tools like robocopy to migrate SMB shares. The common challenges of using common tools are the migration speed and complex management. </p> <p>Starting from Nutanix Files 4.0.2, nutanix includes in-build migration tool. This feature will help customers migrate from existing SMB shares to Nutanix Files flawlessly by taking advantage of Nutnaix Files' internal architecture, providing a robust and optimize migration solution.</p> <p>Lets now explore the migration Tool</p>"},{"location":"files_migrate/files_migrate/#lab-preparation","title":"Lab Preparation","text":"<p>In your WinTools VM, there is a share my_secret mapped under M: drive. We will use this as an existing share to migrate to a remote Nutanix Files.</p> <ol> <li>Connect to your Initials-WinToolsVM via RDP or console as NTNXLAB\\Administrator user</li> <li>Open File Explorer and go to *M:* to check the existing share.</li> <li>Right click the folder confidential &gt; Security &gt; Edit</li> <li>Add user01 to have read access to this folder.    </li> <li>Use putty or any ssh session tool to ssh to FSVM through the File Server client network.</li> <li>Username : nutanix</li> <li> <p>Password : nutanix/4u</p> <p>Note</p> <p>The FSVM should have enabled ssh from client network. If you have difficulty ssh directly to the FSVM, you can choose to ssh to FSVM from CVM CLI.</p> </li> </ol>"},{"location":"files_migrate/files_migrate/#share-migration","title":"Share Migration","text":"<ol> <li>SSH to FSVM through client network IP address.</li> <li>input command : afs</li> <li> <p>Run the following command to create migration source :       <pre><code>migration.source_add source_alias=[sourcename] source_fs_fqdn=[sourceshareIP] source_user=[domainuser]\n</code></pre></p> <p></p> <p>Note</p> <pre><code>The domainuser must have **backup operator role** of the source share. Here in this lab we will use a domain administrator as the source user.\n\nThe domain user should be in the format of **domainname\\\\\\\\user**. e.g. **NTNXLAB\\\\\\\\administrator**\n</code></pre> </li> <li> <p>Once the migration source is added, you can verify it by running :          <pre><code>afs migration.source_list\n</code></pre></p> </li> <li> <p>Then, create a migration plan by runnig this command :         <pre><code>migration.plan_create name=[planname] source_alias=[source name used in step 3] source_share=[sourceshareIP] target_share=[xyz-GSO] migrate=false\n</code></pre> </p> <p>Note</p> <pre><code>If you want to start the migration immediately after the creation of the plan, you can use **migrate=true** at the end of the command.\n</code></pre> </li> <li> <p>Start migration by running this command :      <pre><code>migration.plan_migrate name=[planname used in step 5]\n</code></pre></p> </li> <li> <p>Check the status of the migration :       <pre><code>migration.plan_status name=[planname used in step 5] verbose=true\n</code></pre></p> </li> <li> <p>You may find the migration has failed with the following reason, this is because the share contains user SID that cannot be found in the domain :          </p> </li> <li> <p>To solve this problem, we need to run the migration plan again with an unknown user SIDs attribute in the share :       <pre><code>migration.plan_migrate name=[planname used in step 5] allow_unknown_sids=true\n</code></pre></p> </li> <li> <p>After a few minutes, check the status of the migration.          <pre><code>migration.plan_status name=[planname used in step 5] verbose=true\n</code></pre>         You should see the status to be succeeded this time.         </p> </li> <li> <p>Go back to the file explore of WinTools VM, right click the folder confidential and go to Properties &gt; Security, you can see user01 has read access to this folder. This means the permission has successfully migrated to the target share.         </p> </li> <li> <p>Now let's try to rename and delete some files and do a delta sync of the migration. Go to WinTools VM &gt; M: drive in file explorer. Now do the following :</p> <ul> <li>Delete the file HPE GL Webinar - 20210729.pdf</li> <li>Rename folder confidential to \"Open To Public\"</li> </ul> </li> <li> <p>SSH to FSVM again, run the following command to start migration again and check for status.      <pre><code>   migration.plan_migrate name=[planname used in step 5] allow_unknown_sids=true\n\n   migration.plan_status name=[planname used in step 5] verbose=true\n</code></pre></p> </li> <li> <p>You can see this time only 1 directory and 4 diles are transferred, it shows only changes are migrated instead of full copy.         </p> </li> <li> <p>Go back to WinTools VM to verify the updates on the target share xyz-GSO.</p> </li> </ol>"},{"location":"files_migrate/files_migrate/#takeaway","title":"Takeaway","text":"<p>Files Share Migraiton provides a native tool to migrate SMB shares to Nutanix Files. It does not require any additional cost, installation of software or any complex management. </p>"},{"location":"files_migrate/files_migrate_move/","title":"Files: Share Migration","text":""},{"location":"files_migrate/files_migrate_move/#overview","title":"Overview","text":"<p>Many customers have existing file shares on existing NAS infrastructure that need to be migrated to Nutanix Files. A lot of time they will leverage free migration tools like robocopy to migrate SMB shares. The common challenges of using common tools are the migration speed and complex management. </p> <p>Nutanix offers Nutanix Move tool which will enable migrating files to a new files server. Move also helps in moving virtual machines, but this lab will focus on Move's file migration capabilities.</p> <p>Move also provides a Web UI which could be useful in managing migration of many shares. </p> <p>Lets now explore the Move Files Migration Tool.</p>"},{"location":"files_migrate/files_migrate_move/#lab-preparation","title":"Lab Preparation","text":"<p>We will download a zip file, extract it and use this a source share.</p> <ol> <li> <p>Connect to your Initials-WinToolsVM (E.g User01-WinToolsVM) via RDP or console with the following credentials:</p> <ul> <li>Username - ntnxlab\\administrator  </li> <li>Password - default (refer to HPOC lookup tool)</li> </ul> </li> <li> <p>Download and extract the following folder (right-click and download) using Chrome browser or <code>curl</code> tool</p> </li> <li> <p>Right click on the extracted folder Properties &gt; Sharing</p> </li> <li> <p>Click on Advanced Sharing</p> </li> <li> <p>Select the Share this folder check box</p> <p></p> </li> <li> <p>Click on Apply</p> </li> <li>Click on OK</li> </ol>"},{"location":"files_migrate/files_migrate_move/#setup-files-server-api-permissions","title":"Setup Files Server API Permissions","text":"<ol> <li> <p>In File Console, Configuration &gt; Manage Roles </p> </li> <li> <p>Under REST API access users, click + New User</p> <ul> <li>Username - ntnxlab\\administrator  </li> <li>Password - default (refer to HPOC lookup tool) </li> </ul> </li> <li> <p>Click the check  button to save it</p> <p></p> </li> </ol>"},{"location":"files_migrate/files_migrate_move/#setup-move-for-files-migration","title":"Setup Move for Files Migration","text":""},{"location":"files_migrate/files_migrate_move/#adding-target-file-server","title":"Adding Target File Server","text":"<p>A target file server has been provisioned for you. Refer to the HPOC lookup tool for target file server.</p> <ol> <li> <p>In your WinTools VM, open Chrome browser and type the IP address(find the IP address from the HPOC lookup tool) of the Move VM </p> </li> <li> <p>Logon using the following credentials</p> <ul> <li>Username: nutanix</li> <li>Password: default (refer to HPOC lookup tool)</li> </ul> </li> <li> <p>Choose Files as the Migration Type</p> </li> <li> <p>Click on +New Migration Plan</p> </li> <li> <p>Use MP-userYY (E.g. MP-user01) as the name of your migration plan</p> </li> </ol> <p>Note</p> <p>Another user using the same target file server might have already configured it. </p> <p>Select your target file server from the drop-down menu if it is already configured.</p> <p>If this is the first time to login and without source and target file server, do the following: </p> <ol> <li> <p>Click on + Add New Target </p> </li> <li> <p>Enter the following details:</p> <ul> <li>Name - UserXX-Target (e.g User01-Target)</li> <li>FQDN Details/Address -  POCXXX-X.ntnxlab.local (e.g.POC096-3.ntnxlab.local)</li> <li>Username - ntnxlab\\administrator  (this is the REST API access user)</li> <li>Password - default (refer to HPOC lookup tool)</li> </ul> </li> <li> <p>Click Add</p> </li> </ol>"},{"location":"files_migrate/files_migrate_move/#adding-source-file-server","title":"Adding Source File Server","text":"<ol> <li> <p>Click on + Add New Source </p> </li> <li> <p>Enter the following details:</p> <ul> <li>Name - UserXX-Source (e.g User01-Source)</li> <li>FQDN Details/Address -  X.X.X.X (IP address of your UserXX-WinToolsVM e.g. 10.42.81.116)</li> <li>Username - ntnxlab\\administrator  (this is the REST API access user)</li> <li>Password - default (refer to HPOC lookup tool)</li> </ul> <p></p> </li> <li> <p>Click Add</p> </li> </ol>"},{"location":"files_migrate/files_migrate_move/#performing-migration","title":"Performing Migration","text":"<p>Move UI will now present the wizard to configure the migration plan.</p> <ol> <li> <p>Select the target and source (UserXX-WinToolsVM) file servers</p> </li> <li> <p>Select Mirror as the Replication Type</p> </li> <li> <p>Click on + Add Share and fill the following details:</p> <ul> <li>Source - SampleData_Small</li> <li>Target - usershareXX (e.g. usershare01 depending on your username)</li> </ul> <p></p> </li> <li> <p>Click on the  check mark </p> </li> <li> <p>Click on Next</p> <p>The wizard will now go through a validation phase.</p> </li> <li> <p>Click Next on the Migration Settings page of the wizard</p> </li> <li> <p>Confirm the migration details and click on Save and Start</p> </li> <li> <p>Monitor the progress of the migration by click on the Status message until it is ready to cutover</p> </li> <li> <p>Select the share &gt; Actions &gt; Cutover, the migration will be marked as completed</p> <p>Info</p> <p>Cutover simply means that we can no longer re-sync from source to target.</p> </li> </ol> <p>Using Move multiple shares (from different sources and targets) can be managed from a single GUI.</p>"},{"location":"files_multiprotocol/files_multiprotocol/","title":"Files: Multi-Protocol","text":""},{"location":"files_multiprotocol/files_multiprotocol/#multi-protocol","title":"Multi-protocol","text":"<p>In this exercise you will configure an existing SMB share to also support NFS. Enabling multi-protocol access requires you to configure user mappings and define the native and non-native protocol for a share.</p>"},{"location":"files_multiprotocol/files_multiprotocol/#configure-user-mappings","title":"Configure User Mappings","text":"<p>A Nutanix Files share has the concept of a native and non-native protocol. All permissions are applied using the native protocol. Any access requests using the non-native protocol requires a user or group mapping to the permission applied from the native side. There are several ways to apply user and group mappings including rule based, explicit and default mappings. You will first configure a default mapping.</p> <ol> <li> <p>In Prism Element &gt; File Server, select your file server and     click Launch Files Console.</p> </li> <li> <p>The Files Console will open in a new browser tab</p> </li> <li> <p>Click on Configuration &gt; Authentication</p> </li> <li> <p>Select the User Mapping tab</p> </li> <li> <p>In the Default Mapping click on Edit</p> <p></p> </li> <li> <p>From the Default Mapping page choose both Deny access to NFS     export and Deny access to SMB share as the defaults for when     no mapping is found.</p> <p></p> </li> <li> <p>Complete the initial mapping by clicking on Save</p> </li> <li> <p>In your BootcampFS' Files Console, click on Shares &gt;     Initials-Marketing share</p> </li> <li> <p>Click on three dots on the right hand corner of the     Initials-Marketing share and select Edit</p> </li> <li> <p>From the Update Share page check the box at the bottom which     says Enable multiprotocol access for NFS.</p> <p></p> </li> <li> <p>Click Next then from the Settings page check Allow     Simultaneous access to the same files from both protocols.</p> <p></p> </li> <li> <p>Click Next and then Update from the Summary page.</p> </li> </ol> <p>This concludes the multiprotocol configuration for your Initials-Marketing share</p> <p>Now we will try accessing the share to test multiprotocol configuration</p> <p>You will use a Linux Tools VM as a client for your NFS Files export.</p> <ol> <li> <p>Note the IP address of the VM in Prism, and connect via SSH using     the following credentials:</p> <ul> <li>Username - root</li> <li>Password - ask your instructor</li> </ul> </li> <li> <p>Execute the following:</p> <pre><code>[root@CentOS ~]# mkdir /filesmulti\n[root@CentOS ~]# mount.nfs4 BootcampFS.ntnxlab.local:/Intials-Marketing /filesmulti\n[root@CentOS ~]# dir /filesmulti\n\ndir: cannot open directory /filesmulti: Permission denied\n[root@CentOS ~]#\n</code></pre> <p>Note</p> <p>The mount operation is case sensitive.</p> <p>Because the default mapping is to deny access the Permission denied error is expected. You will now add an explicit mapping to allow access to the non-native NFS protocol user. We will need to get the user ID (UID) to create the explicit mapping.</p> </li> <li> <p>Execute the following command and take note of the UID:</p> <pre><code>[root@CentOS ~]# id\nuid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023\n</code></pre> </li> <li> <p>In Prism Element &gt; File Server, select your file server     (Initials-Marketing)and click Launch Files Console.</p> </li> <li> <p>The Files Console for your share will open in a new browser tab</p> </li> <li> <p>Click on Configuration &gt; Authentication</p> </li> <li> <p>Select the User Mapping tab</p> </li> <li> <p>Click on Configure on the Explicit Mapping pop out</p> </li> <li> <p>Click on + Add Manually</p> </li> <li> <p>Click + Add one-to-one mapping</p> </li> <li> <p>Fill out the following fields:</p> <ul> <li>SMB Name - ntnxlab\\administrator</li> <li>NFS ID - UID from previous step (0 if root)</li> <li>User/Group - User</li> </ul> </li> <li> <p>Click on the  mark to save the mapping</p> <p></p> </li> <li> <p>Click on Save in the Explicit Mapping pop out</p> </li> <li> <p>Click Close if any pop-ups are left</p> </li> <li> <p>Go back to your Linux Tools VM and execute the following:</p> <pre><code>[root@CentOS ~]# dir filesmulti\nMyMovie.flv Sample Data\n</code></pre> </li> </ol> <p>You have been able to successfully configure multiprotocol access for your Initials-Marketing share</p>"},{"location":"files_nfs_export/files_nfs_export/","title":"Files: Create NFS Export","text":""},{"location":"files_nfs_export/files_nfs_export/#overview","title":"Overview","text":"<p>In this exercise you will create and test a NFSv4 export, used to support clustered applications, store application data such as logging, or storing other unstructured file data commonly accessed by Linux clients.</p>"},{"location":"files_nfs_export/files_nfs_export/#using-nfs-exports","title":"Using NFS Exports","text":""},{"location":"files_nfs_export/files_nfs_export/#enabling-nfs-protocol","title":"Enabling NFS Protocol","text":"<p>Note</p> <p>Enabling NFS protocol only needs to be performed once per Files server,    and may have already been completed in your environment. If NFS is    already enabled, proceed to Creating the Export.</p> <ol> <li> <p>In Prism Element &gt; File Server, select your file server and     click Launch Files Console.</p> <p></p> </li> <li> <p>The Files Console will open in a new browser tab</p> </li> <li> <p>Go to Configuration &gt; Authentication &gt; Directory     Services</p> </li> <li> <p>Select Use NFS Protocol with Unmanaged User Management and     Authentication, and click Update.</p> </li> <li> <p>Uncheck Enable NFSv3 by default for all exports</p> </li> <li> <p>Check Enable NFSv4 by default for all exports as we will only     test NFS4 exports in this lab</p> <p></p> </li> <li> <p>Click on Update</p> </li> </ol>"},{"location":"files_nfs_export/files_nfs_export/#creating-the-export","title":"Creating the Export","text":"<ol> <li> <p>In Files Console</p> </li> <li> <p>Select Shares and click on Create a New Share</p> </li> <li> <p>Fill out the following fields:</p> <ul> <li>Name - Initials-logs</li> <li>Description (Optional) - File share for system logs</li> <li>Share Path (Optional) - Leave blank</li> <li>Max Size (Optional) - Leave blank</li> <li>Select Protocol - NFS</li> </ul> <p></p> </li> <li> <p>Click Next.</p> </li> <li> <p>Fill out the following fields:</p> <ul> <li>Select Enable Self Service Restore<ul> <li>These snapshots appear as a .snapshot directory for NFS     clients.</li> </ul> </li> <li>Authentication - System</li> <li>Default Access (For All Clients) - No Access<ul> <li>Click on Add Exceptions</li> </ul> </li> <li>Clients with Read-Write Access - The first 3 octets of your     cluster network. (e.g. <code>10.38.188.*</code>)</li> </ul> <p></p> <p>By default an NFS export will allow read/write access to any host that mounts the export, but this can be restricted to specific IPs or IP ranges.</p> </li> <li> <p>Click Next.</p> </li> <li> <p>Review the Summary and click Create.</p> </li> </ol>"},{"location":"files_nfs_export/files_nfs_export/#testing-the-export","title":"Testing the Export","text":"<p>You will use a Linux Tools VM as a client for your NFS Files export.</p> <ol> <li> <p>Note the IP address of the VM in Prism, and connect via SSH using     the following credentials:</p> <ul> <li>Username - root</li> <li>Password - nutanix/4u</li> </ul> </li> <li> <p>Execute the following:</p> <pre><code>[root@centos ~]# sudo su - centos\n[centos@centos ~]# sudo yum install -y nfs-utils #This installs the NFSv4 client\n[centos@centos ~]# mkdir /home/centos/filesmnt\n[centos@centos ~]# sudo mount.nfs4 BootcampFS.ntnxlab.local:/xyz-logs /home/centos/filesmnt\n[centos@centos ~]# df -kh\nFilesystem                      Size  Used Avail Use% Mounted on\n/dev/mapper/centos_centos-root  8.5G  1.7G  6.8G  20% /\ndevtmpfs                        1.9G     0  1.9G   0% /dev\ntmpfs                           1.9G     0  1.9G   0% /dev/shm\ntmpfs                           1.9G   17M  1.9G   1% /run\ntmpfs                           1.9G     0  1.9G   0% /sys/fs/cgroup\n/dev/sda1                       494M  141M  353M  29% /boot\ntmpfs                           377M     0  377M   0% /run/user/0\nBootcampFS.ntnxlab.local:/      1.0T  7.0M  1.0T   1% /home/centos/filesmnt\n\n[centos@centos ~]# ll /home/centos/filesmnt/\ntotal 0\n</code></pre> </li> <li> <p>Observe that the logs NFS share is mounted in <code>/home/centos/filesmnt</code>.</p> </li> <li> <p>Reboot the VM and observe the export is no longer mounted. To     persist the mount, add it to <code>/etc/fstab</code> by executing the     following:</p> <pre><code>[root@centos ~]# exit # centos user logout \n[root@centos ~]# reboot \n</code></pre> <pre><code># Login to your LinuxToolsVM again\n[yourdesktop ~]# ssh -l root &lt;LinuxToolsVMIPAddress&gt; \n</code></pre> <pre><code>[root@centos ~]# echo 'FSxyz-a-prod.ntnxlab.local:/xyz-logs /home/centos/filesmnt nfs4' &gt;&gt; /etc/fstab\n[root@centos ~]# mount /home/centos/filesmnt\n</code></pre> </li> <li> <p>Once an mount entry is added to <code>/etc/fstab</code>, reboot the VM again.     This is required in some cases where mounts don\\'t persist.</p> </li> <li> <p>The following command will add 100 2MB files filled with random data     to <code>/filesmnt/logs</code>:</p> <pre><code>[root@centos ~]# sudo su - centos\n\n[centos@centos ~]# mkdir /home/centos/filesmnt/host1\n[centos@centos ~]# for i in {1..100}; do dd if=/dev/urandom bs=8k count=256 of=/home/centos/filesmnt/host1/file$i; done\n</code></pre> </li> <li> <p>Return to Files Console</p> </li> <li> <p>Click on Shares &gt; logs to monitor performance and usage of you     NFS export.</p> <p>Note</p> <pre><code>The utilization data is updated every 10 minutes.\n\n![](images/26b.png)\n</code></pre> </li> </ol>"},{"location":"files_replication/files_replication/","title":"Files Replication","text":"<p>Info</p> <pre><code>The estimated time to complete this lab is 60 minutes\n</code></pre>"},{"location":"files_replication/files_replication/#overview","title":"Overview","text":"<p>Smart disaster recovery (DR) is a data protection service for Nutanix Files.</p> <p>Smart DR facilitates share-level data replication and file-server-level disaster recovery. In the event of a planned or unplanned loss of service, you can restore write access to protected shares by failing-over to a recovery site file server. Protection policies indicate failover details, including the primary location, recovery location, and the replication schedule. Rather than having a single protection policy for an entire file server, you can configure unique policies for different shares.</p> <p>A short recovery time objective (RTO), helps ensure continuous availability of data once you fail over to a recovery site. Configuring Active Directory (AD) and domain name system (DNS) entries facilitates seamless client access redirection.</p> <p>The following figure gives an overview implementation of Replication in Nutanix Files.</p> <p></p>"},{"location":"files_replication/files_replication/#typical-data-protection-setup-for-smart-dr","title":"Typical Data Protection Setup for Smart DR","text":"<p>The data protection process consists of the following procedures:</p> <ul> <li>Configuring a Files Protection Policy to replicate share data to a recovery site</li> <li>Configuring AD and DNS access for seamless client failover (if AD is different)</li> <li>Configuring a reverse replication policy</li> <li>Performing a planned or unplanned failover to DR site</li> <li>Failing back to the primary site</li> </ul>"},{"location":"files_replication/files_replication/#lab-setup","title":"Lab Setup","text":"<p>We have already created File Server in the DR site and migrated files from existing share to xyz-GSO in the previous labs. We now will replicate the xyz-GSO share to the DR File Server.</p> <p>Note</p> <pre><code>Make sure you have finished the labs of \"Deploy Nutanix Files\", \"Create SMB Share\" &amp; \"Share Migration\" before this lab.\n</code></pre>"},{"location":"files_replication/files_replication/#configure-files-protection-policy-in-prism-central-and-replicate","title":"Configure Files Protection Policy in Prism Central and Replicate","text":"<p>Smart DR feature for Files share replication is activated and maintained in Prism Central using Files Manager. In this section we will configure Smart DR requirements in Prism Central.</p> <p>The Files Manager lets you view and control all of your file servers from a single control plane. Clicking a file server directs you to Nutanix Files in Prism Element (PE) where you can manage the shares, exports, and configurations of the file server. File server alerts for all registered file servers appear in a single pane for consolidated viewing, as do file server events.</p> <p>The Files Manager provides the Smart DR service for Nutanix Files, which lets you protect file servers at the share-level.</p> <ol> <li> <p>Logon to Prism Central</p> </li> <li> <p>Click on  &gt; Services &gt; Files</p> Can't see Files Manager? <p>If Files Manager is not enabled in your Prism Central, you will need    to Enable Files using instruction in Files Manager</p> <p></p> </li> <li> <p>In Files Manager, click on Data Protection &gt; Polices &gt; + New Policy</p> </li> <li> <p>Select your FSXYZ-#-prod as the Primary Location (Source File Server)</p> </li> <li> <p>Select usershareXX as the Shares to be protected</p> <p>Note</p> <p>Selecting the source Files server will automatically select all the shares within this files server to be protected</p> </li> <li> <p>Select your FSXYZ-#-dr (e.g. FS002-3-dr) as the Recovery Location (Target File Server)</p> </li> <li> <p>Select the Recovery Point Objective (RPO) as 2 minutes and     Start Immediately. (this is the lowest you can set as of now)</p> <p>Note</p> <p>You can ignore the following warning as the this is just a test for Smart DR feature. In a customer environment the source and Target Files servers will be in different AOS clusters.</p> <p>FSXYZ-#-dr is on the same AOS cluster as the source. It is recommended to have target file servers on a different AOS cluster.</p> </li> <li> <p>Make sure your selection looks as follows:</p> <p></p> </li> <li> <p>Click on Next at the bottom of the screen</p> </li> <li> <p>Fill in the following details in Settings section:</p> <ul> <li>Name - initials-repl-policy (e.g.xyz-repl-policy)</li> <li>Description - Protection Policy for XYZ Prod to DR Replication (Optional)</li> </ul> </li> <li> <p>Click on Create</p> </li> <li> <p>Monitor the Tasks until the policy is created and the policy should show in the Data Protection &gt; Polices in a few minutes</p> <p></p> <p>Wait a few minutes until all the files are replicated and RPO Compliant will have a green-dot to indicate initial synchronization</p> </li> <li> <p>Go to Data Protection &gt; Replication Jobs and observe the     replication jobs and duration. The initial replication will take time based on the amount of data and network speeds. But the subsequent replications will be based on incremental changes only.</p> <p></p> </li> <li> <p>Go to Data Protection &gt; Protected File Servers to check the Active and Standby File servers. (Active indicated by a green A)</p> <p></p> </li> <li> <p>Verify it shows the DR Files Server with the source PROD share (e.g.\\FSXYZ-#*-dr.ntnxlab.local\\usershareXX)</p> </li> </ol>"},{"location":"files_replication/files_replication/#failover-share","title":"Failover Share","text":"<p>We have set up replication of a share between two Files servers. Now we are able to test failover of the share to the DR File server.</p> <p>There are two failover methods:</p> <ul> <li>Planned Failover - allows a reverse-replication to the source File Server</li> <li>Unplanned Failover - no reverse-replication (as an admin doesn't know when the primary site will be operational again)</li> </ul> <p>Both these methods are manually triggered by an administrator.</p> <p>In this lab we will test a Planned Failover</p> <ol> <li> <p>Go to Prism Central &gt; Services &gt; Files (if you are note already on that page)</p> </li> <li> <p>Go to Data Protection &gt; Protected File Servers</p> </li> <li> <p>Click on Failover as shown here</p> <p></p> </li> <li> <p>Select Planned Failover</p> </li> <li> <p>Select Create a Reverse-Replication Policy and fill in the following</p> <ul> <li>Recovery Point Objective (RPO) - 10 minutes</li> <li>Policy Name - Reverse-xyz-repl-policy (e.g. Reverse-xyz-repl-policy)</li> </ul> <p></p> </li> <li> <p>Click Next</p> </li> <li> <p>In the Active Directory and DNS Configuration fill the following:     (to ensure access to files after failover)</p> <ul> <li>Username - administrator@ntnxlab.local</li> <li>Password - nutanix/4u</li> <li>Preferred Domain Controller - ntnxlab.local or leave blank</li> <li>Preferred Name Server - 10.X.X.X or leave blank (Your AD IP address)</li> </ul> </li> <li> <p>Select the Use the same credentials as the Active Directory     check-box (in our lab both the AD and DNS server are the same)</p> </li> <li> <p>Click on Failover</p> </li> <li> <p>Monitor the Events in Prism Central</p> </li> <li> <p>Once Failover is completed, return to Files &gt; Data Protection &gt; Protected File Servers in Prism Central and check the Active and Standby File servers. (Active indicated by a green A)</p> </li> <li> <p>Confirm that FSXYZ-#-dr (e.g. FS002-3-dr) server is now     the active server</p> <p></p> </li> <li> <p>Return to your Windows Tools VM and access the failed over share in Windows Explorer</p> </li> <li> <p>Login to your Windows Tools VM with the following credentials</p> <ul> <li>Username - administrator@ntnxlab.local</li> <li>Password - nutanix/4u</li> </ul> </li> <li> <p>Browse to the location of your source share now hosted on PROD and DR Files server, you will find that both File Servers point to the same location.</p> <ul> <li>Prod path - \\FSXYZ-#-prod.ntnxlab.local\\usershareXX</li> <li>DR path - \\FSXYZ-#-dr.ntnxlab.local\\usershareXX</li> </ul> <p></p> <p>This demonstrates that the users can access the share without having the change the file server names providing continuous access to the shares.</p> </li> <li> <p>Logon to your AutoAD server and open DNS management (from Search     button &gt; type DNS )</p> <p></p> <p>You can notice that both PROD and DR file servers point to the IP address of the DR File Server.</p> </li> <li> <p>Go to Data Protection &gt; Replication Jobs and verify that the source Files server is now FSXYZ-#-dr server</p> <p></p> </li> <li> <p>Go to Data Protection &gt; Policies and verify a reverse replication policy is present, that means data is replicating from DR to production File Server now.</p> <p></p> </li> </ol>"},{"location":"files_replication/files_replication/#failback-share","title":"Failback Share","text":"<p>Continue from the previous lab, we will see how to failback a share to the Source site after the environment is recovered.</p> <p>In this lab we will test a Planned Failover</p> <ol> <li> <p>Go to Prism Central &gt; Services &gt; Files (if you are not already on that page)</p> </li> <li> <p>Go to Data Protection &gt; Protected File Servers</p> </li> <li> <p>Click on Failback as shown here</p> </li> <li> <p>In the Active Directory and DNS Configuration fill the following (to ensure access to files after failover)</p> <ul> <li>Username - administrator@ntnxlab.local</li> <li>Password - nutanix/4u</li> <li>Preferred Domain Controller - ntnxlab.local or leave blank</li> <li>Preferred Name Server - Your AD IP address or leave blank</li> </ul> </li> <li> <p>Select the Use the same credentials as the Active Directory check-box (in our lab both the AD and DNS server are the same)</p> </li> <li> <p>Click on Next</p> </li> <li> <p>Files now gives you a visual of the failed-back environment and informs you that the Reverse Replication policy will be deleted</p> <p></p> </li> <li> <p>Click on Failback</p> </li> <li> <p>Monitor the Events in Prism Central</p> </li> <li> <p>Once the failover is done, go to your Windows Tools VM and logon to the share hosted on PROD files server (e.g     \\FSXYZ-#-prod\\usershareXX )</p> </li> <li> <p>Logon to your AutoAD server once again and open DNS management (from Search button &gt; type DNS )</p> <p></p> <p>You can notice that the PROD file server has been reverted to the previous IP address.</p> </li> <li> <p>We have successfully failed back the share to the PROD site. Now users can connect to the share as usual.</p> </li> </ol>"},{"location":"files_replication/files_replication/#takeaway","title":"Takeaway","text":"<p>Nutanix Files Smart DR makes it easy for administrators to configure replication of shares between Nutanix Files servers without needing third-party integrations.</p> <p>For information about Files Manager and Smart DR features, refer to this documentation URL.</p>"},{"location":"files_smb_share/files_smb_share/","title":"Files: Create SMB Share","text":""},{"location":"files_smb_share/files_smb_share/#overview","title":"Overview","text":"<p>In this exercise you will create and test a SMB share. SMB share are used to support home directories, user profiles, and other unstructured file data such as departmental shares commonly accessed by Windows clients.</p>"},{"location":"files_smb_share/files_smb_share/#lab-setup","title":"Lab Setup","text":"<ol> <li>Connect to the Windows Tools VM via RDP or console</li> <li>Download the sample files for File Analytics to the Tools VM</li> </ol>"},{"location":"files_smb_share/files_smb_share/#using-smb-shares","title":"Using SMB Shares","text":""},{"location":"files_smb_share/files_smb_share/#creating-the-share","title":"Creating the Share","text":"<ol> <li> <p>In Prism Element &gt; File Server, click on your FSXYZ-#-prod File Server</p> </li> <li> <p>Click on Launch Files Console </p> <p>This will open in a new browser tab</p> <p></p> </li> <li> <p>Select Shares and click on Create a New Share</p> </li> <li> <p>Fill out the following fields:</p> <ul> <li>Name - usershareXX (e.g. usershare01)</li> <li>Description (Optional) - Departmental share for GSO team</li> <li>Share Path (Optional) - Leave blank. This field allows you     to specify an existing path in which to create the nested share.</li> <li>Max Size (Optional) - Leave blank. This field allows you to set a hard quota for the individual share.</li> <li>Primary Protocol Access - SMB</li> </ul> <p></p> </li> <li> <p>Click Next.</p> </li> <li> <p>Select the following options:</p> <ul> <li>Enable Self Service Restore,</li> <li>Enable Compression and</li> <li>Enable Access Based Enumeration</li> </ul> <p></p> <p>As you are creating a departmental share, it should be created as a Standard share. This means that all top level directories and files within the share, as well as connections to the share, are served from a single Files VM.</p> <p>Distributed shares are appropriate for home directories, user profiles, and application folders. This type of share shards top level directories across all Files VMs and load balances connections across all Files VMs within the Files cluster.</p> <p>Access Based Enumeration (ABE) ensures that only files and folders which a given user has read access are visible to that user. This is commonly enabled for Windows file shares.</p> <p>Self Service Restore allows users to leverage Windows Previous Version to easily restore individual files to previous revisions based on Nutanix snapshots.</p> </li> <li> <p>Click Next.</p> </li> <li> <p>Review the Summary and click Create.</p> <p></p> </li> <li> <p>Use the same step to create a SMB share with name : ransomwaretestXX (e.g. ransomwaretest01) </p> </li> </ol>"},{"location":"files_smb_share/files_smb_share/#testing-the-share","title":"Testing the Share","text":"<ol> <li> <p>Connect to your Initials-WinToolsVM via RDP or console using     NTNXLAB\\Administrator user</p> <p>Note</p> <p>The Tools VM has already been joined to the NTNXLAB.local    domain. You could use any domain joined VM to complete the following    steps.</p> </li> <li> <p>Open <code>\\\\FS*XYZ*-#-prod.ntnxlab.local\\</code> in File Explorer.</p> <p></p> </li> <li> <p>Test accessing the usershareXX (where XX is your username) share by creating a text file into the share.</p> <ul> <li>The NTNXLAB\\Administrator user was specified as a Files     Administrator during deployment of the Files cluster, giving it     read/write access to all shares by default.</li> <li>Managing access for other users is no different than any other     SMB share.</li> </ul> </li> <li> <p>Right-click usershareXX &gt; Properties.</p> </li> <li> <p>Select the Security tab and click Advanced.</p> <p></p> </li> <li> <p>Select Users (FSXYZ-#-prod\\Users) and click Remove.</p> </li> <li> <p>Click Add.</p> </li> <li> <p>Click Select a principal and specify Everyone in the     Object Name field. Click OK.</p> <p></p> </li> <li> <p>Fill out the following fields and click OK:</p> <ul> <li>Type - Allow</li> <li>Applies to - This folder only</li> <li>Select Read &amp; execute</li> <li>Select List folder contents</li> <li>Select Read</li> <li>Select Write</li> </ul> <p></p> </li> <li> <p>Click OK &gt; OK &gt; OK to save the permission changes.</p> <p>All users will now be able to create folders and files within the xyz-GSO share.</p> <p>It is common for shares utilized by many people to leverage quotas to ensure fair use of resources. Files offers the ability to set either soft or hard quotas on a per share basis for either individual users within Active Directory, or specific Active Directory Security Groups.</p> </li> </ol>"},{"location":"files_smb_share/files_smb_share/#adding-share-level-quota","title":"Adding Share Level Quota","text":"<ol> <li> <p>In Prism Element &gt; File Server, click on your FSXYZ-A-prod File Server</p> </li> <li> <p>Click on Launch Files Console</p> <p></p> </li> <li> <p>Select Shares &gt; usershareXX (where XX is your username share)</p> <p></p> </li> <li> <p>This will open the share details</p> </li> <li> <p>Click on Quota Policies &gt; New Quota Policy</p> <p></p> </li> <li> <p>Fill out the following fields and click Save:</p> <ul> <li>Select User Group</li> <li>User or Group - SSP Developers</li> <li>Quota - 10 GiB</li> <li>Enforcement Type - Hard Limit</li> </ul> <p></p> </li> <li> <p>Click Add.</p> </li> <li> <p>This will enforce quota limits on the shares for AD user group SSP     Developers to stay within limit</p> </li> <li> <p>With the usershareXX (where XX is your username) &gt; Summary selected, review the     Capacity Summary, Performance Summary and Share     Properties tabs to understand the available on a per share basis,     including the number of files &amp; connections, storage utilization     over time, latency, throughput, and IOPS.</p> <p></p> </li> <li> <p>Remove all the user group policy before goint to the next lab.</p> </li> </ol>"},{"location":"files_tiering/files_tiering/","title":"Files Smart Tiering","text":""},{"location":"files_tiering/files_tiering/#overview","title":"Overview","text":"<p>Data storage can get quite expensive for applications using file service storage especially for data that does not access for months or years.</p> <p>To enable applications to have data that is infrequently used to be tiered to object storage which may be cheaper and in a different region, Nutanix Files provides tiering feature to tier to either Nutanix Objects or varies cloud base object storage.</p> <p>This will enable customers to do the following:</p> <ul> <li>Tier to third-party objects based cloud storage</li> <li>Reduce storage consumption and cost for storing data that are old and infrequently used</li> <li>Decouple the application from managing storage and have Nutanix's proven HCI Storage features to effectively manage data</li> <li>Use industry standards for Objects like storage</li> </ul>"},{"location":"files_tiering/files_tiering/#possible-tiering-configurations","title":"Possible Tiering Configurations","text":"<p>Nutanix Objects is capable of tiering to any S3 compatible objects store provider.</p> <p></p> <p>To accomplish this tiering we need the following:</p> <ul> <li>Destination S3 Storage<ul> <li>Destination S3 access URL</li> <li>Destination Access key</li> <li>Destination Secret key</li> </ul> </li> <li>Networking between source and destination<ul> <li>Physical connectivity</li> <li>Firewall and security allowing for this connection</li> </ul> </li> </ul>"},{"location":"files_tiering/files_tiering/#lab-setup","title":"Lab Setup","text":"<p>In this lab, you will walk through a Nutanix Files Smart Tiering feature to Nutanix Objects only. The configuration procedure remains the same for other supported S3 service on public cloud.</p> <p>We will use the File Server and Object Store in the lab for this lab.</p>"},{"location":"files_tiering/files_tiering/#setup-endpoint-in-object-store-configuration","title":"Setup Endpoint in Object Store Configuration","text":"<p>In this section you will setup endpoints for tiering from Nutanix Objects that has been pre-deployed for you to AWS S3.</p>"},{"location":"files_tiering/files_tiering/#get-configuration-details-on-nutanix-objects","title":"Get Configuration Details on Nutanix Objects","text":"<ol> <li> <p>Logon to Prism Central</p> </li> <li> <p>Click on  &gt; Services &gt; Objects</p> </li> <li> <p>Record the Objects Public IP for ntnx-objects</p> </li> <li> <p>Click Access Keys &gt; + Add People</p> </li> <li> <p>Click + Add Directory to add directory service for users, then fill the following :</p> <ul> <li>Name - ntnxlab</li> <li>Domain - ntnxlab.local</li> <li>Directory URL - ldap://[Your AD IP Address]:389</li> <li>Username - ntnxlab\\Administrator</li> <li>Password - nutanix/4u</li> </ul> </li> <li> <p>Click Add </p> </li> <li> <p>Search for adminuserXX as the user and click the checkbox next to it where XX is the user number assigned to you.     </p> </li> <li> <p>Click Next &gt; Generate Key &gt; Download Key</p> </li> <li> <p>Once you download the key and open the key file, you will see information like this. Keep the secret key and access key for later session :</p> Sample file only - do not copy<pre><code>Username: adminuser01@ntnxlab.local\n\nAccess Key: PE_5lYhc5exAc-_gDzO2_xO8FGDdJg4_\n\nSecret Key: fIRBeAzHNOE7sl3Wei88h_LFiL0KozKb\n\nDisplay Name: Adminuser01\n</code></pre> </li> <li> <p>Go back to Prism Central, click on  &gt; Services &gt; Objects</p> </li> <li> <p>Click the Objects Public IP to access the Object Browser, use the Access Key and Secret Key to login</p> </li> <li> <p>then click Create Bucket, use tier-target as the name and click the checkbox of Enable versioning, then click Create.         </p> </li> </ol>"},{"location":"files_tiering/files_tiering/#enable-and-configure-files-smart-tiering","title":"Enable and Configure Files Smart Tiering","text":"<ol> <li> <p>Logon to Prism Element</p> </li> <li> <p>Go to File Server, and click Launch Files Console next to FSxyz-a-prod.</p> </li> <li> <p>In Files Console, go to Data Management &gt; Tiering, you can see that Files Smart Tiering is not enabled yet. </p> </li> <li> <p>Click 1.  Logon to Prism Central </p> <p>Note</p> <p>In real deployment, you can also choose to enable Smart Tiering in Data Lens.</p> </li> <li> <p>Click Add Tiering Location</p> </li> <li> <p>Fill in the following : </p> <ul> <li>Name - ntnx-objects</li> <li>Storage Type - Nutanix Object Store</li> <li>Service Host - [ntnx-objects Objects Public IP]</li> <li>Access Key - Access Key of adminuser01</li> <li>Secret Key - Secret Key of adminuser01</li> <li>Proxy Server - leave blank</li> <li>Bucket Name - tier-target</li> <li>Retention Period - 6 Month</li> <li>Check Skip SSL Certificate Validation</li> </ul> </li> <li> <p>Click Add </p> </li> <li> <p>Once tiering location is configured, we can start to set policy. Click Set a Capacity Threshold, you can set a capacity threshold so that tiering policy will only be run when the capacity usage is exceeded the percentage. For this lab, we do not set threshold so do not click Set Capacity Threshold and keep Manual selected under When to Tier. Then click Save.</p> </li> <li> <p>Now click Create a Tiering Policy</p> </li> <li> <p>Here you can define the tiering policy by setting the Access Time and File Size. You can set these two to any acceptable values.</p> <p>Note</p> <p>File Size need to set at least 64KB.</p> </li> <li> <p>Click Excludes Shares and exclude all shares except xyz-GSO.     </p> </li> </ol>"},{"location":"files_tiering/files_tiering/#verify-smart-tiering-behavior","title":"Verify Smart Tiering behavior","text":"<p>To speed up, we will verify the behavior by doing manual tiering in AFS CLI.</p> <ol> <li>Go to usershareXX, find a file and record the file path.</li> <li> <p>SSH to FSVM via FSVM's client network or CVM</p> </li> <li> <p>Enter in CLI and then tier the following file:      <pre><code>    afs\n    tiering.tier usershareXX file_paths=\"file path you recorded\"\n</code></pre> </p> </li> <li> <p>You can check the tiering status by the following command, you can see this file has a status Offline, showing that it is already tiered.</p> <pre><code>    tiering.status usershareXX file_paths=\"file path you recorded\"\n</code></pre> <p>Note</p> <p>If you try to tier a file less than 64KB, you will find it was not tiered successfully.</p> <p></p> </li> <li> <p>You can login to your WintoolsVM and check the share folder of usershareXX. You will see the tiered file with a CROSS on the file icon. That means this file only has a stub in the file share. The actual content is in object store target.</p> <p></p> </li> <li> <p>Now right click the tiered file &gt; Open With &gt; Paint. You can do read access to a tiered file. </p> </li> </ol> <p>You have successfully tiered from Nutanix Files to Objects using Smart Tiering.</p>"},{"location":"files_tiering/files_tiering/#takeaways","title":"Takeaways","text":"<p>What are the key things you should know about Nutanix Files Smart Tiering?</p> <ul> <li>Nutanix Files allows easy configuration for tiering data to object stores (cloud and on-premises)</li> <li>Tiering feature is managed either from Files Console (On-prem) or Nutanix Data Lens (on cloud)</li> <li>After a file is tiered to object store, you can still do inline read. The file needs to be recalled to file share in order to overwrite.</li> <li>Nutanix enables applications to store and tier data to any S3-based object storage without lock-in</li> </ul>"},{"location":"lab_preparation/lab_preparation/","title":"Lab preparation","text":""},{"location":"lab_preparation/lab_preparation/#general","title":"General","text":"<ol> <li>Login to PC &gt; Administration &gt; LCM &gt; Inventory &gt; Perform Inventory</li> <li>Upgrade PC (if not on 2022.6.x), Files Manager, Objects (if not on 3.6) from PC</li> <li>Repeat previous step to upgrade Files Manager to 4.3</li> <li>Login to PE &gt; LCM &gt; Inventory &gt; Perform Inventory</li> <li>Upgrade Files from 4.2.1.1 to 4.3</li> <li>Enable PC MSP : <ol> <li>PC LCM &gt; Settings &gt; uncheck Enable HTTPS &gt; Save</li> <li>PC Gear Icon &gt; Prism Central Management &gt; PC on Microservices Infrastructure &gt; Enable Now &gt; Continue &gt; Validate &gt; Enable</li> </ol> </li> <li>Delete unuse VMs : SSH to CVM &gt; acli &gt; vm.delete Peer* &gt; yes</li> </ol>"},{"location":"lab_preparation/lab_preparation/#change-fs-name-remove-fa","title":"Change FS name &amp; Remove FA:","text":"<ol> <li>leave domain</li> <li>change name to FS[xyz]-[#]-prod</li> <li>enable SMB protocol in FIle Console</li> <li>Delete File Analytics : PE &gt; File Server &gt; Manage File Analytics &gt; Delete</li> </ol>"},{"location":"lab_preparation/lab_preparation/#fix-file-console-bug","title":"Fix File Console bug","text":"<ol> <li>login to CVM</li> <li>vi /home/nutanix/minerva/bin/minerva_cvm_test_patch.py<ul> <li>put this in the .py:     <pre><code>    import env\n    import gflags\n    import thread\n    import util.base.log as log\n\n    from minerva.cvm.server import MinervaCvmServer\n\n    FLAGS = gflags.FLAGS\n\n    original_MinervaCvmServer__initialize = MinervaCvmServer._MinervaCvmServer__initialize\n\n    def monkey_MinervaCvmServer__initialize(self):\n    original_MinervaCvmServer__initialize(self)\n    log.INFO(\"Starting minerva_cvm_gateway frm monkey_patch: %s\" % FLAGS.minerva_is_light_compute_node)\n    if FLAGS.minerva_is_light_compute_node:\n        thread.start_new_thread(self.start_minerva_cvm_gateway, ())\n    log.INFO(\"GATEWAY MONKEY PATCH WORKING\")\n\n    MinervaCvmServer._MinervaCvmServer__initialize = monkey_MinervaCvmServer__initialize\n</code></pre></li> </ul> </li> <li>Set permission :          <pre><code>    chmod 750 /home/nutanix/minerva/bin/minerva_cvm_test_patch.py\n</code></pre></li> <li>SCP to all other CVMs and restart minerva_cvm :          <pre><code>    for i in `svmips`; do echo \"CVM IP: $i\"; scp /home/nutanix/minerva/bin/minerva_cvm_test_patch.py $i:/home/nutanix/minerva/bin/minerva_cvm_test_patch.py; ssh $i /usr/local/nutanix/cluster/bin/genesis stop minerva_cvm; done\n</code></pre></li> <li>restart cluster :          <pre><code>    cluster start\n</code></pre></li> <li>verify the result :          <pre><code>    allssh 'grep \"GATEWAY MONKEY PATCH WORKING\" /home/nutanix/data/logs/minerva_cvm.log; ps -eaf | grep minerva_cvm_gateway | grep -v grep | tail -n 1'\n</code></pre></li> </ol>"},{"location":"lab_preparation/lab_preparation/#enable-data-lens","title":"Enable Data Lens","text":"<ol> <li>VM &gt; FSVM &gt; Launch Console &gt; login : nutanix | nutanix/4u</li> <li>enable FSVM ssh :          <pre><code>    afs misc.ssh_on_client_network enable\n</code></pre></li> <li>find CVM Cluster UUID from PE</li> <li>find FSVM UUID in FSVM CLI:          <pre><code>    zeus_config_printer | grep cluster_uuid\n</code></pre></li> <li> <p>go to website : register UUID</p> <ul> <li>put \"faqa\" in Data Pipeline</li> <li>add both UUID in this site to register </li> </ul> </li> <li> <p>Modify cfs.config         <pre><code>    allssh \"vi /home/nutanix/minerva/nusights/config/cfs/cfs.config\"\n</code></pre></p> <ul> <li>Add the following code to the file     <pre><code>    modify endpoint_info_list as follow\n        endpoint_info_list {\n        endpoint: \"faqa.nusightsinfra.com:443\"\n        uuid: \"&lt;CLUSTER_UUID&gt;\"\n        is_governing: true\n    }\n</code></pre></li> </ul> </li> <li> <p>Modify minerva_nvm.gflags         <pre><code>    allssh \"vi /home/nutanix/config/minerva_nvm.gflags\"\n</code></pre></p> <ul> <li>Add the following code to the file     <pre><code>    --minerva_cfs_dump_transported_data_to_file=true\n    --minerva_cfs_debug_level=3\n    --minerva_cfs_control_center_poll_freq_secs=120\n</code></pre></li> </ul> </li> <li> <p>Stop FS         <pre><code>    allssh genesis stop minerva_nvm\n</code></pre></p> </li> <li> <p>Run this         <pre><code>    zkrm /appliance/physical/cfs_configuration\n    allssh USE_SAFE_RM=no rm -rf /home/nutanix/data/nusights/*\n    cluster start\n</code></pre></p> </li> <li> <p>Check if fa_cloud_collector is running         <pre><code>    ps -aef | grep fa_cloud_collector\n</code></pre></p> </li> <li> <p>Verify pulse         <pre><code>    allssh \"curl http://127.0.0.1:2042/h/connectivity_status | python -m json.tool\"\n</code></pre></p> </li> <li> <p>Wait for 10 mins, login to https://datalens-qa.nutanix.com/home to check if the FS is there.</p> </li> <li> <p>Enable Data Lens for FS</p> </li> <li> <p>Go to Ransomware Protection, enable</p> </li> </ol>"},{"location":"lab_preparation/lab_preparation/#prepare-wintoolsvm","title":"Prepare WinToolsVM","text":"<ol> <li>Create a SMB share in prod FS : DLtest-prod</li> <li>Copy sample_files_for_migration.zip and unzip the files in DLtest-prod</li> <li>Create a folder \"my_secret\" in C:\\</li> <li>Copy sample_files_for_migration.zip and unzip the files in my_secret</li> <li>Share my_secret as a share folder.</li> <li>Map my_secret as M:\\ drive in the wintoolsVM</li> </ol>"},{"location":"lab_preparation/lab_preparation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"lab_preparation/lab_preparation/#data-lens-is-not-updating-with-new-files-operations","title":"Data Lens is not updating with new Files operations","text":"<p>To resolve the problem, you need to re-enable Data Lens for the FS:</p> <ol> <li> <p>ssh to FSVM, restart minerva</p> <pre><code>    genesis stop minerva_nvm\n    cluster start\n</code></pre> </li> <li> <p>Go to Data Lens, search for your FS, click disable &gt; Disable Nutanix Data Lens</p> </li> <li> <p>After finishing disable, find the FS in Data Lens, click the FS name to go to the dashboard page of the FS.</p> </li> <li> <p>Click the gear icon on the top right corner &gt; Manage Data </p> </li> <li> <p>Go to File Server tag &gt; delete the audit data &gt; Confirm </p> </li> <li> <p>After deletion completed, seach for your FS again, and click Enable</p> </li> </ol>"},{"location":"lab_preparation/lab_preparation/#have-failed-to-fetch-error-when-trying-to-set-smart-dr-policy-in-file-manager","title":"Have \"Failed to fetch\" Error when trying to set Smart DR policy in File Manager","text":"<p>What you will see:   </p> <ol> <li> <p>Resolve by doing this:</p> <pre><code>    docker exec -it files_manager /bin/bash\n    vi /etc/hosts \n</code></pre> </li> <li> <p>Add this line in the hosts file.</p> <pre><code>    [PCIP] iam-proxy.ntnx-base\n</code></pre> </li> <li> <p>Check again if the fetching problem esists.</p> </li> </ol>"},{"location":"lab_preparation/lab_preparation_v2.1/","title":"Lab preparation v2.1","text":""},{"location":"lab_preparation/lab_preparation_v2.1/#prepare-pc-pe","title":"Prepare PC &amp; PE","text":"<ol> <li>Login to PC or PE, find the IP Address of a WinToolsVM and login through remote desktop.</li> <li>Open Chrome, login to portal.nutanix.com using your account, go to Download &gt; Prism Central.</li> <li>Find Prism Central Upgrade (version: pc.2023.x), download the image (make sure it is .tar.gz) and matadata.</li> <li>Login to PC in the WinToolsVM</li> <li>Click gear icon on the top right corner &gt; Upgrade Prism Central &gt; Upload the Prism Central binary, upload the image and metadata and then choose Upload Now</li> <li>After uploading the image, choose the uploaded version of pc.2023.x, click Upgrade &gt; Upgrade Now &gt; Continue (It will take more than 30 minutes, so can prepare the file server in parallel)</li> <li>After upgrade complete, login to PC again, go to Admin Center &gt; LCM &gt; Inventory &gt; Perform Inventory</li> <li>After inventory complete, go to update, update Files Manager twice, and Objects Manager &amp; Objects Service</li> <li>Target versions</li> <li>PC : 2023.4</li> <li>AOS : 6.5.2.5</li> <li>Files : 4.4.0.1</li> <li>Files Manager : 4.4.0.1</li> <li>Objects : 4.2</li> <li>Delete unuse VMs : SSH to CVM &gt; acli &gt; vm.delete Peer* &gt; yes</li> </ol>"},{"location":"lab_preparation/lab_preparation_v2.1/#prepare-file-server-and-fa","title":"Prepare File Server and FA:","text":"<ol> <li>**[for Data Lens Lab only] ** Create a new File Server in PE</li> <li>Create a file server named FSXYZ for Data Lens lab (e.g. POC012)<ol> <li>4x FSVM, minimum vCPU &amp; RAM</li> <li>Both storage network and client network are Primary Network</li> <li>Use SMB<ol> <li>AD = ntnxlab.local</li> <li>Username = administrator@ntnxlab.local</li> <li>Password = nutanix/4u</li> <li>Make administrator@ntnxlab.local as the file server admin</li> </ol> </li> <li>Use NFS<ol> <li>Authenication = unmanaged</li> </ol> </li> <li>Do not create protection domain</li> </ol> </li> <li>Create file servers for general lab per user in PC</li> <li>Login to PC &gt; admin center &gt; inventory &gt; perform inventory</li> <li>Go to PC &gt; Files &gt; Create file server <ol> <li>Choose FS version = 4.4.0.1</li> <li>Named FSXYZ-#-prod (e.g. FS012-1-prod)</li> <li>1x FSVM, minimum vCPU &amp; RAM</li> <li>Both storage network and client network are Primary Network</li> <li>Follow the setting of FSXYZ</li> <li>Once created, go to the XXX to enable SMB and NFS</li> <li>Make adminuser##@ntnxlab.local as the file server admin</li> </ol> </li> <li>Remove File Server BootcampFS</li> <li>leave domain</li> <li>Delete the File Server</li> <li>Delete specific related entities</li> </ol>"},{"location":"lab_preparation/lab_preparation_v2.1/#prepare-active-directory-for-data-lens-lab-only","title":"Prepare Active Directory [for Data Lens Lab only]","text":"<ol> <li>Create user groups</li> <li>Remote desktop to AutoAD (10.xx.xx.41)</li> <li>Go to PowerShell ISE, login as administrator</li> <li>paste the following and execute     ```bash         for ($num = 1; $num -le 9; $num++) {             New-ADGroup -Name Group0$num -GroupCategory Security -GroupScope Global             Add-ADGroupMember -Identity \"Group0$num\" -Members adminuser0$num, user0$num         }</li> </ol>"},{"location":"lab_preparation/lab_preparation_v2.1/#enable-data-lens-for-data-lens-lab-only","title":"Enable Data Lens **[for Data Lens Lab only] **","text":"<ol> <li>VM &gt; FSVM &gt; Launch Console &gt; login : nutanix | nutanix/4u</li> <li>enable FSVM ssh :          <pre><code>    afs misc.ssh_on_client_network enable\n</code></pre></li> <li>find CVM Cluster UUID from PE</li> <li>find FSVM UUID in FSVM CLI:          <pre><code>    zeus_config_printer | grep cluster_uuid\n</code></pre></li> <li> <p>go to website : register UUID</p> <ul> <li>put \"faqa\" in Data Pipeline</li> <li>add both UUID in this site to register </li> </ul> </li> <li> <p>Modify cfs.config         <pre><code>    allssh \"vi /home/nutanix/minerva/nusights/config/cfs/cfs.config\"\n</code></pre></p> <ul> <li>Add the following code to the file     <pre><code>    modify endpoint_info_list as follow\n        endpoint_info_list {\n        endpoint: \"faqa.nusightsinfra.com:443\"\n        uuid: \"&lt;CLUSTER_UUID&gt;\"\n        is_governing: true\n    }\n</code></pre></li> </ul> </li> <li> <p>Modify minerva_nvm.gflags         <pre><code>    allssh \"vi /home/nutanix/config/minerva_nvm.gflags\"\n</code></pre></p> <ul> <li>Add the following code to the file     <pre><code>    --minerva_cfs_dump_transported_data_to_file=true\n    --minerva_cfs_debug_level=3\n    --minerva_cfs_control_center_poll_freq_secs=120\n</code></pre></li> </ul> </li> <li> <p>Stop FS         <pre><code>    allssh genesis stop minerva_nvm\n</code></pre></p> </li> <li> <p>Run this         <pre><code>    zkrm /appliance/physical/cfs_configuration\n    allssh USE_SAFE_RM=no rm -rf /home/nutanix/data/nusights/*\n    cluster start\n</code></pre></p> </li> <li> <p>Check if fa_cloud_collector is running         <pre><code>    ps -aef | grep fa_cloud_collector\n</code></pre></p> </li> <li> <p>Verify pulse         <pre><code>    allssh \"curl http://127.0.0.1:2042/h/connectivity_status | python -m json.tool\"\n</code></pre></p> </li> <li> <p>Wait for 10 mins, login to https://datalens-qa.nutanix.com/home to check if the FS is there.</p> </li> <li> <p>Enable Data Lens for FS</p> </li> <li> <p>Go to Ransomware Protection, enable</p> </li> </ol>"},{"location":"objects_buckets_users_access_control/objects_buckets_users_access_control/","title":"Objects: Creating Buckets, Users, and Access Control","text":""},{"location":"objects_buckets_users_access_control/objects_buckets_users_access_control/#overview","title":"Overview","text":""},{"location":"objects_buckets_users_access_control/objects_buckets_users_access_control/#create-bucket-in-prism-central","title":"Create Bucket In Prism Central","text":"<p>A bucket is a sub-repository within an object store which can have policies applied to it, such as versioning, WORM, etc. By default a newly created bucket is a private resource to the creator. The creator of the bucket by default has read/write permissions, and can grant permissions to other users.</p> <ol> <li> <p>Go to Prism Central, click Infrastructure &gt; Objects</p> </li> <li> <p>Click on ntnx-objects Objects Store</p> </li> <li> <p>The Objects Store management will open in a new browser tab</p> </li> <li> <p>Go to Bucket, click Create Bucket, and fill out the following fields:</p> <ul> <li>Name - your-name-my-bucket</li> <li>Enable Versioning - Checked</li> <li>Multiprotocol Access - disabled (selecting Enable     versioning will disable this)</li> </ul> <p></p> </li> <li> <p>Click Create.</p> <p>Note</p> <p>Buckets created via Prism Central are owned by the Prism Central    admin.</p> <p>If versioning is enabled, new versions can be uploaded of the same object for required changes, without losing the original data.</p> <p>Lifecycle policies define how long to keep data in the system.</p> <p>Once the bucket is created, it can be configured with WORM.</p> <p>WORM (Write Once, Read Many) storage prevents the editing, overwriting, renaming, or deleting data and is crucial in heavily regulated industries (finance, healthcare, public agencies, etc.) where sensitive data is collected and stored. Examples include e-mails, account information, voice mails, and more.</p> <p>Note</p> <p>If WORM is enabled on the bucket, this will supersede any lifecycle policy.</p> </li> <li> <p>Check the box next to your your-name-my-bucket bucket, and click on Actions dropdown and choose Configure WORM</p> </li> <li> <p>Check the box next to Enable WORM bucket,</p> </li> <li> <p>Enter 1 month as the retention periodand click Enable WORM.</p> <p>Note</p> <p>You have the ability to define a WORM data retention period on a per bucket basis.</p> </li> <li> <p>Create the another bucket in ntnx-objects-dr with the name : your-name-my-bucket-dr and follow the same configuration as your-name-my-bucket.</p> </li> </ol>"},{"location":"objects_buckets_users_access_control/objects_buckets_users_access_control/#buckets-user-management","title":"Buckets User Management","text":"<p>In this exercise you will create generate access and secret keys to access the object store, that will be used throughout the lab.</p> <ol> <li> <p>Go to Prism Central, click Infrastructure &gt; Objects</p> </li> <li> <p>From the left panel of Objects UI, click on Access Keys and click Configure Directories</p> </li> <li> <p>Check if domain ntnxlab.local is added to the directory service. If not, add that in by clicking + Add Directory.</p> <ul> <li>Directory Type - Active Directory</li> <li>Name - ntnxlab</li> <li>Domain - ntnxlab.local</li> <li>Directory URL - ldap://[domain controller IP]:389</li> <li>Username - ntnxlab\\administrator</li> <li>Password - nutanix/4u</li> </ul> <p></p> </li> <li> <p>If directory is added, go to Add People from Access Keys.</p> <p></p> </li> <li> <p>Select Search for people in a directory service and search for adminuser## where ## is your assigned number.</p> <p></p> </li> <li> <p>Click Next.</p> </li> <li> <p>Click Generate Keys to generate a ket.</p> <p></p> </li> <li> <p>Click Download Keys to download a .txt file containing the     Access Key and Secret Key.</p> <p></p> </li> <li> <p>Click Close.</p> </li> <li> <p>Open the file with a text editor.</p> <p></p> <p>Note</p> <p>Keep the text files open so that you have the access and secret keys    readily available for future labs.</p> </li> <li> <p>Go to Prism Central, click Infrastructure &gt; Objects</p> </li> <li> <p>Click on ntnx-objects Objects Store</p> </li> <li> <p>The Objects Store management will open in a new browser tab (if it     not already open)</p> </li> <li> <p>Click on your-name-my-bucket bucket, and click on User     Access</p> </li> <li> <p>Click on Edit User Access button</p> <p>This is where you will be able to share your bucket with other users. You can configure read only, full access , or custom, on a per user basis.</p> </li> <li> <p>Add the user (email address) you created earlier, click Full Access</p> <p></p> </li> <li> <p>Click on Save</p> </li> <li> <p>Do the same user access setting to your-name-my-bucket-dr in ntnx-objects-dr</p> </li> </ol>"},{"location":"objects_buckets_users_access_control/objects_buckets_users_access_control/#adding-users-to-buckets-share","title":"Adding Users to Buckets Share","text":""},{"location":"objects_buckets_users_access_control/objects_buckets_users_access_control/#accessing-creating-buckets-with-objects-browser","title":"Accessing &amp; Creating Buckets With Objects Browser","text":"<p>In this exercise you will use Objects Browser to create and use buckets in the object store using your generated access key.</p>"},{"location":"objects_buckets_users_access_control/objects_buckets_users_access_control/#download-the-sample-images","title":"Download the Sample Images","text":"<ol> <li>Login to Initials-Windows-ToolsVM via RDP using the following     credentials:<ul> <li>Username - NTNXLAB\\Administrator</li> <li>password - ask your Instructor</li> </ul> </li> <li>Use this     link     to download the sample files to your Windows-ToolsVM. Once the     download is complete, extract the contents of the .zip file.</li> </ol>"},{"location":"objects_buckets_users_access_control/objects_buckets_users_access_control/#use-object-browser-to-create-a-bucket","title":"Use Object Browser to Create A Bucket","text":"<ol> <li> <p>From the Objects UI, Locate the ntnx-objects object store.</p> <p></p> </li> <li> <p>The objects browser will open in a new tab</p> </li> <li> <p>Enter the following fields from the downloaded key file (while you     created user access) and click Login button:</p> <ul> <li>Access Key - Generated When User Created</li> <li>Secret Key - Generated When User Created</li> </ul> <p></p> </li> <li> <p>Click the + Create Bucket.</p> </li> <li> <p>Enter the following name for your bucket, and click Create:</p> <ul> <li>Bucket Name - intials-test-bucket</li> <li>Enable versioning - checked</li> </ul> <p></p> <p>Note</p> <p>Bucket names must be lower case and only contain letters, numbers,    periods and hyphens.</p> <p>Additionally, all bucket names must be unique within a given Object    Store. Note that if you try to create a folder with an existing    bucket name (e.g. your-name-my-bucket), creation of the folder    will not succeed.</p> <p>Creating a bucket in this fashion allows for self-service for entitled users, and is no different than a bucket created via the Prism Buckets UI.</p> </li> <li> <p>Click into your intials*-test-bucket bucket, and Click the + and Upload Objects**.</p> </li> <li> <p>Click on Select Folder</p> <p></p> </li> <li> <p>Navigate to your downloads directory and find the Sample Pictures     folder. Upload one or more pictures to your bucket.</p> </li> <li> <p>That is how easy it is to use the Objects Browser.</p> </li> </ol>"},{"location":"objects_buckets_users_access_control/objects_buckets_users_access_control/#object-versioning","title":"Object Versioning","text":"<p>Object versioning allows the upload of new versions of the same object for required changes, without losing the original data. Versioning can be used to preserve, retrieve and restore every version of every object stored within a bucket, allowing for easy recovery from unintended user action and application failures.</p> <ol> <li> <p>Open Notepad in Initials-Windows-ToolsVM.</p> </li> <li> <p>Type \"version 1.0\" in Notepad, then save the file with a name (e.g.     A.txt)</p> </li> <li> <p>In Objects Browser, upload the text file to your     intials*-test-bucket** bucket.</p> </li> <li> <p>Make changes to the text file (e.g. A.txt) in Notepad and save it     with the same name, overwriting the original file.</p> </li> <li> <p>Upload the modified file to your bucket. If desired, you can update     and upload the file multiple times.</p> </li> <li> <p>Back in Prism Cental &gt; Services &gt; Objects, click on Object     Stores.</p> </li> <li> <p>In Objects Browser of your intials*-test-bucket** bucket</p> </li> <li> <p>Select your A.txt file</p> </li> <li> <p>Under the Actions menu, scroll down to select Versions</p> </li> <li> <p>You should be able to see the different versions of the file A.txt</p> <p></p> </li> </ol>"},{"location":"objects_buckets_users_access_control/objects_buckets_users_access_control/#takeaways","title":"Takeaways","text":"<p>What are the key things you should know about Nutanix Objects?</p> <ul> <li>Nutanix Objects provides a simple and scalable S3-compatible object     storage solution, optimized for DevOps, Long Term Retention and     Backup Target use cases.</li> <li>Nutanix Objects can be deployed on an AHV cluster, with ESXi support     on the roadmap.</li> <li>Nutanix Objects will be enabled and deployed from Prism Central.</li> </ul>"},{"location":"objects_cli_scripts/objects_cli_scripts/","title":"Objects: Using From CLI &amp; Scripts","text":"<p>Info</p> <p>The estimated time to complete this lab is 60 minutes.</p>"},{"location":"objects_cli_scripts/objects_cli_scripts/#overview","title":"Overview","text":""},{"location":"objects_cli_scripts/objects_cli_scripts/#accessing-objects-from-the-cli","title":"Accessing Objects from the CLI","text":"<p>While tools like the Object Browser help to visualize how data is access within an object store, Buckets is primarily an object store service that is designed to be accessed and consumed over S3 APIs.</p> <p>Amazon's S3 (Simple Storage Service) is the largest public cloud storage service, and has subsequently become the de-facto standard object storage API due to developer and ISV adoption. Buckets provides an S3 compliant interface to allow for maximum portability, as well as support for existing cloud native applications.</p> <p>In this exercise you will leverage <code>s3cmd</code> to access your buckets using the CLI.</p> <p>You will need the Access Key and Secret Key for the user created earlier in this lab.</p>"},{"location":"objects_cli_scripts/objects_cli_scripts/#setting-up-s3cmd-cli","title":"Setting up s3cmd (CLI)","text":"<p>This section of the lab is done on using Linux Tools VM.</p> <ol> <li> <p>Login to the initials-Linux-ToolsVM, with the following     credentials</p> <ul> <li>Username - root</li> <li>Password - default nutanix password</li> </ul> </li> <li> <p>Run the following command to configure access to the Object Store:</p> <p>Note</p> <p>For anything not specified below, just hit enter to leave the defaults. Do NOT set an encryption password and do NOT use HTTPS protocol.</p> <p><pre><code>s3cmd --configure\n</code></pre> The execution will be as follows: see tabs for template and a sample execution </p> Template Command ExecutionSample Command Execution <pre><code>s3cmd --configure\n#\n- Access Key  - **Access Key**\n- Secret Key  - **Secret Key**\n- Default Region [US]**  - us-east-1\n- S3 Endpoint [s3.amazonaws.com]**  - *OBJECT-STORE-IP*\n- DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]**  - *OBJECT-STORE-IP*\\ :80\n- Encryption password** - Leave Blank\n- Path to GPG program [/usr/bin/gpg]**  - Leave Blank\n- Use HTTPS protocol [Yes]**  - No\n- HTTP Proxy server name**  - Leave Blank\n\nTest access with supplied credentials? [Y/n] y\nPlease wait, attempting to list all buckets...\nSuccess. Your access key and secret key worked fine :-)\n\nNow verifying that encryption works...\nNot configured. Never mind.\n\nSave settings? [y/N] y\nConfiguration saved to '/root/.s3cfg'\n</code></pre> <pre><code>s3cmd --configure\n#\n- Access Key: Ke2hEtehmOZoXYCrQnzUn_2EDD9Eqf0L\n- Secret Key: p6sxh_FhxEyIteslQJKfDlezKrtJro9C\n- Default Region: us-east-1\n- S3 Endpoint: 10.20.95.51\n- DNS-style bucket+hostname: port template for accessing a bucket: 10.20.95.51:80\n- Encryption password:\n- Path to GPG program: /usr/bin/gpg\n- Use HTTPS protocol: False\n- HTTP Proxy server name:\n\nTest access with supplied credentials? [Y/n] y\nPlease wait, attempting to list all buckets...\nSuccess. Your access key and secret key worked fine :-)\n\nNow verifying that encryption works...\nNot configured. Never mind.\n\nSave settings? [y/N] y\nConfiguration saved to '/root/.s3cfg'\n</code></pre> </li> <li> <p>Type Y and press Return to save the configuration.</p> </li> <li> <p>Type on Linux Tools VM <code>nano .s3cfg</code> then edit the value <code>signature_v2 = True</code></p> </li> </ol>"},{"location":"objects_cli_scripts/objects_cli_scripts/#create-a-bucket-and-add-objects-to-it-using-s3cmd-cli","title":"Create A Bucket And Add Objects To It Using s3cmd (CLI)","text":"<ol> <li> <p>Now let's use s3cmd to create a new bucket called your-name-clibucket.</p> </li> <li> <p>From the same Linux command line, run the following command:</p> <pre><code>s3cmd mb s3://yourname-clibucket\n</code></pre> </li> <li> <p>You should see the following output:</p> <pre><code>Bucket 's3://yourname-clibucket/' created\n</code></pre> </li> <li> <p>List your bucket with the <code>ls</code> command:</p> <pre><code>s3cmd ls\n</code></pre> </li> <li> <p>You will see a list of all the buckets in the object-store.</p> </li> <li> <p>To see just your buckets run the following command:</p> <pre><code>s3cmd ls | grep yourname\n</code></pre> </li> <li> <p>Now that we have a new bucket, let's upload some data to it.</p> </li> <li> <p>If you do not already have the Sample-Pictures.zip, download it to your Linux-ToolsVM.</p> <pre><code>curl https://peerresources.blob.core.windows.net/sample-data/SampleData_Small.zip -O -J -L\nmkdir sample-pictures\n#extract only the picture files\nunzip -j SampleData_Small.zip *.png -d sample-pictures\n</code></pre> </li> <li> <p>List images in sample-pictures folder</p> <pre><code>ls sample-pictures\n</code></pre> </li> <li> <p>Run the following command to upload one of the images to your bucket:</p> Make sure to replace $IMAGENAME with an image name listed from the previous step<pre><code>cd sample-pictures\ns3cmd put --acl-public --guess-mime-type $IMAGENAME s3://&lt;your-bucket-name&gt;/$IMAGENAME\n</code></pre> </li> <li> <p>You should see the following output:</p> <p><pre><code>s3cmd put --acl-public --guess-mime-type yahoo-identity-provider-config.png s3://lb-cli-bucket/yahoo-identity-provider-config.png\n</code></pre> <pre><code>upload: 'yahoo-identity-provider-config.png' -&gt; 's3://lb-cli-bucket/yahoo-identity-provider-config.png'  [1 of 1]\n63937 of 63937   100% in    0s     4.51 MB/s  done\nPublic URL of the object is: http://10.38.188.18/lb-clibucket/yahoo-identity-provider-config.png\n</code></pre></p> </li> <li> <p>If desired, repeat with more images.</p> </li> <li> <p>Run the la command to list all objects in all buckets:</p> <pre><code>s3cmd la\n</code></pre> </li> <li> <p>To see just objects in your buckets, run the following command:</p> <pre><code>s3cmd la | grep *initials*\n</code></pre> </li> </ol>"},{"location":"objects_cli_scripts/objects_cli_scripts/#creating-and-using-buckets-from-scripts","title":"Creating and Using Buckets From Scripts","text":"<p>While tools like the Objects Browser help to visualize how data is access within an object store, Nutanix Objects is primarily an object store service that is designed to be accessed and consumed over S3 APIs.</p> <p>Amazon Web Services's S3 (Simple Storage Service) is the largest public cloud storage service, and has subsequently become the de-facto standard object storage API due to developer and ISV adoption. Objects provides an S3 compliant interface to allow for maximum portability, as well as support for existing \\\"cloud native\\\" applications.</p> <p>In this exercise you will use Boto 3, the AWS SDK for Python, to manipulate your buckets using Python scripts.</p> <p>You will need the Access Key and Secret Key for the user account created earlier in this lab.</p>"},{"location":"objects_cli_scripts/objects_cli_scripts/#listing-and-creating-buckets-with-python","title":"Listing and Creating Buckets with Python","text":"<p>In this exercise, you will modify a sample script to match your environment, which will list all the buckets available to that user. You will then modify the script to create a new bucket using the existing S3 connection.</p> <ol> <li> <p>From the Initials-Linux-ToolsVM, run <code>vi list-buckets.py</code> and     paste in the script below. You will need to modify the     endpoint_ip, access_key_id, and secret_access_key values     before saving the script.</p> <p>Note</p> <p>If you are not comfortable with <code>vi</code> or alternative command line    text editors, you can modify the script in a GUI text editor then    paste the completed script into <code>vi</code>.</p> <p>In <code>vi</code>, type <code>i</code> and then right-click to paste into the text file.</p> <p>Press Ctrl + C then type <code>:wq</code> and press Return to save the    file.</p> <pre><code>#!/usr/bin/python\n\nimport boto3\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nendpoint_ip=\"OBJECT-STORE-IP\" #Replace this value\naccess_key_id=\"ACCESS-KEY\" #Replace this value\nsecret_access_key=\"SECRET-KEY\" #Replace this value\nendpoint_url= \"https://\"+ endpoint_ip +\":443\" \n\nsession = boto3.session.Session()\ns3client = session.client(service_name=\"s3\", aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key, endpoint_url=endpoint_url, verify=False)\n\n# list the buckets\nresponse = s3client.list_buckets()\n\nfor b in response['Buckets']:\n  print (b['Name'])\n</code></pre> </li> <li> <p>Execute <code>python list-buckets.py</code> to run the script. Verify that the     output lists any buckets you have created for using your first user     account.</p> </li> <li> <p>Using the previous script as a base, and the Boto 3 Documentation,     can you modify the script to create a new bucket and then list     all buckets?</p> </li> </ol>"},{"location":"objects_cli_scripts/objects_cli_scripts/#uploading-multiple-files-to-buckets-with-python","title":"Uploading Multiple Files to Buckets with Python","text":"<ol> <li> <p>From the Initials-Linux-ToolsVM, run the following to create     100 1KB files to be used as sample data for uploading:</p> <pre><code>cd ..;mkdir sample-files\nfor i in {1..100}; do dd if=/dev/urandom of=sample-files/file$i bs=1024 count=1; done\n</code></pre> <p>While the sample files contain random data, these could just as easily be log files that need to be rolled over and automatically archived, surveillance video, employee records, and so on.</p> </li> <li> <p>Create a new script based on the example below:</p> <pre><code>#!/usr/bin/python\n\nimport boto3\nimport glob\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# user defined variables\nendpoint_ip= \"OBJECT-STORE-IP\" #Replace this value\naccess_key_id=\"ACCESS-KEY\" #Replace this value\nsecret_access_key=\"SECRET-KEY\" #Replace this value\nbucket=\"BUCKET-NAME-TO-UPLOAD-TO\" #Replace this value\nname_of_dir=\"sample-files\"\n\n# system variables\nendpoint_url= \"https://\"+endpoint_ip+\":443\"\nfilepath = glob.glob(\"%s/*\" % name_of_dir)\n\n# connect to object store\nsession = boto3.session.Session()\ns3client = session.client(service_name=\"s3\", aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key, endpoint_url=endpoint_url, verify=False)\n\n# go through all the files in the directory and upload\nfor current in filepath:\n    full_file_path=current\n    m=re.search('sample-files/(.*)', current)\n    if m:\n      object_name=m.group(1)\n    print(\"Path to File:\",full_file_path)\n    print(\"Object name:\",object_name)\n    response = s3client.put_object(Bucket=bucket, Body=full_file_path, Key=object_name)\n</code></pre> <p>The put_object method is used for the file upload. Optionally this method can be used to define the metadata, content type, permissions, expiration, and other key information associated with the object.</p> <p>Core S3 APIs resemble RESTful APIs for other web services, with PUT calls allowing for adding objects and associated settings/metadata, GET calls for reading objects or information about objects, and DELETE calls for removing objects.</p> <p>Execute the script and use the following command to verify that the sample files are available.</p> <pre><code>s3cmd ls s3://yourname-clibucket/\n</code></pre> <pre><code># example command and output\n\ns3cmd ls s3://lb-clibucket/\n\n2021-12-20 05:56        18   s3://lb-clibucket/file1\n2021-12-20 05:56        19   s3://lb-clibucket/file10\n2021-12-20 05:56        20   s3://lb-clibucket/file100\n2021-12-20 05:56        19   s3://lb-clibucket/file11\n2021-12-20 05:56        19   s3://lb-clibucket/file20\n2021-12-20 05:56        19   s3://lb-clibucket/file2\n&lt;output snipped&gt;\n</code></pre> </li> </ol> <p>Similar S3 SDKs are available for languages including Java, JavaScript, Ruby, Go, C++, and others, making it very simple to leverage Nutanix Buckets using your language of choice.</p>"},{"location":"objects_cli_scripts/objects_cli_scripts/#takeaways","title":"Takeaways","text":"<p>What are the key things you should know about Nutanix Objects?</p> <ul> <li>Nutanix Objects provides a simple and scalable S3-compatible object     storage solution, optimized for DevOps, Long Term Retention and     Backup Target use cases.</li> <li>Nutanix Objects can be deployed on an AHV cluster, with ESXi support     on the roadmap.</li> <li>Nutanix Objects will be enabled and deployed from Prism Central.</li> </ul>"},{"location":"objects_deploy/objects_deploy/","title":"Objects: Deploy","text":""},{"location":"objects_deploy/objects_deploy/#overview","title":"Overview","text":"<p>Data is growing faster than ever before, and much of the new data generated every second is unstructured. Video, backups, images, and e-mail archives are all examples of unstructured data that can cause issues at scale using traditional file and block storage solutions.</p> <p>Unlike file or block storage, object storage is a data storage architecture designed for unstructured data at the Petabyte scale. Object storage manages data as objects, where each object contains the data itself, a variable amount of metadata, and a globally unique identifier. There is no filesystem overhead as there is in block and file storage, so it can be easily scaled out at a global level.</p> <p>Nutanix Objects is a S3-compatible object storage solution that leverages the underlying Nutanix storage fabric which allows it to benefit from features such as encryption, compression, and erasure coding (EC-X).</p> <p>Objects allow users to store petabytes of unstructured data on the Nutanix platform, with support for features such as WORM (write once, read many) that supports object versioning that is required for regulatory compliance, and easy integration with 3rd party backup software and S3-compatible applications.</p> <p>What are the use cases for Nutanix Objects?</p> <ul> <li> <p>DevOps</p> <ul> <li>Single global namespace for multi-geography collaboration for teams spread around the world</li> <li>S3 support</li> <li>Time-to-first-byte of 10ms or less</li> </ul> </li> <li> <p>Long Term Data Retention</p> <ul> <li>WORM compliance</li> <li>Object versioning</li> <li>Lifecycle policies</li> </ul> </li> <li> <p>Backup Target</p> <ul> <li>Support for HYCU, Commvault, Veeam, Veritas, and others that accept S3 as backup target.</li> <li>Ability to support multiple backup clients simultaneously.</li> <li>Ability to handle really small and really large backup files     simultaneously with a key-value store based metadata     structure and multi-part upload capabilities.</li> </ul> </li> </ul> <p>In this lab, we will walk through a Nutanix Objects deployment and learn how to create, access, and manage buckets.</p>"},{"location":"objects_deploy/objects_deploy/#lab-setup","title":"Lab Setup","text":"<p>This lab requires applications provisioned as part of the Windows Tools VM which is pre-deployed for you.</p> <p>Note</p> <pre><code>Google Chrome is required for this lab.\n</code></pre>"},{"location":"objects_deploy/objects_deploy/#getting-familiar-with-object-storage","title":"Getting Familiar with Object Storage","text":"<p>An object store is a repository for storing objects. Objects are stored in a flat hierarchy and made up of only 3 attributes - an unique key or identifier, the data itself, and an expandable amount of metadata. An object store is a single global namespace on which buckets can be created. A bucket can be thought of as similar to a folder in a file storage environment. However, object storage and file storage are very different. Here are some ways object storage and file storage differ.</p> <p></p>"},{"location":"objects_deploy/objects_deploy/#getting-familiar-with-the-nutanix-objects-environment","title":"Getting Familiar with the Nutanix Objects Environment","text":"<p>This exercise will familiarize you with the Nutanix Objects environment. You will learn:</p> <ul> <li>What constitutes the Microservices Platform (MSP) and the services     that make up Nutanix Objects.</li> <li>How to deploy an Object Store</li> </ul>"},{"location":"objects_deploy/objects_deploy/#view-the-object-storage-services","title":"View the Object Storage Services","text":"<ol> <li> <p>Login into your Prism Central instance.</p> </li> <li> <p>In Prism Central, select  &gt; Services &gt; Objects.</p> <p>View the existing Object Stores. You will be using the ntnx-objects Object Store throughout this lab.</p> </li> <li> <p>Select  &gt; Virtual     Infrastructure &gt; VMs.</p> <p>For the lab deployment, you will see 2 VMs, each preceded with the name of the object store.</p> <p>Objects VM Resource Requirements</p> <pre><code>In a production environment, there would be at a least 5 VMs - 3\nworker VMs (default) and 2 load balancers (envoy). In Medium and\nLarge deployments there would be additional worker and load balancer\nVMs.\n\nFor example, if the name of the object store is **ntnx-objects**,\nthere will be a VM with the name **ntnx-objects-envoy-1**.\n\n| VM             | Purpose                       | vCPUs | Memory     |\n|----------------|-------------------------------|---------|-----------|\n| default-0    |  Kubernetes Node             | 10 | 32 GiB      |\n| envoy-0      |  Load Balancer / Endpoint    |  2 | 4 GiB        |\n</code></pre> </li> </ol> <p>These VMs are deployed by the Microservices Platform (MSP), the Kubernetes-based platform on which multiple future Nutanix services will be run. The service that controls the MSP runs on Prism Central.</p> <p>The default VMs run the Kubernetes cluster. The Kubernetes cluster consists of one or more master nodes, which provides the control plane for the Kubernetes cluster, as well as worker nodes. Kubernetes runs in multi-master mode, which allows for any node to become the master if needed.</p> <p>These nodes run etcd, which is a Kubernetes-level distributed key-value store for storing and replicating the Kubernetes-cluster level metadata. The nodes also run the object store components. This includes:</p> <ul> <li>S3 adapter - Translates the S3 language into the internal system     language</li> <li>Object controller - Handles all the I/O</li> <li>Metadata service - Distributed key-value store to provide     consistency across a massive object store deployment</li> <li>Atlas service - Handles garbage collection and enforces policies     such as life cycle management, versioning, and WORM</li> <li>UI gateway - this is the endpoint for all UI requests, handles     bucket management, stats display, user management interface, etc</li> <li>Zookeeper - Manages the configuration for the object storage cluster</li> <li>IAM service - Handles user authentication for accessing buckets and     objectsd</li> </ul> <p>The envoy VMs provide load balancing across the object controller components. The IP address of these VMs are the IP that can be used by clients to access the object store. It is the first point of entry for an object request (for example, an S3 GET or PUT). It then forwards this request to one of the worker VMs (specifically, the S3 adapter service running as part of the object-controller pod).</p>"},{"location":"objects_deploy/objects_deploy/#walk-through-the-object-store-deployment","title":"Walk Through the Object Store Deployment","text":"<p>In this exercise you will walk through the steps of creating an Object Store.</p> Are you thinking of a Global namespace design? <p>Global Namespace is supported starting from Objects 4.0. We do not include this in this lab but feel free to reach out to the instructor to understand more.</p> <ol> <li> <p>In Prism Central, click Infrastructure &gt; Objects, </p> <p></p> </li> <li> <p>click Create Object Store.</p> <p></p> </li> <li> <p>Review the prerequisites and click Continue.</p> </li> <li> <p>Fill out the following fields:</p> <ul> <li>Object Store Name - ntnx-objects-dr</li> <li>Cluster - your cluster (e.g. PHX-SPOCxyz-a)</li> <li>Worker nodes - 1</li> </ul> <p></p> <p>Note</p> <pre><code>Starting from 4.2, once microservices infrastructure (CMSP) was enabled in Prism Central, all objects clusters will be deployed at the sub domain of Prism Central. That is why by default **prism-central.cluster.local** is selected as the domain of the deployment.\nYou can always add additional domains for the object store after the deployment is completed.\n</code></pre> </li> <li> <p>Click Next.</p> </li> <li> <p>In Storage network settings choose Primary Network</p> </li> <li> <p>Provide two available IP addresses for Object Store Storage Network     static IPs (2 IPs required)</p> <p>Select two available IPs in your network (just ping a few IPs in your cluster's subnet to check if they are avaialable or use a port scanner to determine this)</p> <p>Note</p> <pre><code>Do not use IP addresses that is in the DHCP range, you can go to Prism Element &gt; Network Configuration to check the IP range used for IP address management.\n</code></pre> <p>Warning</p> <pre><code>Due to the lab network setup, some IPs maybe reserved but not in use that you cannot ping it but Objects does not allow you to use those IPs. If you see some errors aobut network in use, try to use other IP addresses.\n</code></pre> <p></p> </li> <li> <p>Click Next</p> </li> <li> <p>In Public network settings choose Primary Network</p> </li> <li> <p>Provide one available consecutive IP addresses in a range, for     Object Store Storage Network static IPs (1 IP required)</p> <p>Select one available IP in your network (just ping a few IPs in your cluster's subnet to check if it is avaialable or use a port scanner to determine this)</p> <p></p> </li> <li> <p>Click on Save and Continue</p> </li> <li> <p>The wizard will run through all System Requirements Validation to     validate resources for Objects store deployment</p> </li> <li> <p>You will see a confirmation screen once all the validation checks     are run</p> <p></p> <p>Note</p> <pre><code>If there are some errors showing in the validation check, you can download the log to check the error messages.\n</code></pre> </li> <li> <p>Click on Create Object Store to create a new Object Store.</p> </li> </ol>"},{"location":"objects_deploy/objects_deploy/#takeaways","title":"Takeaways","text":"<p>What are the key things you should know about Nutanix Objects?</p> <ul> <li>Nutanix Objects provides a simple and scalable S3-compatible object     storage solution, optimized for DevOps, Long Term Retention and     Backup Target use cases.</li> <li>Nutanix Objects can be deployed on an AHV and ESXi clusters on Nutanix.</li> <li>Nutanix Objects will be enabled and deployed from Prism Central.</li> </ul>"},{"location":"objects_multi_cluster/objects_multi_cluster/","title":"Objects: Multi-cluster","text":"<p>Info</p> <p>The estimated time to complete this lab is 10 minutes.</p>"},{"location":"objects_multi_cluster/objects_multi_cluster/#overview","title":"Overview","text":"<p>Data Storage can be expensive and often object stores can scale to very large deployments well into the PetaBytes. Customers need the ability to deploy large scale clusters that can scale across multiple AOS clusters. Nutanix Objects enables customers to do that using the Multi-Cluster feature.</p>"},{"location":"objects_multi_cluster/objects_multi_cluster/#objects-multi-cluster_1","title":"Objects Multi-Cluster","text":"<p>Nutanix Objects Multi-Cluster is able to scale across any AHV or vSphere cluster that meets the following criteria:</p> <ul> <li>Joined to the same Prism Central as primary Objects cluster</li> <li>Has remain storage capacity</li> <li>Is running AOS 5.11.2+</li> <li>Located in same datacenter as primary cluster</li> </ul> <p>Warning</p> <pre><code>  Multi-Cluster is not suitable for cross-WAN communication\n</code></pre> <p>Note</p> <pre><code>  Cross-WAN multi-cluster is supported with global namespace feature on Objects 4.0. Please reach out to the instructor for more detail.\n</code></pre>"},{"location":"objects_multi_cluster/objects_multi_cluster/#lab-setup","title":"Lab Setup","text":"<p>In this lab, you will only walk through a Nutanix Objects Multi-Cluster feature assuming your Prism Central has two registered Prism Element clusters</p> <p></p> <p>At high level we will implement the following:</p> <ul> <li>An Objects instance that utilizes the storage of two AOS clusters</li> </ul> <p>Note</p> <pre><code>Google Chrome is required for this lab.\n</code></pre>"},{"location":"objects_multi_cluster/objects_multi_cluster/#setup-objects-multi-cluster","title":"Setup Objects Multi-Cluster","text":"<p>In the following section you will see how easily you can add additional storage capacity from other Nutanix AOS clusters into your Nutanix Objects deployment.</p> <ol> <li> <p>In Prism Central, select  &gt; Services &gt; Objects</p> <p></p> </li> <li> <p>Choose your Objects Store</p> <p></p> </li> <li> <p>Click on Clusters</p> <p></p> </li> <li> <p>Choose Add Cluster</p> <p></p> </li> <li> <p>Select your secondary cluster</p> <p>Note</p> <p>Only compatible clusters will appear in this window. Nutanix Objects    will only use storage on secondary cluster, no additional compute    resources are required.</p> <p></p> </li> <li> <p>Configure storage limitations for Nutanix Objects on secondary     cluster and click Done</p> <p>Note</p> <p>Storage will be utilized in a round-robin fashion across all of the    clusters registered to Objects Multi-Cluster. Once a cluster nears    it's storage hard limit, it will be placed into a secondary pool,    receiving no further writes until all clusters registered are placed    in that secondary pool. This ensures no cluster runs out of storage.</p> <p></p> </li> <li> <p>View Clusters Listed in Clusters pane</p> <p>Note</p> <p>Secondary cluster storage will be made available within just a    couple of minutes after adding it to Multi-Cluster.</p> <p></p> </li> </ol>"},{"location":"objects_multi_cluster/objects_multi_cluster/#takeaways","title":"Takeaways","text":"<p>What are the key things you should know about Nutanix Objects Multi-Cluster?</p> <ul> <li>Nutanix Objects allows easy configuration for adding additional AOS     clusters to your object store deployment</li> <li>Intelligent distribution of data allows storage to be properly     utilized across clusters of disparate sizes</li> <li>Multi-Cluster allows customers to reclaim land-locked storage in     clusters that have spare capacity</li> </ul>"},{"location":"objects_multiprotocol/objects_multiprotocol/","title":"Objects: Multiprotocol","text":"<p>In this section, we will see how to create a bucket and let a NFS client access this to be able to do regular folder and files operations.</p> <ul> <li>Objects multiprotocol is a new feature with Objects 3.3.1</li> <li>Multiprotocol feature only works with a fresh install of Object     3.3.1 onwards<ul> <li>Upgrading existing Objects 3.2.1 to 3.3.1 or above doesn't     allow for this functionality (this is due to a MSP integration     issue. Might be resolved in future)</li> </ul> </li> <li>Multiprotocol enabled bucket doesn't support versioning of objects     that it hosts</li> <li>This lab uses a fresh install of Objects 3.4.0.2 - Objects     multiprotocol feature is available as is</li> </ul>"},{"location":"objects_multiprotocol/objects_multiprotocol/#lab-agenda","title":"Lab Agenda","text":"<p>We will do the following in this lab:</p> <ul> <li>Configure NFS access to a specific NFS client (your     Initials-Linux-ToolsVM) for the objects store</li> <li>Create bucket and specify NFS multiprotocol access</li> <li>Mount the bucket as a NFS share in the NFS client (your     Initials*-Linux-ToolsVM)</li> <li>Perform file and folder level operation in the NFS client</li> <li>Perform file and folder level operation in the objects browser</li> </ul>"},{"location":"objects_multiprotocol/objects_multiprotocol/#configure-nfs-allow-client","title":"Configure NFS Allow Client","text":"<p>In this section we will allow your LinuxToolsVM to be able to access buckets using NFS3 protocol.</p> <ol> <li> <p>Go to Prism Central</p> </li> <li> <p>Go to  &gt; Services &gt; Objects</p> </li> <li> <p>Click on ntnx-objects Objects Store</p> </li> <li> <p>The Objects Store management will open in a new browser tab</p> </li> <li> <p>Click on NFS Clients Allowlist</p> </li> <li> <p>Click on Add Client</p> <p></p> </li> <li> <p>Enter your Initials-Linux-ToolsVM IP address followed by <code>/32</code>     to specify access only to this client</p> <p></p> <p>Note</p> <p>You are also able to allow a range of clients by denoting the a CIDR    block</p> <p>E.g. 10.42.32.192/<code>26</code> 10.42.4.128/<code>25</code></p> </li> <li> <p>Click on + Add and Save at the bottom of the pop-up window</p> </li> </ol>"},{"location":"objects_multiprotocol/objects_multiprotocol/#create-bucket-in-prism","title":"Create Bucket In Prism","text":"<p>A bucket is a sub-repository within an object store which can have policies applied to it, such as versioning, WORM, etc. By default a newly created bucket is a private resource to the creator. The creator of the bucket by default has read/write permissions, and can grant permissions to other users.</p> <ol> <li> <p>Go to Prism Central</p> </li> <li> <p>Go to  &gt; Services &gt; Objects</p> </li> <li> <p>Click on ntnx-objects Objects Store</p> </li> <li> <p>The Objects Store management will open in a new browser tab</p> </li> <li> <p>Click Create Bucket, and fill out the following fields:</p> <ul> <li>Name - Intials-multi</li> <li>Enable NFS v3 access - checked</li> <li>Owner UID - 8888</li> <li>Owner GID - 8888</li> </ul> </li> <li> <p>For Default Access Permissions select in the following:</p> <p>Warning</p> <pre><code>  We are configuring all permissions and priveleges here. However in a\n  production environment you would be careful about giving the\n  appropriate permissions.\n</code></pre> <ul> <li>Files<ul> <li>Owner - read, write, execute</li> <li>Group - read, write, execute</li> <li>Others - read, write, execute</li> </ul> </li> <li>Directory<ul> <li>Owner - read, write, execute</li> <li>Group - read, write, execute</li> <li>Others - read, write, execute</li> </ul> </li> <li>Advanced Settings - default (leave as-is)</li> </ul> <p></p> </li> <li> <p>Click Create</p> </li> </ol>"},{"location":"objects_multiprotocol/objects_multiprotocol/#buckets-user-management","title":"Buckets User Management","text":"<p>Warning</p> <pre><code>  Perform these steps only if you **have not** created a user before and\n  **have not** downloaded the access keys in the previous [Buckets &amp; UAC](../objects_buckets_users_access_control/objects_buckets_users_access_control.md) section.\n</code></pre> <p>In this exercise you will create generate access and secret keys to access the object store, that will be used throughout the lab.</p> <ol> <li> <p>Go to Prism Central</p> </li> <li> <p>Go to  &gt; Services &gt; Objects</p> </li> <li> <p>From the Objects UI, click on Access Keys and click Add People.</p> <p></p> </li> <li> <p>Select Add people not in a directory service and enter your     e-mail address.</p> <p></p> </li> <li> <p>Click Next.</p> </li> <li> <p>Click Generate Keys to generate a ket.</p> <p></p> </li> <li> <p>Click Download Keys to download a <code>.txt</code> file containing the     Access Key and Secret Key.</p> <p></p> </li> <li> <p>Click Close.</p> </li> <li> <p>Open the file with a text editor.</p> <p></p> </li> </ol>"},{"location":"objects_multiprotocol/objects_multiprotocol/#adding-users-to-buckets-share","title":"Adding Users to Buckets Share","text":"<p>In this section, we will add user to the Intials-multi bucket, so we can access the bucket to upload/create files and folders.</p> <ol> <li> <p>Go to Prism Central</p> </li> <li> <p>Go to  &gt; Services &gt; Objects</p> </li> <li> <p>Click on ntnx-objects Objects Store</p> </li> <li> <p>The Objects Store management will open in a new browser tab (if it     not already open)</p> </li> <li> <p>Click on Intials-multi bucket, and click on Share</p> <p></p> </li> <li> <p>Click on Edit User Access button</p> <p>This is where you will be able to share your bucket with other users. You can configure read access (download), write access (upload), or both, on a per user basis.</p> </li> <li> <p>Add the user (email address)you created earlier, with Read and     Write permissions</p> <p></p> </li> <li> <p>Click on Save</p> </li> </ol>"},{"location":"objects_multiprotocol/objects_multiprotocol/#accessing-bucket-on-nfs-client","title":"Accessing Bucket on NFS Client","text":"<p>In this section we will mount the Intials-multi bucket as a NFSv3 share on the initials-Linux-ToolsVM to create files and folders.</p> <ol> <li> <p>Login to the Initials-Linux-ToolsVM, with the following     credentials</p> <ul> <li>Username - root</li> <li>Password - default nutanix password</li> </ul> </li> <li> <p>Optional step - make sure nfs-utils package is installed if not     already done so.</p> <pre><code>yum install -y nfs-utils\n</code></pre> </li> <li> <p>Change user to centos</p> <pre><code>sudo su - centos\n</code></pre> </li> <li> <p>Edit the <code>/etc/fstab</code> file to include the following nfs mount</p> <pre><code>sudo vi /etc/fstab\nsudo mkdir -p /mnt/buckets\n\n# Add this line to the end of the file\n\n&lt;object-store-IP&gt;:/xyz-multi /mnt/buckets   nfs rw,noauto,user 0 0\n# example below\n# 10.42.32.136:/xyz-multi /mnt/buckets  nfs rw,noauto,user 0 0\n</code></pre> </li> <li> <p>Mount the bucket as a NFS share</p> <pre><code>mount /mnt/buckets\n</code></pre> </li> <li> <p>Create a directory and some files under the new directory</p> <p><pre><code>cd /mnt/buckets\nmkdir mydir1\ncd mydir1\n</code></pre> <pre><code>for i in {1..10}; do echo \"writing file$i ..\"; touch file$i.txt; echo \"this is file$i\" &gt; file$i.txt; done\n</code></pre> <pre><code># list your files\nls -l\n</code></pre> <pre><code>[mydir1]$ ll\n# output here\n-rw-rw-r-- 1 centos centos   15 Feb 23 23:25 file10.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file1.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file2.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file3.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file4.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file5.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file6.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file7.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file8.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file9.txt\n</code></pre></p> </li> <li> <p>Now go to the Objects browser GUI by going back to Prism Central</p> </li> <li> <p>Go to  &gt; Services &gt; Objects</p> </li> <li> <p>Click on ntnx-objects Objects Store</p> </li> <li> <p>The Objects Store management will open in a new browser tab</p> </li> <li> <p>Click on Intials-multi bucket and Launch Objects Browser</p> <p>This will open in a new browser tab</p> <p></p> </li> <li> <p>Provide the access key and secret key you downloaded before in the     <code>buckets_sharing</code>{.interpreted-text role=\"ref\"} section</p> <p></p> </li> <li> <p>Click on Login</p> </li> <li> <p>Check if your files are present in the Intials-multi bucket</p> <p></p> <p>Info</p> <p>Although you see directories, these are mere objects. It is a cosmetic    representation of a folder like structure in Objects Browser. However, if you analyse the buckets, these objects would be stored differently.</p> </li> <li> <p>Download one of the files, by selecting the file and selecting     Download from the drop down menu.</p> <p></p> </li> <li> <p>Verify the contents of the file</p> <p></p> </li> <li> <p>Create a new sub-directory through Object Browser by clicking on +     New Folder and entering the name mysubdir1</p> </li> <li> <p>Click on Create</p> <p></p> </li> <li> <p>Return to your Initials-Linux-ToolsVM and list the share to     see if newly created <code>subdir1</code> is present</p> <pre><code>[mydir]$ ll\n\n# output here\n#\n-rw-rw-r-- 1 centos centos   15 Feb 23 23:25 file10.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file1.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file2.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file3.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file4.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file5.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file6.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file7.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file8.txt\n-rw-rw-r-- 1 centos centos   14 Feb 23 23:25 file9.txt\ndrwxrwxrwx 2   8888   8888 4096 Feb 23 23:01 mysubdir1  # &lt;&lt; this is the sub directory you created in Objects Browser\n\n# Note the the UID and GID for the directory created from Objects Browser side\n</code></pre> </li> <li> <p>Add a few more folders and files from the Objects browser side and     check if it shows on the NFS client side.</p> </li> </ol> <p>You have successfully completed this lab and tested multiprotocol access to a bucket.</p>"},{"location":"objects_multiprotocol/objects_multiprotocol/#takeaways","title":"Takeaways","text":"<ul> <li>Objects 3.3.x onwards allows multiprotocol access for objects</li> <li>This is recommended for read-heavy workloads with sequential     accesses, E.g. Backup targets, log archives, large media files, etc.     Access cannot be enabled or disabled once the bucket is created.</li> <li>Administrators can easily switch between access patterns (s3 or NFSv3) to suit their requirements with managing objects</li> </ul>"},{"location":"objects_replication/objects_replication/","title":"Objects Streaming Replication","text":"<p>Info</p> <pre><code>The estimated time to complete this lab is 20 minutes\n</code></pre>"},{"location":"objects_replication/objects_replication/#overview","title":"Overview","text":"<p>Streaming Replication is a data protection service for Nutanix Objects.</p> <p>With objects streaming replication, you can replicate objects between different Nutanix object stores using streaming replication, a native data protection capability of Nutanix Objects. Streaming replication copies every object (and object part) that is written to a protected bucket to a corresponding remote bucket (or buckets) as soon as the PUT request finishes. Streaming replication copies object metadata and WORM lock timers along with the data. Streaming replication generally provides a low recovery-point objective (RPO), although factors such as write rate, object size, and intersite bandwidth all influence the RPO.</p> <p>Nutanix Objects supports up to five source buckets replicating to a single destination bucket (fan-in replication) and one source bucket replicating to up to three destination buckets (fan-out replication).</p> <p>With Nutanix Objects versions 3.5 and later, you can skip replicating delete markers to the destination bucket so that deletions on the source bucket aren't mirrored on the destination bucket. The life-cycle policies on the source and destination buckets are also entirely separate so that the destination bucket can have a much longer retention period than the source bucket.</p> <p>The source and destination buckets are read and write, meaning that a Nutanix Objects disaster recovery setup is inherently active-active. This setup helps speed up the failover and failback processes between object stores.</p>"},{"location":"objects_replication/objects_replication/#lab-setup","title":"Lab Setup","text":"<p>We will use the two Object Stores that has been created for this lab setup.</p> <p>Note</p> <pre><code>Make sure you have finished the labs of \"Deploy Nutanix Objects\", \"Create Buckets\" before this lab.\n</code></pre>"},{"location":"objects_replication/objects_replication/#create-replication-rule","title":"Create replication rule","text":"<ol> <li> <p>Go to Prism Central, click Infrastructure &gt; Objects</p> </li> <li> <p>In Object Stores, click ntnx-objects to enter another UI to manage ntnx-objects.</p> </li> <li> <p>Go to Buckets, select the bucket you created previously - your-name-my-bucket, then Actions &gt; Replications Rules.</p> <p></p> </li> <li> <p>From the bucket management UI, click + Create Rule.</p> </li> <li> <p>In Namespace, choose ntnx-objects-dr and put your-name-my-bucket-dr as the bucket name, then click Save.</p> </li> <li> <p>After it is saved, you can see the replication rule created.</p> <p></p> </li> </ol>"},{"location":"objects_replication/objects_replication/#verify-replication-behavior","title":"Verify replication behavior","text":"<ol> <li> <p>Go to Prism Central, click Infrastructure &gt; Objects</p> </li> <li> <p>Select ntnx-objects and then click Actions &gt; Launch Objects Browser</p> </li> <li> <p>Login using the key pairs generated before.</p> </li> <li> <p>Login to ntnx-objects-dr Objects Browser following the same steps.</p> </li> <li> <p>Go to both buckets you created from the two object stores, try to upload different files from two buckets and see if the objects get replicated to another object store.</p> </li> </ol>"},{"location":"objects_replication/objects_replication/#takeaway","title":"Takeaway","text":"<p>Nutanix Objects Streaming Replication enables a simplified management of object replication of buckets between multiple object stores, allows bi-directional active-active replication without the need of third-party tools.</p>"},{"location":"objects_versioning_access_control/objects_versioning_access_control/","title":"Objects: Versioning and Access Controls","text":"<p>Info</p> <p>The estimated time to complete this lab is 60 minutes.*</p>"},{"location":"objects_versioning_access_control/objects_versioning_access_control/#overview","title":"Overview","text":""},{"location":"objects_versioning_access_control/objects_versioning_access_control/#accessing-creating-buckets-with-objects-browser","title":"Accessing &amp; Creating Buckets With Objects Browser","text":"<p>In this exercise you will use Objects Browser to create and use buckets in the object store using your generated access key.</p>"},{"location":"objects_versioning_access_control/objects_versioning_access_control/#download-the-sample-images","title":"Download the Sample Images","text":"<ol> <li>Login to Initials-Windows-ToolsVM via RDP using the following     credentials:<ul> <li>Username - NTNXLAB\\Administrator</li> <li>password - ask your Instructor</li> </ul> </li> <li>Use this     link     to download the sample files to your Windows-ToolsVM. Once the     download is complete, extract the contents of the .zip file.</li> </ol>"},{"location":"objects_versioning_access_control/objects_versioning_access_control/#use-object-browser-to-create-a-bucket","title":"Use Object Browser to Create A Bucket","text":"<ol> <li> <p>From the Objects UI, Locate the ntnx-objects object store.</p> <p></p> </li> <li> <p>The objects browser will open in a new tab</p> </li> <li> <p>Enter the following fields from the downloaded key file (while you     created user access) and click Login button:</p> <ul> <li>Access Key - Generated When User Created</li> <li>Secret Key - Generated When User Created</li> </ul> <p></p> </li> <li> <p>Click the + Create Bucket.</p> </li> <li> <p>Enter the following name for your bucket, and click Create:</p> <ul> <li>Bucket Name - intials-test-bucket</li> <li>Enable versioning - checked</li> </ul> <p></p> <p>Note</p> <p>Bucket names must be lower case and only contain letters, numbers,    periods and hyphens.</p> <p>Additionally, all bucket names must be unique within a given Object    Store. Note that if you try to create a folder with an existing    bucket name (e.g. your-name-my-bucket), creation of the folder    will not succeed.</p> <p>Creating a bucket in this fashion allows for self-service for entitled users, and is no different than a bucket created via the Prism Buckets UI.</p> </li> <li> <p>Click into your intials*-test-bucket bucket, and Click the + and Upload Objects**.</p> </li> <li> <p>Click on Select Folder</p> <p></p> </li> <li> <p>Navigate to your downloads directory and find the Sample Pictures     folder. Upload one or more pictures to your bucket.</p> </li> <li> <p>That is how easy it is to use the Objects Browser.</p> </li> </ol>"},{"location":"objects_versioning_access_control/objects_versioning_access_control/#object-versioning","title":"Object Versioning","text":"<p>Object versioning allows the upload of new versions of the same object for required changes, without losing the original data. Versioning can be used to preserve, retrieve and restore every version of every object stored within a bucket, allowing for easy recovery from unintended user action and application failures.</p> <ol> <li> <p>Open Notepad in Initials-Windows-ToolsVM.</p> </li> <li> <p>Type \"version 1.0\" in Notepad, then save the file with a name (e.g.     A.txt)</p> </li> <li> <p>In Objects Browser, upload the text file to your     intials*-test-bucket** bucket.</p> </li> <li> <p>Make changes to the text file (e.g. A.txt) in Notepad and save it     with the same name, overwriting the original file.</p> </li> <li> <p>Upload the modified file to your bucket. If desired, you can update     and upload the file multiple times.</p> </li> <li> <p>Back in Prism Cental &gt; Services &gt; Objects, click on Object     Stores.</p> </li> <li> <p>In Objects Browser of your intials*-test-bucket** bucket</p> </li> <li> <p>Select your A.txt file</p> </li> <li> <p>Under the Actions menu, scroll down to select Versions</p> </li> <li> <p>You should be able to see the different versions of the file A.txt</p> <p></p> </li> </ol>"},{"location":"objects_versioning_access_control/objects_versioning_access_control/#takeaways","title":"Takeaways","text":"<p>What are the key things you should know about Nutanix Objects?</p> <ul> <li>Nutanix Objects provides a simple and scalable S3-compatible object     storage solution, optimized for DevOps, Long Term Retention and     Backup Target use cases.</li> <li>Nutanix Objects can be deployed on an AHV cluster, with ESXi support     on the roadmap.</li> <li>Nutanix Objects will be enabled and deployed from Prism Central.</li> </ul>"},{"location":"peer/peer/","title":"Peer Global File Service {#peer}","text":"<p>The estimated time to complete this lab is 60 minutes.</p> <p>Google Chrome, Apple Safari, or Microsoft Edge is recommended for this lab.</p>"},{"location":"peer/peer/#overview","title":"Overview","text":"<p>The explosive growth of unstructured data has driven organizations to seek solutions to efficiently store, share, manage, and protect an ever-growing universe of data while deriving new value and intelligence. Since 1993, Peer Software has focused on these requirements and more by building best-of-breed data management and real-time replication solutions for distributed on-premises and cloud storage environments.</p> <p>Peer's flagship offering, Peer Global File Service (PeerGFS), features enterprise-class replication technology with integrated file locking and a globally accessible namespace that powers multi-site, multi-vendor, and multi-cloud deployment.</p> <p>PeerGFS enables fast local data access for users and applications at different locations, protects against version conflicts, makes data highly available, and allows Nutanix Files to co-exist with legacy NAS platforms to ease adoption of Files into existing environments.</p> <p>Key use cases for combining Peer Software with Nutanix include:</p> <ul> <li>Global File Sharing and Collaboration - Deliver fast local     access to shared project files for distributed teams while ensuring     version integrity and high availability.</li> <li>HA and Load Balancing for User and Application Data - Enable     high availability and load balancing of end user data as well as     application data.</li> <li>Storage Interoperability - Cross-platform support powers     coexistence of Nutanix Files with existing NAS and hybrid cloud     storage systems, as well as between file and object formats.</li> <li>Analysis and Migration - Analyze existing storage for resource     planning, optimization, and migrations. Analysis combined with     real-time replication powers minimally disruptive data migrations.</li> </ul> <p>How does it work?</p> <p></p> <p>Working from left to right, users interact with the SMB shares on the Nutanix Files cluster via a public LAN. When SMB activity occurs on the Files cluster through these shares, the Peer Partner Server (referred to as a Peer Agent) is notified via the File Activity Monitoring API from Files. The Peer Agent accesses the updated content via SMB, and then facilitates the flow of data to one or many remote and/or local file servers.</p> <p>In this lab you will configure Peer Global File Service to create an Active-Active file services solution with Nutanix Files, replicate content from Nutanix Files to Nutanix Objects, and use our File System Analyzer tool to analyze some sample data.</p>"},{"location":"peer/peer/#lab-setup","title":"Lab Setup","text":"<p>::: note ::: title Note :::</p> <p>This lab requires the <code>windows_tools_vm</code>{.interpreted-text role=\"ref\"}. :::</p>"},{"location":"peer/peer/#files","title":"Files","text":"<p>This lab requires an existing Nutanix Files deployment on your assigned cluster. Details on how to configure Nutanix Files for use with Peer Global File Service can be found in the Configuring Nutanix Files section below.</p>"},{"location":"peer/peer/#peer-vms","title":"Peer VMs","text":"<p>In this exercise, you will be using three shared VMs, all of which should already be available on your assigned cluster.</p> <p>VM Name Description</p> <p>PeerMgmt            This server is running the Peer Management                           Center.</p> <p>PeerAgent-Files     This server will manage the Nutanix Files                           cluster.</p> <p>PeerAgent-Win       This Windows File Server will be used as a                           target for replication.</p>"},{"location":"peer/peer/#configuring-nutanix-files","title":"Configuring Nutanix Files","text":"<p>Peer Global File Service requires both a File Server Admin account as well as REST API access to orchestrate replication to or from Nutanix Files.</p> <ol> <li> <p>Log in to Prism Element (e.g. 10.XX.YY.37) on your Nutanix     cluster.</p> </li> <li> <p>Navigate to File Server from the drop down navigation and select     the BootcampFS deployment.</p> </li> <li> <p>Click + Share/Export and fill out the following fields:</p> <ul> <li>Name - Initials-Peer</li> <li>Description (Optional) - Leave blank.</li> <li>File Server - BootcampFS</li> <li>Share Path (Optional) - Leave blank.</li> <li>Max Size (Optional) - Leave blank.</li> <li>Select Protocol - SMB</li> </ul> <p></p> </li> <li> <p>Click Next, Next, and then Create.</p> </li> <li> <p>Click Manage roles.</p> <p></p> </li> <li> <p>Under Add admins, NTNXLAB\\Administrator should already be     added as a File Server Admin. If not, click + New user and     add NTNXLAB\\Administrator.</p> <p></p> <p>::: note ::: title Note :::</p> <p>In a production environment, you would likely use an Active Directory service account for Peer. :::</p> </li> <li> <p>Under REST API access users, check to see if a peer account     has already been created. If not, click + Add new user, fill out     the following fields, and click Save:</p> <ul> <li> <p>Username - peer</p> <p>The username must be in all lower case.</p> </li> <li> <p>Password - nutanix/4u</p> </li> </ul> <p></p> <p>::: note ::: title Note :::</p> <p>All participants on a single Nutanix AOS cluster will be sharing the same BootcampFS file server, as well as the peer API account. :::</p> </li> <li> <p>Click Close.</p> </li> </ol>"},{"location":"peer/peer/#staging-test-data-on-peeragent-win","title":"Staging Test Data on PeerAgent-Win","text":"<p>The final step of staging the lab is creating some sample data on PeerAgent-Win, which will be acting as a Windows File Server. Peer is capable of replicating between multiple Files clusters, as well as between a mix of Files and other NAS platforms. For this lab, you will be replicating between your Nutanix Files cluster and a Windows File Server.</p> <ol> <li> <p>Connect to your Initials-Windows-ToolsVM via RDP using the     following credentials:</p> <ul> <li>Username - NTNXLAB\\Administrator</li> <li>Password - nutanix/4u</li> </ul> </li> <li> <p>Open File Explorer and navigate to <code>\\\\PeerAgent-Win\\Data</code>.</p> </li> <li> <p>Create a copy of the Sample Data folder. Rename the copy to     Initials-Data as shown below.</p> <p></p> </li> </ol>"},{"location":"peer/peer/#connecting-to-the-peer-management-center-web-interface","title":"Connecting to the Peer Management Center Web Interface","text":"<p>The Peer Management Center (PMC) serves as the centralized management component for Peer Global File Service. It does not store any file data but does facilitate communication between locations, so it should be deployed at a location with the best connectivity. A single deployment of PMC can manage 100 or more Agents/file servers.</p> <p>For this lab, you will be accessing a shared PMC deployment via a web interface.</p> <ol> <li> <p>Open a non-Firefox browser (Chrome, Edge, and Safari will all work)     on your Initials-Windows-ToolsVM VM or on your laptop.</p> </li> <li> <p>If you are using a browser on your Initials-Windows-ToolsVM     VM, browse to https://PeerMgmt:8443/hub</p> </li> <li> <p>If you are using a browser on your laptop, log in to Prism     Element (e.g. 10.XX.YY.37) on your Nutanix cluster to find the IP     of the PeerMgmt VM, then browse to     https://IP-of-PeerMgmt-Server:8443/hub</p> </li> <li> <p>When prompted to login, use the following credentials:</p> <ul> <li>Username - admin</li> <li>Password - nutanix/4u</li> </ul> </li> <li> <p>Once connected, confirm that PeerAgent-Files and     PeerAgent-Win both appear in green in the Agents view in the     bottom left of the PMC web interface.</p> <p></p> <p>::: note ::: title Note :::</p> <p>If the two Agent servers are not appearing in the bottom left of the PMC web interface, go into Prism and reboot the VMs named PeerAgent-Files and PeerAgent-Win. :::</p> </li> </ol>"},{"location":"peer/peer/#introduction-to-peer-global-file-service","title":"Introduction to Peer Global File Service","text":"<p>Peer Global File Service utilizes a job-based configuration engine. Several different job types are available to help tackle different file management challenges. A job represents a combination of:</p> <ul> <li>Peer Agents.</li> <li>The file servers that are being monitored by those Agents.</li> <li>A specific share/volume/folder of data on each file server.</li> <li>Various settings tied to replication, synchronization and/or     locking.</li> </ul> <p>When creating a new job, you will be prompted by a dialog outlining the different job types and why you would use each type.</p> <p>Available job types include:</p> <ul> <li>Cloud Backup and Replication - Real-time replication from     enterprise NAS devices to public and private object storage with     support for volume-wide point-in-time recovery. Each file is stored     as a single, transparent object with optional version tracking.</li> <li>DFS-N Management - Manages new and existing Microsoft DFS     Namespaces. Can be combined with File Collaboration and/or File     Synchronization jobs to automate DFS failover and failback.</li> <li>File Collaboration - Real-time synchronization combined with     distributed file locking to power global collaboration and project     sharing across enterprise NAS platforms, locations, cloud     infrastructures, and organizations.</li> <li>File Replication - One-way real-time replication from enterprise     NAS platforms to any SMB destination.</li> <li>File Synchronization - Multi-directional real-time     synchronization powering high availability of user and application     data across enterprise NAS platforms, locations, cloud     infrastructures, and organizations.</li> </ul>"},{"location":"peer/peer/#creating-a-new-file-collaboration-job","title":"Creating a New File Collaboration Job","text":"<p>In this section, we will focus on File Collaboration.</p> <ol> <li> <p>In the PMC Web Interface, click File &gt; New Job.</p> </li> <li> <p>Select File Collaboration and click Create.</p> <p></p> </li> <li> <p>Enter Initials - Collab as the name for the job and click     OK.</p> <p></p> </li> </ol>"},{"location":"peer/peer/#files-and-peeragent-files","title":"Files and PeerAgent-Files","text":"<ol> <li> <p>Click Add to begin pairing a Peer Agent with your Nutanix Files     cluster.</p> <p></p> </li> <li> <p>Select Nutanix Files and click Next.</p> <p></p> </li> <li> <p>Select the Agent named PeerAgent-Files and click Next. This     Agent will manage the Files cluster.</p> <p></p> </li> <li> <p>On the Storage Information page, you are prompted to enter     credentials for accessing the storage device. If another participant     sharing your Files cluster has already done the Peer lab, you can     select Existing Credentials as shown here.</p> <p></p> <p>If you are the first participant on this cluster to do the Peer lab, New Credentials will be automatically selected. Fill out the following fields:</p> <ul> <li> <p>Nutanix Files Cluster Name - BootcampFS</p> <p>The NETBIOS name of the Files cluster that will be paired with the Agent selected in the previous step.</p> </li> <li> <p>Username - peer</p> <p>This is the Files API account username configured earlier in the lab and must be in all lower case.</p> </li> <li> <p>Password - nutanix/4u</p> <p>The password associated with the Files API account.</p> </li> <li> <p>Peer Agent IP - PeerAgent-Files IP Address</p> <p>The IP address of the Agent server that will receive real-time notifications from the File Activity Monitoring API built into Files. It is selectable from a drop-down list of available IPs on this Agent server.</p> </li> </ul> </li> <li> <p>Click Validate to confirm Files can be accessed via API using     the provided credentials.</p> <p></p> <p>::: note ::: title Note :::</p> <p>Once you enter these credentials, they are reusable when creating new jobs that use this particular Agent. When you create your next job, select Existing Credentials on this page to display a list of previously configured credentials. :::</p> </li> <li> <p>Click Next.</p> </li> <li> <p>Click Browse to select the share you wish to replicate. You can     also navigate to a subfolder below a share.</p> </li> <li> <p>Select your Initials-Peer share and click OK.</p> <p></p> <p>::: note ::: title Note :::</p> <p>Peer Global File Service supports the replication of data within nested shares starting with Nutanix Files v3.5.1 and above. :::</p> <p>::: note ::: title Note :::</p> <p>You can only select a single share or folder. You will need to create an additional job for each additional share you wish to replicate. :::</p> </li> <li> <p>Click Finish. You have now completed pairing PeerAgent-Files     to Nutanix Files.</p> <p></p> </li> </ol>"},{"location":"peer/peer/#peeragent-win","title":"PeerAgent-Win","text":"<p>To simplify this lab exercise, a second Peer Agent server running on the same cluster will function as a standard Windows File Server. While Peer can be used to replicate shares between Nutanix Files clusters, one of its key advantages is the ability to work with a mix of NAS platforms. This can help drive adoption of Nutanix Files when only a single site has been refreshed with Nutanix Files, but replication is still required to support collaboration or disaster recovery.</p> <ol> <li> <p>Repeat Steps 1-8 in Files and     PeerAgent-Files to add     PeerAgent-Win to the job,     <code>&lt;strong&gt;&lt;font color=\"red\"&gt;making the following changes&lt;/font&gt;&lt;/strong&gt;</code>{=html}:</p> <ul> <li>Storage Platform - Windows File Server</li> <li>Management Agent - PeerAgent-Win</li> <li>Path - C:\\Data\\Initials-Data</li> </ul> <p></p> </li> <li> <p>Click Next.</p> </li> </ol>"},{"location":"peer/peer/#completing-collaboration-job-configuration","title":"Completing Collaboration Job Configuration","text":"<p>Peer offers robust functionality for handling the synchronization of NTFS permissions between shares:</p> <ul> <li> <p>Enable synchronizing NTFS security descriptors in real-time</p> <p>Select this checkbox if you want changes to file and folder permissions to be replicated to the remote file servers as they occur.</p> </li> <li> <p>Enable synchronizing NTFS security descriptors with master host     during initial scan</p> <p>Select this if you want the initial scan to look for and replicate any permissions that are not in sync across file servers. This requires selecting a master host to help resolve situations where the engine cannot pick a winner in a permission discrepancy.</p> </li> <li> <p>Synchronize Security Description Options</p> <p>(Optional) Select the NTFS permission types you would like to replicate.</p> <ul> <li> <p>Owner</p> <p>The NTFS Creator-Owner who owns the object (which is, by default, whoever created it).</p> </li> <li> <p>DACL</p> <p>A Discretionary Access Control List identifies the users and groups that are assigned or denied access permissions on a file or folder.</p> </li> <li> <p>SACL</p> <p>A System Access Control List enables administrators to log attempts to access a secured file or folder. It is used for auditing.</p> </li> </ul> </li> <li> <p>File Metadata Conflict Resolution</p> <p>If there is a permission discrepancy between two or more sites, the permissions set on the file server tied to the master host will override those on the other file servers.</p> </li> <li> <p>For the purposes of this lab exercise, accept the default     configuration and click Next.</p> <p></p> </li> <li> <p>Under Application Support, select Microsoft Office.</p> <p>The Peer synchronization and locking engine is automatically optimized to best support any of the selected applications.</p> <p></p> </li> <li> <p>Click Finish to complete the job setup.</p> </li> </ul>"},{"location":"peer/peer/#starting-a-collaboration-job","title":"Starting a Collaboration Job","text":"<p>Once a job has been created, it must be started to initiate synchronization and file locking.</p> <ol> <li> <p>In the PMC Web Interface, under Jobs, right-click on your     newly created job, and then select Start.</p> <p></p> <p>When the job starts:</p> <ul> <li>Connectivity to all Agents and Files clusters (or other NAS     devices) is checked.</li> <li>The real-time monitoring engine is initialized.</li> <li>A background scan is kicked off to ensure all file servers are     in sync with another.</li> </ul> </li> <li> <p>Double-click the job in the Job pane to view its runtime     information and statistics.</p> <p>::: note ::: title Note :::</p> <p>Click Auto-Update to have the console regularly refresh as files begin replicating. :::</p> <p></p> </li> </ol>"},{"location":"peer/peer/#testing-collaboration","title":"Testing Collaboration","text":"<p>The easiest way to verify synchronization is functioning properly is to open separate File Explorer windows for the Nutanix Files and Windows File Server paths.</p> <p>::: note ::: title Note :::</p> <p>Do not test using an Agent server VM. All activity from these servers are automatically filtered to reduce overhead on the Nutanix Files cluster. :::</p> <ol> <li> <p>Connect to your Initials-Windows-ToolsVM via RDP using the     following credentials:</p> <ul> <li>Username - NTNXLAB\\Administrator</li> <li>Password - nutanix/4u</li> </ul> </li> <li> <p>Open File Explorer and browse to your Nutanix Files share, e.g.,     <code>\\\\BootcampFS\\Initials-Peer</code>. Drag this window to the left side of     the desktop.</p> <p>Note that the sample data seeded in the Windows File Server during lab setup has already been replicated to Nutanix Files.</p> <p>::: note ::: title Note :::</p> <p>You can also verify the replicated files in Prism &gt; File Server. :::</p> </li> <li> <p>Open a second File Explorer window and browse to your Windows File     Server share, e.g., <code>\\\\PeerAgent-Win\\Data\\Initials-Data</code>. Drag this     window to the right side of the desktop.</p> <p></p> </li> <li> <p>In the File Explorer on the left, create a copy of one of the sample     data directories by copying and pasting within the root of the share     (shown below).</p> <p></p> <p></p> </li> <li> <p>The changes that are performed on the Nutanix Files share will be     sent to its paired Agent; the Agent will then facilitate the     replication of these files and folders to the other server (and vice     versa).</p> <p></p> </li> <li> <p>To test file locking, create a new OpenDocument Text file within the     root of your Nutanix Files share, e.g.,     <code>\\\\BootcampFS\\Initials-Peer</code>.</p> <p></p> </li> <li> <p>Name the file. Within a few seconds, it should appear under your     Windows File Server share, e.g.,     <code>\\\\PeerAgent-Win\\Data\\Initials-Data</code>.</p> <p></p> </li> <li> <p>Open the file under the Nutanix Files share with OpenOffice Writer.     Next, open the file with the same name under     <code>\\\\PeerAgent-Win\\Data\\Initials-Data</code>. You should see the following     warning that the file is locked.</p> <p></p> <p>Congratulations! You have successfully deployed an Active-Active file share replicated across two file servers. Using Peer, this same approach can be leveraged to support file collaboration across sites, migrations from legacy solutions to Nutanix Files, or disaster recovery for use cases such as VDI, where user data and profiles need to be accessible from multiple sites for business continuity.</p> </li> </ol>"},{"location":"peer/peer/#working-with-nutanix-objects","title":"Working with Nutanix Objects","text":"<p>Peer Global File Service includes the ability to push data from NAS devices into object storage. The same real-time replication technology used to power the collaboration scenario above can also be used to replicate data into Nutanix Objects with optional snapshot capabilities for point-in-time recovery. All objects are replicated in a transparent format that can be immediately used by other apps and services.</p> <p>This lab section will walk you through the necessary steps to replicate data from Nutanix Files into Nutanix Objects.</p>"},{"location":"peer/peer/#getting-client-ip-and-credentials-for-nutanix-objects","title":"Getting Client IP and Credentials for Nutanix Objects","text":"<p>In order to replicate data into Objects, you need the Client IP of the object store and need to generate access and secret keys. If you already have this information from a prior lab, you can skip this section and re-use that existing information.</p> <ol> <li> <p>Log in to Prism Central (e.g., 10.XX.YY.39) on your Nutanix     cluster, and then navigate to Services &gt; Objects.</p> </li> <li> <p>In the Object Stores section, find the appropriate object store     in the table and note the Client Used IPs.</p> <p></p> </li> <li> <p>Click on the Access Keys section and click Add People to     begin the process for creating credentials.</p> <p></p> </li> <li> <p>Select Add people not in Active Directory and enter your e-mail     address.</p> <p></p> </li> <li> <p>Click Next.</p> </li> <li> <p>Click Download Keys to download a .csv file containing the     Access Key and Secret Key.</p> <p></p> </li> <li> <p>Click Close.</p> </li> <li> <p>Open the file with a text editor.</p> <p></p> <p>::: note ::: title Note :::</p> <p>Keep the text file open so that you have the access and secret keys readily available for the sections below. :::</p> </li> </ol>"},{"location":"peer/peer/#creating-a-new-cloud-replication-job","title":"Creating a New Cloud Replication Job","text":"<p>In this section, we will focus on creating a Cloud Backup and Replication job to replicate data from Nutanix Files into Nutanix Objects.</p> <ol> <li> <p>In the PMC Web Interface, click File &gt; New Job.</p> <p></p> </li> <li> <p>Select Cloud Backup and Replication and click Create.</p> </li> <li> <p>Enter Initials - Replication to Objects as the name for the     job and click OK.</p> <p></p> </li> <li> <p>Select Nutanix Files and click Next.</p> <p></p> </li> <li> <p>Select the Agent named PeerAgent-Files and click Next. This     Agent will manage the Files cluster.</p> <p></p> </li> <li> <p>On the Storage Information page, you will see one of two pages.     If another participant sharing your Files cluster has already done     the Peer lab, you can select their Existing Credentials as shown     here.</p> <p></p> <p>If you are the first participant on this cluster to do the Peer lab, fill out the following fields:</p> <ul> <li> <p>Nutanix Files Cluster Name - BootcampFS</p> <p>The NETBIOS name of the Files cluster that will be paired with the Agent selected in the previous step.</p> </li> <li> <p>Username - peer</p> <p>This is the Files API account username configured earlier in the lab and MUST be in all lower case.</p> </li> <li> <p>Password - nutanix/4u</p> <p>The password associated with the Files API account.</p> </li> <li> <p>Peer Agent IP - PeerAgent-Files IP Address</p> <p>The IP address of the Agent server that will receive real-time notifications from the File Activity Monitoring API built into Files. It will be selectable from a dropdown list of available IPs on this Agent server.</p> </li> </ul> </li> <li> <p>Click Validate to confirm Files can be accessed via API using     the provided credentials.</p> <p></p> <p>::: note ::: title Note :::</p> <p>Once you enter these credentials, they are reusable when creating new jobs that use this particular Agent. When you create your next job, select Existing Credentials on this page to display a list of previously configured credentials. :::</p> </li> <li> <p>Click Next.</p> </li> <li> <p>Select your Initials-Peer share and click OK.</p> <p></p> <p>::: note ::: title Note :::</p> <p>Peer Global File Service supports the replication of data within nested shares starting with Nutanix Files v3.5.1 and above. :::</p> <p>::: note ::: title Note :::</p> <p>With Cloud Backup and Replication, you can select multiple shares and/or folders for a single job. :::</p> </li> <li> <p>On the File Filters page, verify the Default filter selected     as well as the Include Files Without Extensions, and click     Next.</p> <p></p> </li> <li> <p>On the Destination page, select Nutanix Objects and click     Next.</p> <p></p> </li> <li> <p>On the Nutanix Objects Credentials page, fill out the following     fields:</p> <ul> <li> <p>Description -- Name your destination</p> <p>This is a short name for the Objects credential configuration.</p> </li> <li> <p>Access Key</p> <p>The Access Key associated with the Objects account.</p> </li> <li> <p>Secret Key</p> <p>The Secret Key associated with the Objects account.</p> </li> <li> <p>Service Point</p> <p>The client access IP address or FDQN name of the object store.</p> </li> </ul> <p></p> <p>::: note ::: title Note :::</p> <p>Refer to the Getting Client IP and Credentials for Nutanix Objects section above for the appropriate access and secret keys, as well as the Client IP of the object store. :::</p> </li> <li> <p>Click Validate to confirm Objects can be accessed using the     provided configuration.</p> <p></p> </li> <li> <p>Click OK in the Success window, and then click Next.</p> </li> <li> <p>On the Bucket Details page, deselect the Automatically name     checkbox, and then provide a unique bucket name of     initials-peer-objects.</p> <p></p> <p>::: note ::: title Note :::</p> <p>The bucket name MUST be in all lower case. :::</p> </li> <li> <p>On the Replication and Retention Policy page, select Existing     Policy, Continuous Data Protection, and then click Next.</p> <p></p> </li> <li> <p>Click Next on the Miscellaneous Options, Email Alerts,     and SNMP Alerts pages.</p> </li> <li> <p>Review the configuration on the Confirmation screen, and then     then click Finish.</p> <p></p> </li> </ol>"},{"location":"peer/peer/#starting-a-cloud-replication-job","title":"Starting a Cloud Replication Job","text":"<p>Once a job has been created, it must be started to initiate replication.</p> <ol> <li> <p>In the PMC Web Interface, right-click on your newly created job,     and then select Start.</p> <p></p> </li> <li> <p>Double-click the job in the Job pane to view its runtime     information and statistics.</p> <p></p> <p>::: note ::: title Note :::</p> <p>Click Auto-Update to have the console regularly refresh as files begin replicating. :::</p> </li> </ol>"},{"location":"peer/peer/#verifying-replication","title":"Verifying Replication","text":"<p>::: note ::: title Note :::</p> <p>This exercise requires the <code>windows_tools_vm</code>{.interpreted-text role=\"ref\"}. :::</p> <p>The easiest way to verify that files have been replicated into Nutanix Objects is to use the Cyberduck tool on your Initials-Windows-ToolsVM</p> <ol> <li> <p>Connect to your Initials-Windows-ToolsVM via RDP using the     following credentials:</p> <ul> <li>Username - NTNXLAB\\Administrator</li> <li>Password - nutanix/4u</li> </ul> </li> <li> <p>Launch Cyberduck (Click the Window icon &gt; Down Arrow &gt;     Cyberduck).</p> <p>If you are prompted to update Cyberduck, click Skip This Version.</p> </li> <li> <p>Click on Open Connection.</p> <p></p> </li> <li> <p>Select Amazon S3 from the dropdown list.</p> <p></p> </li> <li> <p>Fill out the following fields for the user created earlier, and then     click Connect:</p> <ul> <li>Server - Objects Client Used IP</li> <li>Port - 443</li> <li>Access Key ID - Generated When User Created</li> <li>Password (Secret Key) - Generated When User Created</li> </ul> <p>::: note ::: title Note :::</p> <p>See the Getting Client IP and Credentials for Nutanix Objects section above for the appropriate access and secret keys, as well as the Client IP of the object store. :::</p> <p></p> </li> <li> <p>Check the Always Trust checkbox, and then click Continue in     the The certificate is not valid dialog box.</p> <p></p> </li> <li> <p>Click Yes to continue installing the self-signed certificate.</p> </li> <li> <p>Navigate to the appropriate bucket set above and verify that it     contains content.</p> <p></p> <p>Congratulations! You have successfully setup replication between Nutanix Files and Nutanix Objects! Using Peer, this same approach can be leveraged to support scenarios including coexistence of file data with object-based apps and services as well as point-in-time recovery of enterprise NAS data backed by Objects.</p> </li> </ol>"},{"location":"peer/peer/#analyzing-existing-environments","title":"Analyzing Existing Environments","text":"<p>::: note ::: title Note :::</p> <p>This exercise requires the <code>windows_tools_vm</code>{.interpreted-text role=\"ref\"}. :::</p> <p>As the capacity of file server environments increase at a record pace, storage admins often do not know how users and applications are leveraging these file server environments. This fact becomes most evident when it is time to migrate to a new storage platform. The File System Analyzer is a tool from Peer Software that is designed to help partners discover and analyze existing file and folder structures for the purpose of planning and optimization.</p> <p>The File System Analyzer performs a very fast scan of one or more specified paths, uploads results to Amazon S3, assembles key pieces of information into one or more Excel workbooks, and emails reports with links to access the workbooks.</p> <p>As this tool is primarily for our partners, we would love to hear any feedback you have on it. Reach out to us on Slack via the #_peer_software_ext channel with comments and suggestions.</p>"},{"location":"peer/peer/#installing-and-running-the-file-system-analyzer","title":"Installing and Running the File System Analyzer","text":"<ol> <li> <p>Connect to your Initials-Windows-ToolsVM via RDP using the     following credentials:</p> <ul> <li>Username - NTNXLAB\\Administrator</li> <li>Password - nutanix/4u</li> </ul> </li> <li> <p>Within the VM, download the File System Analyzer installer:     https://www.peersoftware.com/downloads/fsa/13/FileSystemAnalyzer_win64.exe</p> </li> <li> <p>Run the installer and select Immediate Installation.</p> <p></p> <p>Once the installation is complete, the File System Analyzer wizard is automatically launched.</p> </li> <li> <p>The Introduction screen provides details on information     collected and reported by the utility. Click Next.</p> <p></p> </li> <li> <p>The Contact Information screen collects information used to     organize the output of the File System Analyzer and to send the     final reports. Fill out the following fields:</p> <ul> <li>Company -- Enter your company name.</li> <li>Location -- Enter the physical location of the server that     is running the File System Analyzer. In multi-site environments,     this could be a city or state name. A data center name also     works.</li> <li>Project -- Enter a project name or business reason for     running this analysis. This (and the Company and Location     fields) are used solely to organize the final reports.</li> <li>Mode -- Select the mode of operation to be used -- General     Analysis or Migration Preparation. Migration     Preparation is useful when preparing for a migration project     between storage systems. In addition to collecting standard     telemetry on file systems, this mode also offers the option to     test performance of both the existing and new storage systems to     help gauge potential migration performance and timing. For this     lab, we will use General Analysis.</li> <li>Name/Phone/Title -- (Optional) Enter your name and contact     information.</li> <li>Email -- Enter the email address to which the final reports     will be sent. For multiple addresses, enter a comma-separated     list.</li> <li>Upload Region -- Select US, EU, or APAC to tell     the File System Analyzer which S3 location to use for uploading     the final reports.</li> </ul> <p><pre><code>&lt;strong&gt;&lt;font color=\"red\"&gt;Be sure to enter your own details into the wizard page shown below. Otherwise, the final report will not be sent to you.&lt;/font&gt;&lt;/strong&gt;\n</code></pre> </p> </li> <li> <p>Click Next.</p> <p>The File System Analyzer can be configured to scan one or more paths. These paths can be local (e.g., <code>D:\\MyData</code>) or a remote UNC Path (e.g., <code>\\\\files01\\homes1</code>).</p> </li> <li> <p>Add the following paths:</p> <ul> <li><code>C:\\</code> - The local C: drive of Initials-Windows-ToolsVM</li> <li><code>\\\\BootcampFS\\&lt;Your Share Name&gt;\\</code> - A share previously created     on Nutanix Files</li> </ul> <p></p> <p></p> </li> <li> <p>Click Next.</p> <p>Click the Start button to begin scanning the entered paths. When all scans, analyses, and uploads are complete, you will see a status that is similar to the following:</p> <p></p> </li> <li> <p>File System Analyzer will also email the report to all configured     addresses. To view the full report, click the hyperlink(s) listed     under Detailed Reports in the email. If multiple paths were     scanned, you will also see a link to a cumulative report across all     paths.</p> <p></p> <p>::: note ::: title Note :::</p> <p>Report download links are active for 24 hours only. Contact Peer Software to access any expired reports. :::</p> <p>Some systems may open these workbooks in a protected mode, displaying this message in Excel:</p> <p></p> <p>If you see this message at the top of Excel, click Enable Editing to fully open the workbook. If you do not do this, the pivot tables and charts will not load properly.</p> </li> </ol>"},{"location":"peer/peer/#summary-reports","title":"Summary Reports","text":"<p>Summary reports contain overall statistical and historical information across all paths that have been selected to be scanned. When you open a summary report, you are greeted with a worksheet like this:</p> <p></p> <p>Each summary report may contain some or all of the following worksheets:</p> <ul> <li>InfoSheet -- Details about this specific run. This page will     also show Total Bytes formatted in both decimal (1 KB is 1,000     bytes) and binary (1 KiB is 1,024 bytes) forms.</li> <li>CollectiveResults -- A list of all paths scanned along with     high-level statistics for each.</li> <li>History-Bytes -- Contains historical changes in bytes for each     time each path is scanned.</li> <li>History-Files -- Contains historical changes in total number of     files for each time each path is scanned.</li> <li>History-Folders -- Contains historical changes in total numbers     of folders for each time each path is scanned.</li> </ul> <p>::: note ::: title Note :::</p> <p>History worksheets will only appear after running multiple scans. :::</p>"},{"location":"peer/peer/#volume-reports","title":"Volume Reports","text":"<p>Volume reports give more detailed information about a specific path that has been scanned. When you open a volume report, you are greeted with a worksheet like this:</p> <p></p> <p>Each volume report may contain some or all of the following worksheets:</p> <ul> <li>Overview -- A series of pivot tables and charts showing     high-level statistics about the path that was scanned.</li> <li>InfoSheet -- Details about this specific scan. This page will     also show Total Bytes formatted in both decimal (1 KB is 1,000     bytes) and binary (1 KiB is 1,024 bytes) forms.</li> <li>OverallStats -- Overall statistics for the folder that was     scanned. This includes total bytes, files, folders, etc.</li> <li>Analysis -- Includes a pivot table and a pair of charts     highlighting additional statistics about the path that was scanned.</li> <li>History -- Shows statistics from each scan of this volume.</li> <li>HistoryCharts -- Contains charts showing historical changes in     files, folders, and bytes for this volume.</li> <li>HighSubFolderCounts -- A list of all folders containing more     than 100 child directories.</li> <li>HighByteCounts -- A list of all folders containing more than     10GB of child file data.</li> <li>HighFileCounts -- A list of all folders containing more than     10,000 child files.</li> <li>LargeFiles -- A list of all discovered files that are 10GB or     larger.</li> <li>DeepPaths -- A list of all discovered folder paths that are 15     levels deep or deeper.</li> <li>LongPaths -- A list of all discovered folder paths that are 256     characters or longer.</li> <li>ReparsePointsSummary -- A summary of all reparse points     discovered, regardless of file or folder.</li> <li>ReparsePoints -- A list of all folder reparse points discovered.</li> <li>TimeAnalysis -- A breakdown of total files, folders, and bytes     by age.</li> <li>LastModifiedAnalysis -- A view of all files, folders, and bytes     modified each hour for the past year. These numbers are then totaled     and averaged to show files, folders, and bytes modified by: day of     week; month; hour of the day; day of month; and day of year.</li> <li>CreatedAnalysis -- A view of all files, folders, and bytes     created each hour for the past year. These numbers are then totaled     and averaged to show files, folders, and bytes created by day of     week, month, hour of the day, day of month, and day of year.</li> <li>LastAccessedAnalysis -- A view of all files, folders, and bytes     accessed each hour for the past year. These numbers are then totaled     and averaged to show files, folders, and bytes accessed by: day of     week; month; hour of the day; day of month; and day of year.</li> <li>TLDAnalysis - A list of each folder immediately under a     specified path with statistics for each of these subfolders. In a     user home directory environment, each of these subfolders should     represent a different user.</li> <li>TopTLDsByTotals -- A series of pivot tables and charts showing     the top ten top-level directories based on total bytes used, total     files, and total folders.</li> <li>TopTLDsByLastModBytes -- A pivot table and chart showing top 10     top-level directories based on most bytes modified in the past year.</li> <li>TopTLDsByLastModFiles -- A pivot table and chart showing top 10     top-level directories based on most files modified in the past year.</li> <li>LegacyTLDs -- A list of all top-level directories that do not     contain any files modified in the past 365 days.</li> <li>TreeDepth -- A tally of bytes, folders, and files found at each     depth level of the folder structure. For customers doing a     pre-migration analysis, depths that appear as green are good     candidates for PeerSync Migration's tree depth setting.</li> <li>FileExtInfo -- A list of all discovered extensions, including     pivot tables sorted by total bytes and total files.</li> <li>FileAttributes -- A summary of all file and folder attributes     found.</li> <li>SmallFileAnalysis -- A list of all files discovered below a     certain size. This page is useful for estimating the storage impact     of small files on storage platforms that have large minimum file     sizes on disk.</li> <li>SIDCache -- A list of all the owners and SID strings that have     been discovered.</li> </ul> <p>::: note ::: title Note :::</p> <p>History worksheets will only appear after running multiple scans. :::</p> <p>Here is a sample of the LastModifiedAnalysis page mentioned above:</p> <p></p>"},{"location":"peer/peer/#integrating-with-microsoft-dfs-namespace","title":"Integrating with Microsoft DFS Namespace","text":"<p>Peer Global File Service includes the ability to create and manage Microsoft DFS Namespaces (DFS-N). When this DFS-N integration is combined with its real-time replication and file locking engine, PeerGFS powers a true global namespace that spans locations and storage devices.</p> <p>As part of its DFS namespace management capabilities, PeerGFS also automatically redirects users away from a failed file server. When that failed server comes back online, PeerGFS brings this file server back in-sync, and then re-enables user access to it. This is an essential Disaster Recovery feature for any deployment looking to leverage Nutanix Files for user profile and user data shares for VDI environments.</p> <p>The following screenshot shows the PMC interface with a DFS Namespace under management.</p> <p></p>"},{"location":"peer/peer/#takeaways","title":"Takeaways","text":"<ul> <li>Peer Global File Service is the only solution which can provide     Active-Active replication for Nutanix Files clusters.</li> <li>Peer also supports multiple legacy NAS platforms and supports     replication within mixed environments. This helps ease adoption of     and migration to Nutanix Files.</li> <li>Peer can directly manage Microsoft Distributed File Services (DFS)     namespaces, allowing multiple file servers to be presented through a     single namespace. This is a key component for supporting true     Active-Active DR solutions for file sharing.</li> <li>Peer can replicate files from Nutanix Files and other NAS platforms     into Nutanix Objects with optional snapshot capabilities for     point-in-time recovery. All objects are in a transparent format that     can be immediately used by other apps and services.</li> <li>Peer offers tools for analyzing existing file servers to help with     resource planning, optimization, and minimally disruptive     migrations.</li> </ul>"},{"location":"rbac/rbac/","title":"Role Base Access Control for NUS","text":""},{"location":"rbac/rbac/#overview","title":"Overview","text":"<p>Cluster role-based access control (RBAC) feature enables a super-admin user to define different access right to Nutanix Files and Objects in Prism Central. Admin can choose default or customized role to assign to users to provide admin or view access to a particular Files or Objects entity. The access right can be assigned to File Server or Object Store level so different user can manage different FS or OSS according to the management requirement.</p>"},{"location":"rbac/rbac/#lab-setup","title":"Lab Setup","text":"<p>The lab has enabled MSP on Prism Central which is a pre-requisite of using RBAC. In real deployment, you should first enable microservices infrastruture before starting this lab.</p>"},{"location":"rbac/rbac/#define-roles","title":"Define Roles","text":"<ol> <li> <p>Login to Prism Central as admin, select  &gt; Administration &gt; Roles, click Create Role</p> <p></p> </li> <li> <p>In this menu, fill out the fields:</p> <ul> <li>Put Role Name: NUS View Only User</li> <li>Expand Object Store, click View Access</li> <li>Expand File Server, click View Access </li> <li>Click Change next to Set custom permissions in File Server</li> <li>Click Create File Server Share, then click Save </li> <li>Click Save</li> </ul> </li> <li> <p>You will find NUS user is showing in the Role.     </p> </li> </ol>"},{"location":"rbac/rbac/#assign-users-to-roles","title":"Assign Users to Roles","text":"<ol> <li> <p>In Prism Central as admin, select  &gt; Administration &gt; Roles, click the checkbox next to NUS user</p> </li> <li> <p>Click Action &gt; Manage Assignment &gt; Add New</p> </li> <li> <p>In the Search User box, put userXX and select userXX@ntnxlab.local (where XX is the assigned user number to you)     </p> </li> <li> <p>In the right side, choose File Server as the Entity Type, then select Individual entity</p> </li> <li> <p>In the search box, find and select FSXYZ-#-prod</p> </li> <li> <p>Choose Object Store as the Entity Type, in the search box. find and select ntnx-objects </p> </li> <li> <p>Click Save</p> </li> </ol> <p>Note</p> <pre><code>You can add multiple users / file servers / object stores into the same assignment.\n</code></pre>"},{"location":"rbac/rbac/#verify-user-access-right","title":"Verify User Access Right","text":"<ol> <li> <p>Logout from admin to Prism Central</p> <p>Note</p> <pre><code>If you do not want to logout, you can use another browser or go to Incognito Mode of the browser so you can use different users to login at the same time.\n</code></pre> </li> <li> <p>Make sure you are on the page showing Login with your Company ID</p> </li> <li> <p>Login as userXX@ntnxlab.local, and click Log In     </p> </li> <li> <p>Click , you will see the list of service is shorter.</p> </li> <li> <p>Click Services, you can only see Files and Objects</p> <p>Note</p> <pre><code>You maybe able to see more services in Prism Central, but when you click in, you will see you have no permission to access.\n</code></pre> </li> <li> <p>Click Objects, you can see only ntnx-objects can be accessed and the Create Object Store button is disappeared. Click ntnx-objects</p> </li> <li> <p>You can compare the GUI of admin and userXX@ntnxlab.local, you will see userXX@ntnxlab.local has only read-only access and cannot Create Bucket.     </p> </li> <li> <p>Go back to your admin login's Prism Central.</p> </li> <li> <p>Select  &gt; Administration &gt; Roles, click the checkbox next to NUS user &gt; Actions &gt; Update Role</p> </li> <li> <p>Expand Object Store, click Change next to Set customer permission</p> </li> <li> <p>Check the tickbox of Create Object Store, then Save </p> </li> <li> <p>Go back to userXX@ntnxlab.local's Prism Central, refresh the page.</p> </li> <li> <p>Now go to Objects, you can see the Create Object Store button is appeared again.</p> </li> <li> <p>Now go to Files, you will only see FSxyz-#-prod in File Server, click the tickbox next to FSxyz-#-prod and click Actions, you can see you cannot do any of them. It is because you have only view access and create share access. You do not have right to update any File Server settings.</p> <p></p> <p>Tip</p> <pre><code>If you are using PE deploy File Server, the launch PE console from PC and then launching File server page is permitted for Super Admin, Cluster Admin and user who has Prism Admin and Files Admin role\n</code></pre> </li> <li> <p>Now click FSxyz-#-prod to enter the file management console, you can view everything related to this File Server. </p> </li> <li> <p>Click Shares &gt; Create a New Share, follow the steps to create a share.</p> </li> <li> <p>Click Data Management &gt; Protection, you have no right to configure anything from here.</p> </li> </ol>"},{"location":"rbac/rbac/#takeaway","title":"Takeaway","text":"<p>By using role base access control, administrator can manage user access to Files and Objects, control access right according to File Server and Object Store with very granular control of the access right. </p>"},{"location":"tools_vms/linux_tools_vm/","title":"Linux Tools VM {#linux_tools_vm}","text":""},{"location":"tools_vms/linux_tools_vm/#overview","title":"Overview","text":"<p>This CentOS VM image will be staged with packages used to support multiple lab exercises.</p> <p>Deploy this VM on your assigned cluster if directed to do so as part of Lab Setup.</p> <pre><code>&lt;body&gt;&lt;font color=\"red\"&gt;Only deploy the VM once, it does not need to be cleaned up as part of any lab completion.&lt;/font&gt;&lt;/body&gt;\n</code></pre>"},{"location":"tools_vms/linux_tools_vm/#deploying-linux-tools-vm","title":"Deploying Linux Tools VM","text":"<p>In Prism Central &gt; select <code>bars</code>{.interpreted-text role=\"fa\"} &gt; Compute and Storage &gt; VMs, and click Create VM.</p> <ol> <li>Fill out the following fields:<ul> <li>Name - Initials-Linux-ToolsVM</li> <li>Description - (Optional) Description for your VM.</li> <li>Number of VMs - 1</li> <li>vCPU(s) - 1</li> <li>Number of Cores per vCPU - 2</li> <li>Memory - 2 GiB</li> </ul> </li> <li>Click Next</li> <li>Under Disks select Attach Disk<ul> <li>Type - DISK</li> <li>Operation - Clone from Image</li> <li>Image - Linux_ToolsVM.qcow2</li> <li>Capacity - leave at default size</li> <li>Bus Type - leave at default SCSI Setting</li> </ul> </li> <li>Click Save</li> <li>Under Networks select Attach to Subnet<ul> <li>VLAN Name - Primary</li> <li>Network Connection State - Connected</li> <li>Assignment Type - Assign with DHCP</li> </ul> </li> <li>Click Save</li> <li>Click Next at the bottom</li> <li>In Management section<ul> <li>Categories - leave blank</li> <li>Timezone - leave at default UTC</li> <li>Guest Customization - No customization</li> </ul> </li> <li>Click Create VM at the bottom</li> <li>Go back to Prism Central &gt; select <code>bars</code>{.interpreted-text     role=\"fa\"} &gt; Compute and Storage &gt; VMs</li> <li>Select your Initials-Linux-ToolsVM</li> <li>Under Actions drop-down menu, choose Power On</li> </ol> <p>Login to the VM via SSH or Console session, using the following credentials:</p> <ul> <li>Username - root</li> <li>password - nutanix/4u</li> </ul>"},{"location":"tools_vms/windows_tools_vm/","title":"Windows Tools VM {#windows_tools_vm}","text":""},{"location":"tools_vms/windows_tools_vm/#overview","title":"Overview","text":"<p>This Windows Server 2012 R2 image comes pre-installed with a number of tools, including:</p> <ul> <li>Microsoft Remote Server Administration Tools (RSAT)</li> <li>PuTTY, CyberDuck, WinSCP</li> <li>Sublime Text 3, Visual Studio Code</li> <li>OpenOffice</li> <li>Python</li> <li>pgAdmin</li> <li>Chocolatey package manager</li> <li>Google Chrome browser</li> </ul> <p>Deploy this VM on your assigned cluster if directed to do so as part of Lab Setup.</p> <pre><code>&lt;body&gt;&lt;font color=\"red\"&gt;Only deploy the VM once, it does not need to be cleaned up as part of any lab completion.&lt;/font&gt;&lt;/body&gt;\n</code></pre>"},{"location":"tools_vms/windows_tools_vm/#deploying-windows-tools-vm","title":"Deploying Windows Tools VM","text":"<p>In Prism Central &gt; select <code>bars</code>{.interpreted-text role=\"fa\"} &gt; Compute and Storage &gt; VMs, and click Create VM.</p> <ol> <li>Fill out the following fields:<ul> <li>Name - Initials-Windows-ToolsVM</li> <li>Description - (Optional) Description for your VM.</li> <li>Number of VMs - 1</li> <li>vCPU(s) - 1</li> <li>Number of Cores per vCPU - 2</li> <li>Memory - 2 GiB</li> </ul> </li> <li>Click Next</li> <li>Under Disks select Attach Disk<ul> <li>Type - DISK</li> <li>Operation - Clone from Image</li> <li>Image - WinToolsVM-Q1CY21.qcow2</li> <li>Capacity - leave at default size</li> <li>Bus Type - leave at default SCSI Setting</li> </ul> </li> <li>Click Save</li> <li>Under Networks select Attach to Subnet<ul> <li>VLAN Name - Primary</li> <li>Network Connection State - Connected</li> <li>Assignment Type - Assign with DHCP</li> </ul> </li> <li>Click Save</li> <li>Click Next at the bottom</li> <li>In Management section<ul> <li>Categories - leave blank</li> <li>Timezone - leave at default UTC</li> <li>Guest Customization - No customization</li> </ul> </li> <li>Click Create VM at the bottom</li> <li>Go back to Prism Central &gt; select <code>bars</code>{.interpreted-text     role=\"fa\"} &gt; Compute and Storage &gt; VMs</li> <li>Select your Initials-Windows-ToolsVM</li> <li>Under Actions drop-down menu, choose Power On</li> </ol> <p>Login to the VM via RDP or Console session, using the following credentials:</p> <ul> <li>Username - NTNXLAB\\Administrator</li> <li>password - nutanix/4u</li> </ul>"},{"location":"volumes_deploy/template/","title":"template","text":"<p>execute the following commands</p> <pre><code>ls -ltrh\n</code></pre> <pre><code>s = \"Python syntax highlighting\"\nprint s\n</code></pre> <pre><code>Get-Date\n</code></pre> <pre><code>metadata:\n  name: gary\n  namespace: default\n</code></pre> <ol> <li>this is step 1</li> <li>this is step 2</li> <li>this is step 3</li> </ol> <ul> <li>this point 1</li> <li>this is point 2</li> </ul> Note <p>This is a note</p> <p>Info</p> <pre><code>This is information\n\nPlease go [here](https://www.facebook.com) to read more about my lif\n</code></pre> Do you like tips? <p>This is a tip</p>"},{"location":"volumes_deploy/volumes_deploy/","title":"Volumes","text":"<p>In this section, we will create a Nutanix Volume and then using Windows VM to connect to the volume using iSCSI protocol.</p>"},{"location":"volumes_deploy/volumes_deploy/#volumes-setup","title":"Volumes Setup","text":""},{"location":"volumes_deploy/volumes_deploy/#check-iscsi-data-service-ip","title":"Check iSCSI Data Service IP","text":"<ol> <li> <p>Click the Cluster name in the upper left hand corner to check the iSCSI data service IP</p> </li> <li> <p>Close Cluster Details and proceed to Configure Guests</p> </li> </ol> <p></p>"},{"location":"volumes_deploy/volumes_deploy/#get-the-iscsi-iqn-name-from-wintoolsvm","title":"Get the iSCSI iqn name from WinToolsVM","text":"<ol> <li> <p>Login to initials-WinToolsVM on your cluster with username \"administrator\" and your password. Click Windows icon from bottom left corner. Click the Search icon and enter iscsi. Click iSCSI Initiator.</p> <p></p> </li> <li> <p>Click Yes to start iSCSI service </p> <p></p> </li> <li> <p>Click the Configuration tab to find the Initiator Name as the iqn. Make a note of it for a later step.</p> <p></p> </li> </ol>"},{"location":"volumes_deploy/volumes_deploy/#create-a-volume-group-vg","title":"Create a Volume Group (VG)","text":"<ol> <li> <p>Go to Prism Element, navigate to the Storage Dashboard, click + Volume Group to create a new Volume Group.</p> </li> <li> <p>In the Volume Group Window give the volume group a name Intials-vg, add 2 new disks and select default container, input size for each of the disk of 10 and click Add.</p> </li> <li> <p>Click Save.     </p> </li> </ol>"},{"location":"volumes_deploy/volumes_deploy/#connect-volumes-disks-to-windows-vm","title":"Connect Volumes Disks to Windows VM","text":"<ol> <li> <p>Locate the VG you just created in Storage Dashboard &gt; Table &gt; Volume Group. Click on the VG and click Update. </p> </li> <li> <p>Under Client, click + Add New Client, locate the iqn you identify in your WinToolsVM in Client IQN/IP Address and click Add. The iqn will be selected to add as client.</p> <p></p> </li> <li> <p>Switch back to your WinToolsVM. In iSCSI initiator properties click on the Targets tab. Type in the iSCSI Data Service IP Address you found in Prism Element and click Quick Connect. You will see the target volume group we previously created. As we created 2 disks in the VG, you will see 2 targets discovered. </p> <p></p> </li> <li> <p>Click Connect.</p> </li> </ol> <p>Note</p> <p>Volumes does not require multipath I/O (MPIO) configuration on the client. Volumes provides highly available and high performance access to the iSCSI LUNs to clients natively.</p> <ol> <li> <p>Click the WIndows Icon and search for Create and format hard disk partitions. Then you will go to Disk Management MMC.</p> </li> <li> <p>From here you can see the 2 raw disks you created. You can now put the disks online, initialize, format and assign drive letters to them.</p> <p></p> </li> </ol>"},{"location":"volumes_deploy/volumes_deploy/#conclusion","title":"Conclusion","text":"<p>As part of the Nutanix Unified Storage family, Nutanix Volumes is an enterprise-class software-defined storage that exposes storage resources directly to guest operating systems or physical hosts using the iSCSI protocol. This lab shows how easy to setup and configure iSCSI connection from client and consume VGs from Nutanix Cloud Platform.</p>"}]}